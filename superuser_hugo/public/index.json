[{"content":"Long pandemic no see! I submitted two proposals for SRECon 20 Asia which was supposed to happen in Australia. One talk got selected. But instead of the conference, COVID-19 happened. The conference was delayed at first and ultimately cancelled. Comes 2021 and now the conference is arranged in virtual format. I submit the selected talk again, under MLOps section and it got selected again.\nThe talk describes the approach to detect application hotspots our team developed in collaboration with the ML team.\nLeveraging ML to Detect Application HotSpots [@scale, of Course!] The talk\u0026rsquo;s description looked something like:\nThis talk will explore various analyses done on service latency metrics and their correlation, while LinkedIn\u0026rsquo;s data-center is under a stress test. Note: you do not need to be a machine learning expert to make sense of this talk. We will not be diving deeper into the mathematics part of it but would rather focus on the approach.\nSlides and Recording Slides YouTube ","permalink":"http://localhost:1313/posts/srecon-21-ml-application-hotspot-detect/","summary":"Long pandemic no see! I submitted two proposals for SRECon 20 Asia which was supposed to happen in Australia. One talk got selected. But instead of the conference, COVID-19 happened. The conference was delayed at first and ultimately cancelled. Comes 2021 and now the conference is arranged in virtual format. I submit the selected talk again, under MLOps section and it got selected again.\nThe talk describes the approach to detect application hotspots our team developed in collaboration with the ML team.","title":"SRECon'21: Leveraging ML to Detect Application HotSpots [@scale, of Course!]"},{"content":"Everything in Python is an object. And that includes functions. Let\u0026rsquo;s see what I learned while I was trying to work with Google cloud functions with python runtime.\nPython Functions Since functions too are objects, we can see what all attributes a function contains as following\n\u0026gt;\u0026gt;\u0026gt; def hello(name): ... print(f\u0026#34;Hello, {name}!\u0026#34;) ... \u0026gt;\u0026gt;\u0026gt; dir(hello) [\u0026#39;__annotations__\u0026#39;, \u0026#39;__call__\u0026#39;, \u0026#39;__class__\u0026#39;, \u0026#39;__closure__\u0026#39;, \u0026#39;__code__\u0026#39;, \u0026#39;__defaults__\u0026#39;, \u0026#39;__delattr__\u0026#39;, \u0026#39;__dict__\u0026#39;, \u0026#39;__dir__\u0026#39;, \u0026#39;__doc__\u0026#39;, \u0026#39;__eq__\u0026#39;, \u0026#39;__format__\u0026#39;, \u0026#39;__ge__\u0026#39;, \u0026#39;__get__\u0026#39;, \u0026#39;__getattribute__\u0026#39;, \u0026#39;__globals__\u0026#39;, \u0026#39;__gt__\u0026#39;, \u0026#39;__hash__\u0026#39;, \u0026#39;__init__\u0026#39;, \u0026#39;__init_subclass__\u0026#39;, \u0026#39;__kwdefaults__\u0026#39;, \u0026#39;__le__\u0026#39;, \u0026#39;__lt__\u0026#39;, \u0026#39;__module__\u0026#39;, \u0026#39;__name__\u0026#39;, \u0026#39;__ne__\u0026#39;, \u0026#39;__new__\u0026#39;, \u0026#39;__qualname__\u0026#39;, \u0026#39;__reduce__\u0026#39;, \u0026#39;__reduce_ex__\u0026#39;, \u0026#39;__repr__\u0026#39;, \u0026#39;__setattr__\u0026#39;, \u0026#39;__sizeof__\u0026#39;, \u0026#39;__str__\u0026#39;, \u0026#39;__subclasshook__\u0026#39;] While there are a lot of them, let\u0026rsquo;s look at some interesting ones\nglobals This attribute, as the name suggests, has references of global variables. If you ever need to know what all global variables are in the scope of this function, this will tell you. See how the function start seeing the new variable in globals\n\u0026gt;\u0026gt;\u0026gt; hello.__globals__ {\u0026#39;__name__\u0026#39;: \u0026#39;__main__\u0026#39;, \u0026#39;__doc__\u0026#39;: None, \u0026#39;__package__\u0026#39;: None, \u0026#39;__loader__\u0026#39;: \u0026lt;class \u0026#39;_frozen_importlib.BuiltinImporter\u0026#39;\u0026gt;, \u0026#39;__spec__\u0026#39;: None, \u0026#39;__annotations__\u0026#39;: {}, \u0026#39;__builtins__\u0026#39;: \u0026lt;module \u0026#39;builtins\u0026#39; (built-in)\u0026gt;, \u0026#39;hello\u0026#39;: \u0026lt;function hello at 0x7fe4e82554c0\u0026gt;} # adding new global variable \u0026gt;\u0026gt;\u0026gt; GLOBAL=\u0026#34;g_val\u0026#34; \u0026gt;\u0026gt;\u0026gt; hello.__globals__ {\u0026#39;__name__\u0026#39;: \u0026#39;__main__\u0026#39;, \u0026#39;__doc__\u0026#39;: None, \u0026#39;__package__\u0026#39;: None, \u0026#39;__loader__\u0026#39;: \u0026lt;class \u0026#39;_frozen_importlib.BuiltinImporter\u0026#39;\u0026gt;, \u0026#39;__spec__\u0026#39;: None, \u0026#39;__annotations__\u0026#39;: {}, \u0026#39;__builtins__\u0026#39;: \u0026lt;module \u0026#39;builtins\u0026#39; (built-in)\u0026gt;, \u0026#39;hello\u0026#39;: \u0026lt;function hello at 0x7fe4e82554c0\u0026gt;, \u0026#39;GLOBAL\u0026#39;: \u0026#39;g_val\u0026#39;} code This is an interesting one! As everything in python is an object, this includes the bytecode too. The compiled python bytecode is a python code object. Which is accessible via __code__ attribute here. A function has an associated code object which carries some interesting information.\n# the file in which function is defined # stdin here since this is run in an interpreter \u0026gt;\u0026gt;\u0026gt; hello.__code__.co_filename \u0026#39;\u0026lt;stdin\u0026gt;\u0026#39; # number of arguments the function takes \u0026gt;\u0026gt;\u0026gt; hello.__code__.co_argcount 1 # local variable names \u0026gt;\u0026gt;\u0026gt; hello.__code__.co_varnames (\u0026#39;name\u0026#39;,) # the function code\u0026#39;s compiled bytecode \u0026gt;\u0026gt;\u0026gt; hello.__code__.co_code b\u0026#39;t\\x00d\\x01|\\x00\\x9b\\x00d\\x02\\x9d\\x03\\x83\\x01\\x01\\x00d\\x00S\\x00\u0026#39; There are more code attributes which you can enlist by \u0026gt;\u0026gt;\u0026gt; dir(hello.__code__)\nFunction Attributes This might sound weird at first but you can do function_name.foo='bar', like it is an object! (it is, though) These are function attributes and it will be associated with the function.\n\u0026gt;\u0026gt;\u0026gt; hello.foo=\u0026#34;bar\u0026#34; \u0026gt;\u0026gt;\u0026gt; hello.foo \u0026#39;bar\u0026#39; How does this work? The famous __dict__ attribute which carries these fellas.\n\u0026gt;\u0026gt;\u0026gt; hello.__dict__ {\u0026#39;foo\u0026#39;: \u0026#39;bar\u0026#39;} I personally have not yet come across any use for this feature. There are some explained in this feature\u0026rsquo;s proposal at PEP 232\nThe Cloud Functions Cloud functions, as the name suggest, run a unit of code, that is a function. Here is what a Hello World would look like\ndef hello_world(request): \u0026#34;\u0026#34;\u0026#34;Responds to any HTTP request. Args: request (flask.Request): HTTP request object. Returns: The response text or any set of values that can be turned into a Response object using `make_response \u0026lt;http://flask.pocoo.org/docs/1.0/api/#flask.Flask.make_response\u0026gt;`. \u0026#34;\u0026#34;\u0026#34; request_json = request.get_json() if request.args and \u0026#39;message\u0026#39; in request.args: return request.args.get(\u0026#39;message\u0026#39;) elif request_json and \u0026#39;message\u0026#39; in request_json: return request_json[\u0026#39;message\u0026#39;] else: return \u0026#39;Hello World!\u0026#39; A cloud function should be expecting a Flask\u0026rsquo;s request object as a parameter. This function will be accessible under https://\u0026lt;some-domain\u0026gt;.com/hello_world. The endpoint name is the same as the function name\nNow, of course, you won\u0026rsquo;t have just one function and you would want to test/run these functions locally. Since the injected request parameter is of flask, there needs to be a way to run this function with flask locally\nFlask While working with flask, functions are generally decorated with flask\u0026rsquo;s @app.route decorator and flask would be injecting parameters to that functions if any. HTTP request\u0026rsquo;s content is accessed by a global variable like from flask import request while the cloud function expects it as a parameter to function.\nTo overcome this, we can create a function that wraps the google cloud\u0026rsquo;s above function. Something like\nfrom flask import Flask, request app=Flask(\u0026#34;test\u0026#34;) @app.route(\u0026#34;/hello_world\u0026#34;) def hello_test(): return hello_world(request) Sweet! This works as expected. But there is one issue though. If there are a lot of google functions, an equal number of test functions will be required.\nEnters the Metaprogrammer Python is a dynamic language, so let\u0026rsquo;s make use of it. Here\u0026rsquo;s the plan\nDiscover the google functions dynamically Create wrapper function which generates flask compatible functions out of step 1 above Register them to our test flask app dynamically Let\u0026rsquo;s Discover If all the functions are in one file, discovering them would look like this. We import the module which contains all the functions, we go through each variable in that module and take a note of all the callable ones (ie: the functions)\n# suppose all the functions are defined in main.py local_vars = importlib.import_module(\u0026#34;main\u0026#34;).__dict__ for name, value in local_vars.items(): if callable(value): print(name) Wrapper function GCF\u0026rsquo;s function takes request as a parameter which would be incompatible with the function that flask\u0026rsquo;s route decorator takes. For this, what we can do is create a wrapper function which will call our original google function. And original function takes imported request arg. So make_function here is a closure.\nfrom flask import request app = flask.Flask(__name__) def make_function(f): def _function(): return f(request) return _function wrapped_hello_world = make_function(hello_world) Register Routes Once we have the wrapped function, we can register it with flask.\napp.route(\u0026#34;/hello_world\u0026#34;)(wrapped_hello_world) This works well. But as soon as the second wrapper function is registered, it greets us with\n\u0026gt;\u0026gt;\u0026gt; app.route(\u0026#34;/hi\u0026#34;)(make_function(hi)) Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;/home/sanket/venv/lib/python3.8/site-packages/flask/app.py\u0026#34;, line 1315, in decorator self.add_url_rule(rule, endpoint, f, **options) File \u0026#34;/home/sanket/venv/lib/python3.8/site-packages/flask/app.py\u0026#34;, line 98, in wrapper_func return f(self, *args, **kwargs) File \u0026#34;/home/sanket/venv/lib/python3.8/site-packages/flask/app.py\u0026#34;, line 1282, in add_url_rule raise AssertionError( AssertionError: View function mapping is overwriting an existing endpoint function: _function The error is saying a function called _function already exits for another endpoint. If you look closer, the wrapper returns a function named _function and Flask seems to be using function\u0026rsquo;s name while registering routes. So we cannot have functions with the same name under different routes. Though functions returned by the make_function are different, they have the same name.\nNow scrolling back up, a function has an attribute called __name__. Flask would be using this to get the function\u0026rsquo;s name. Since we know functions we return are different functions, we can change the function\u0026rsquo;s name before returning it.\ndef make_function(f): def _function(): return f(request) _function.__name__ = f.__name__ return _function And then it goes smooth!\nSo here is the final version\nimport importlib from flask import request app = flask.Flask(__name__) def make_function(f): def _function(): return f(request) # setting the name correctly so that flask does not complain _function.__name__ = f.__name__ return _function local_vars = importlib.import_module(\u0026#34;main\u0026#34;).__dict__ for name, value in local_vars.items(): if callable(value): app.route(f\u0026#34;/{name}\u0026#34;)(make_function(value)) app.run() Oh and later one day I realized functools.wraps also renames the function along with couple other things.\n","permalink":"http://localhost:1313/posts/python-metaprogramming-flask-google-cloud-functions/","summary":"Everything in Python is an object. And that includes functions. Let\u0026rsquo;s see what I learned while I was trying to work with Google cloud functions with python runtime.\nPython Functions Since functions too are objects, we can see what all attributes a function contains as following\n\u0026gt;\u0026gt;\u0026gt; def hello(name): ... print(f\u0026#34;Hello, {name}!\u0026#34;) ... \u0026gt;\u0026gt;\u0026gt; dir(hello) [\u0026#39;__annotations__\u0026#39;, \u0026#39;__call__\u0026#39;, \u0026#39;__class__\u0026#39;, \u0026#39;__closure__\u0026#39;, \u0026#39;__code__\u0026#39;, \u0026#39;__defaults__\u0026#39;, \u0026#39;__delattr__\u0026#39;, \u0026#39;__dict__\u0026#39;, \u0026#39;__dir__\u0026#39;, \u0026#39;__doc__\u0026#39;, \u0026#39;__eq__\u0026#39;, \u0026#39;__format__\u0026#39;, \u0026#39;__ge__\u0026#39;, \u0026#39;__get__\u0026#39;, \u0026#39;__getattribute__\u0026#39;, \u0026#39;__globals__\u0026#39;, \u0026#39;__gt__\u0026#39;, \u0026#39;__hash__\u0026#39;, \u0026#39;__init__\u0026#39;, \u0026#39;__init_subclass__\u0026#39;, \u0026#39;__kwdefaults__\u0026#39;, \u0026#39;__le__\u0026#39;, \u0026#39;__lt__\u0026#39;, \u0026#39;__module__\u0026#39;, \u0026#39;__name__\u0026#39;, \u0026#39;__ne__\u0026#39;, \u0026#39;__new__\u0026#39;, \u0026#39;__qualname__\u0026#39;, \u0026#39;__reduce__\u0026#39;, \u0026#39;__reduce_ex__\u0026#39;, \u0026#39;__repr__\u0026#39;, \u0026#39;__setattr__\u0026#39;, \u0026#39;__sizeof__\u0026#39;, \u0026#39;__str__\u0026#39;, \u0026#39;__subclasshook__\u0026#39;] While there are a lot of them, let\u0026rsquo;s look at some interesting ones","title":"Python Metaprogramming: Functions, Flask and Google Cloud Functions"},{"content":"I had a very (seemingly) simple task. Verify my golang http client, talking with an ElasticSearch cluster, is compressing data on wire. Because in trials, there was around 8x data compression and 100ms latency improvement. Sounds simple? Apparently not!\nElasticSearch Side of Things Http compression is enabled by default and it\u0026rsquo;s an easy configuration. Despite it being enabled by default, still added following in config\nhttp.compression: true And verified it works by\n~ $\u0026gt; curl -v http://localhost:9200/_cat/health -H \u0026#34;Accept-Encoding: gzip\u0026#34; \u0026gt; GET /_cat/health HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: localhost:9200 \u0026gt; Accept: */* \u0026gt; Accept-Encoding: gzip \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-type: text/plain; charset=UTF-8 \u0026lt; content-encoding: gzip \u0026lt; content-length: 84 \u0026lt; Client Side of Things We are using olivere/elastic client to talk with ElasticSearch. This client takes a config parameter which is documented as following\nSetGzip(bool) enables or disables compression on the request side. It is disabled by default. Now this setting is supposed to enable compression. It does not. We will come to it. [Point 1]\nAbove ElasticSearch client also accepts an http client. And an HTTP client\u0026rsquo;s transport as well has a compression flag. Here\u0026rsquo;s how an HTTP client looks like, with compression enabled, which is what we had.\ntr := \u0026amp;http.Transport{ MaxIdleConns: 10, IdleConnTimeout: 30 * time.Second, DisableCompression: false, } client := \u0026amp;http.Client{Transport: tr} resp, err := client.Get(\u0026#34;https://example.com\u0026#34;) // Documentation // DisableCompression, if true, prevents the Transport from // requesting compression with an \u0026#34;Accept-Encoding: gzip\u0026#34; // request header when the Request contains no existing // Accept-Encoding value. If the Transport requests gzip on // its own and gets a gzipped response, it\u0026#39;s transparently // decoded in the Response.Body. However, if the user // explicitly requested gzip it is not automatically // uncompressed. DisableCompression bool But how do I verify that the communication indeed was compressing data on the wire?\nInspect Request/Response Headers From ElasticSearch Client I tried printing request response headers right at the place where the ES client performs the request. Pardon the formatting but they looked like following. You can see the request data is zipped but Accept-Encoding is missing. This is one problem that I mentioned. (Perhaps the Vary header is somehow able to achieve that but could not find anything promising). So the library\u0026rsquo;s documentation is bit unclear to me.\nmap[User-Agent:[elastic/5.0.64 (linux-amd64)] Accept:[application/json] Content-Type:[application/json] Content-Encoding:[gzip] Vary:[Accept-Encoding]] Inspect Headers at http.transport Since we enabled compression at transport layer, it is supposed to send relevant headers and compress the payload. Diving into source code of transport, I saw a line setting Accept-Encoding header. req.extraHeaders().Set(\u0026quot;Accept-Encoding\u0026quot;, \u0026quot;gzip\u0026quot;) But printing header did not show that encoding header. Apparently is was set in another variable of struct transportRequest called extra\nreq header at transport: map[Vary:[Accept-Encoding] User-Agent:[elastic/5.0.64 (linux-amd64)] Accept:[application/json] Content-Type:[application/json] Content-Encoding:[gzip]] extra: map[Accept-Encoding:[gzip]] Inspecting Response The same function, which set the above header, receives the response. Printing response\u0026rsquo;s header did not have content encoding.\n// Src code: case re := \u0026lt;-resc: if (re.res == nil) == (re.err == nil) { panic(fmt.Sprintf(\u0026#34;internal error: exactly one of res or err should be set; nil=%v\u0026#34;, re.res == nil)) } if debugRoundTrip { req.logf(\u0026#34;resc recv: %p, %T/%#v\u0026#34;, re.res, re.err, re.err) } if re.err != nil { return nil, pc.mapRoundTripError(req, startBytesWritten, re.err) } fmt.Println(\u0026#34;TRANSPORT RESS-=========-\u0026#34;) fmt.Println(re.res.Header) fmt.Println(\u0026#34;-=========-\u0026#34;) // Outout: map[Content-Type:[application/json; charset=UTF-8]] This is where things get confusing. In the initial curl request, when we set the Accept-Encoding header, the ES server returned gzip response. But when go client set the same header, it did not return gzipped response.\nGoing Further Deep Something seemed wrong, so I went even deeper, to whatever function above transport\u0026rsquo;s roundTrip function was calling. Control seemed to be going to function called readLoop() where we can find following lines scrolling down a bit\nresp.Body = body if rc.addedGzip \u0026amp;\u0026amp; strings.EqualFold(resp.Header.Get(\u0026#34;Content-Encoding\u0026#34;), \u0026#34;gzip\u0026#34;) { fmt.Println(\u0026#34;TRANSPORT RESPPPPPP -=========-\u0026#34;) fmt.Println(resp.Header) resp.Body = \u0026amp;gzipReader{body: body} resp.Header.Del(\u0026#34;Content-Encoding\u0026#34;) resp.Header.Del(\u0026#34;Content-Length\u0026#34;) // Output TRANSPORT RESPPPPPP -=========- map[Content-Type:[application/json; charset=UTF-8] Content-Encoding:[gzip] Content-Length:[128]] HELL YEAH! There they are! The response was decompressed and headers were removed. That is why, at layers above this, we were not seeing the header set.\nSummary We set DisableCompression as false in http client\u0026rsquo;s transport\nTransport, for each request, if not already set, will set Accept-Encoding. (In a variable called extra)\nWhen the response arrives, it\u0026rsquo;ll check if transport added gzip\n3.1 If it did, unzip data 3.2 Remove headers which indicates the zipped response from server.\nSo it works good as a transparent system.\nThe Twist Apparently http.Response struct had a field called Uncompressed which states exactly what I investigated. RTFM!\n// Uncompressed reports whether the response was sent compressed but // was decompressed by the http package. When true, reading from // Body yields the uncompressed content instead of the compressed // content actually set from the server, ContentLength is set to -1, // and the \u0026#34;Content-Length\u0026#34; and \u0026#34;Content-Encoding\u0026#34; fields are deleted // from the responseHeader. To get the original response from // the server, set Transport.DisableCompression to true. Uncompressed bool // Go 1.7s Fin When I read I want off Mr. Golang\u0026rsquo;s Wild Ride, the author\u0026rsquo;s criticism felt valid to me. Later I realized even http timeouts are not as simple as they sound, this piece on cloudflare blog explains that. And now this abstraction for gzip! I feel the convenience that the language gives is good, until it\u0026rsquo;s not!\n","permalink":"http://localhost:1313/posts/golang-http-gzip-compression/","summary":"I had a very (seemingly) simple task. Verify my golang http client, talking with an ElasticSearch cluster, is compressing data on wire. Because in trials, there was around 8x data compression and 100ms latency improvement. Sounds simple? Apparently not!\nElasticSearch Side of Things Http compression is enabled by default and it\u0026rsquo;s an easy configuration. Despite it being enabled by default, still added following in config\nhttp.compression: true And verified it works by","title":"Golang Http Client and Compression"},{"content":"Only if it were as easy as installing debug symbols, attach the process with gdb and py-bt! So we have a python agent, which distributes files, running across the fleet. And on some random hosts, it went haywire! On those set of hosts, the process was using 100% of CPU and not doing anything meaningful work. Restarting the process fixes the problem. I had worked on debugging a stuck process, but this was the opposite. Time to dive deep.\nFirst Obvious Step: strace ... open(\u0026#34;/\u0026#34;, O_RDONLY|O_CLOEXEC) = 4 fstat(4, {st_mode=S_IFDIR|0555, st_size=4096, ...}) = 0 close(4) = 0 open(\u0026#34;/\u0026#34;, O_RDONLY|O_CLOEXEC) = 4 fstat(4, {st_mode=S_IFDIR|0555, st_size=4096, ...}) = 0 close(4) = 0 open(\u0026#34;/\u0026#34;, O_RDONLY|O_CLOEXEC) = 4 fstat(4, {st_mode=S_IFDIR|0555, st_size=4096, ...}) = 0 close(4) = 0 ... So for some reason, the process is constantly opening root dir and calling fstat on it. Why? I don\u0026rsquo;t know!\npdb? Cannot work, because\nI have not worked with it before (I really should) Problem is not reproducible, the 100% CPU usage appeared on random hosts so stopping a process is not an option. Attach gdb to the process? Sure! I have heard it works with python too with extensions. There is this official DebuggingWithGdb guide which suggests vanilla steps of installing debug symbols with yum or apt-get. If life were that simple. The thing is we do not run on system python, the one that you find under /usr/bin/python. There is a separately compiled and packaged python installed on hosts at a non-standard location, so grabbing random debug symbols from the internet would not work.\nSearched on slack chat history (poor man\u0026rsquo;s stackoverflow!) if someone has tried this adventure earlier, and some people sure had. Got a link to debug symbol RPM from chat history and installed that\n$ sudo yum install python37-debuginfo.x86_64 And to get where exactly did that RPM install the debug binary (remember custom in-house built python? It does not install files at any standard location)\n$ sudo rpm -ql python37-debuginfo | less Search for a bin/python3.debug or something similar in that. Once you get the path to debug binary, it would be as simple as this, right?\n$ sudo gdb /usr/lib/debug/foo/bar/python/3.7.1/bin/python3.7.debug \u0026lt;PID\u0026gt; Wrong! Turns out, when we attach gdb using that debug binary, it cannot load most symbols and errors out like\n$ sudo gdb /usr/lib/debug/foo/bar/python/3.7.1/bin/python3.7.debug \u0026lt;PID\u0026gt; ... \u0026lt;http://www.gnu.org/software/gdb/bugs/\u0026gt;... Reading symbols from /usr/lib/debug/foo/bar/python/3.7.1/bin/python3.7.debug...done. Attaching to program: /usr/lib/debug/foo/bar/python/3.7.1/bin/python3.7.debug, process 3536 Reading symbols from /foo/bar/python/3.7.0/lib/libpython3.7m.so.1.0...(no debugging symbols found)...done. Loaded symbols for /foo/bar/python/3.7.0/lib/libpython3.7m.so.1.0 Reading symbols from /lib64/libpthread.so.0...(no debugging symbols found)...done. [Thread debugging using libthread_db enabled] Loaded symbols for /lib64/libpthread.so.0 Reading symbols from /lib64/libdl.so.2...(no debugging symbols found)...done. Loaded symbols for /lib64/libdl.so.2 Reading symbols from /lib64/libutil.so.1...(no debugging symbols found)...done. Loaded symbols for /lib64/libutil.so.1 Reading symbols from /lib64/librt.so.1...(no debugging symbols found)...done. Loaded symbols for /lib64/librt.so.1 Reading symbols from /lib64/libm.so.6...(no debugging symbols found)...done. Loaded symbols for /lib64/libm.so.6 Reading symbols from /lib64/libc.so.6...(no debugging symbols found)...done. Missing separate debuginfos, use: debuginfo-install python37_3_7_0-0.0.9-3.7.0.el6.x86_64 Despite given the debug binary, gdb somehow is not able to load symbols. Meaning python binary that is run as a process and the debug binary that we provided do not correspond to the same version or build of python. The command on the last line did not work either saying package not found. Let\u0026rsquo;s first confirm the running python binary\n$ ps auxww | grep \u0026lt;PID\u0026gt; root \u0026lt;PID\u0026gt; 81.1 0.0 227152 40560 ? R 07:50 158:57 /foo/bar/python/3.7/bin/python3.7 /abc/def/python-app.pex $ sudo yum whatprovides /foo/bar/python/3.7/bin/python3.7 python37_3_7_0-0.0.9-3.7.0.el6.x86_64 : Python 3.7.0 from straightforward source build [On a side note, the running pex file is a PythonEXecutable, something like zipped virtualenv]\nApparently, the binary that runs python is 3.7.0 and build version 0.0.9 (not sure what exactly does that mean) and that version did not match with original debug binary I installed earlier. To find corresponding debug binary, which would not be available on the internet, I searched on local artifact store (remember debuginfo-install was not able to find it) where there was a promising-looking python37-debuginfo_linux_rhel6_x86_64-0.0.9-3.7.0.el6.x86_64.rpm which exactly corresponds to the binary version that process is running. Okay, now using same rpm -ql command we find the debug binary and run\n$ sudo gdb /usr/lib/debug/foo/bar/python/3.7.0/bin/python3.debug \u0026lt;PID\u0026gt; ... Reading symbols from /usr/lib/debug/foo/bar/python/3.7.0/bin/python3.debug...done. Attaching to program: /usr/lib/debug/foo/bar/python/3.7.0/bin/python3.debug, process 3536 Reading symbols from /foo/bar/python/3.7.0/lib/libpython3.7m.so.1.0...Reading symbols from /usr/lib/debug/foo/bar/python/3.7.0/lib/libpython3.7m.so.1.0.debug...done. done. Loaded symbols for /foo/bar/python/3.7.0/lib/libpython3.7m.so.1.0 So that symbols are getting loaded. Now just run py-bt and get backtrace, right? Wrong!\n(gdb) py-bt Undefined command: \u0026#34;py-bt\u0026#34;. Try \u0026#34;help\u0026#34;. Okay, gdb. That was supposed to work! Why did it not? Turns out, all these python related macros does not come built-in but they are kind of added at runtime. CPython interpreter ships with a script that gdb loads and these python magic macros work. That script in an ideal world, loads automatically and you don\u0026rsquo;t need to do anything. But remember where we live :) The script is called python37-gdb.py but it was for some strange reason not shipped with the debug binary installed. Again, in an ideal world, the script is supposed to come with the package and should be autoloaded. This is where you can get it from python source code Tools/gdb/libpython.py It is named a bit differently. During build time it gets renamed I guess. Anyway, here\u0026rsquo;s how you load the file and then the macros would work! Yeeeey!!!\n(gdb) source /path/to/libpython.py Now process was getting paused at some memcpy function. From original strace output it seemed to be getting stuck in an infinite open calls. So that\u0026rsquo;s our pointer. We set a breakpoint at open call and then take a backtrace. And we get to know who\u0026rsquo;s doing it!\n(gdb) break open Breakpoint 1 at 0x3a60a0ef70 (gdb) c Continuing. Breakpoint 1, 0x0000003a60a0ef70 in open64 () from /lib64/libpthread.so.0 (gdb) py-bt Traceback (most recent call first): \u0026lt;built-in method open of module object at remote 0x7fd738659db8\u0026gt; File \u0026#34;/foo/bar/python/3.7/lib/python3.7/zipfile.py\u0026#34;, line 180, in is_zipfile with open(filename, \u0026#34;rb\u0026#34;) as fp: File \u0026#34;.bootstrap/pex/third_party/__init__.py\u0026#34;, line 104, in containing File \u0026#34;.bootstrap/pex/third_party/__init__.py\u0026#34;, line 156, in _iter_importables (frame information optimized out) (frame information optimized out) \u0026lt;built-in method exec of module object at remote 0x7fd738709c28\u0026gt; (frame information optimized out) (frame information optimized out) Cool stuff! We know form pex\u0026rsquo;s third party module we\u0026rsquo;re making those calls. Let\u0026rsquo;s go frame by frame\n(gdb) py-up #12 Frame 0x7fd73546c230, for file /foo/bar/python/3.7/lib/python3.7/zipfile.py, line 180, in is_zipfile (filename=\u0026#39;/\u0026#39;, result=False) with open(filename, \u0026#34;rb\u0026#34;) as fp: (gdb) py-up #15 Frame 0x7fd737c02d30, for file .bootstrap/pex/third_party/__init__.py, line 104, in containing (cls=\u0026lt;type at remote 0x26a2d48\u0026gt;, root=\u0026#39;/abc/def/python-app.pex/.bootstrap\u0026#39;, prefix=\u0026#39;//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////...(truncated) The second frame looks suspicious. The function containing has an argument prefix \u0026lsquo;/////////\u0026hellip;.\u0026rsquo; and we also saw in our initial strace output the program opening \u0026lsquo;/\u0026rsquo; repeatedly. So the dots connect. Next? Go to pex\u0026rsquo;s github repo and search for reported issues and there indeed was PR#638 Here\u0026rsquo;s the code snippet from the earlier version\nclass _ZipIterator(namedtuple(\u0026#39;_ZipIterator\u0026#39;, [\u0026#39;zipfile_path\u0026#39;, \u0026#39;prefix\u0026#39;])): @classmethod def containing(cls, root): prefix = \u0026#39;\u0026#39; path = root while path: if zipfile.is_zipfile(path): return cls(zipfile_path=path, prefix=prefix + os.sep if prefix else \u0026#39;\u0026#39;) prefix = os.path.join(prefix, os.path.basename(path)) path = os.path.dirname(path) raise ValueError(\u0026#39;Could not find the zip file housing {}\u0026#39;.format(root)) So this method is passed a zip argument (the pex python file) and zipfile.is_zipfile will return true and program proceeds happily. But when does it not, it modifies path as a parent dir using os.path.dirname(...) and while loop continues. The parent dir is not zip either, so it goes to its parent dir. And so it goes on till path is /. Now parent of / is / itself so while loop continues infinitely and we see 100% CPU usage and process doing nothing else. The issue is explained here by author who raised a fix.\nThe Root Cause So ideally the zip (.pex) file is supposed to exist. This particular scenario happened while we were moving away from pex. We install a newer packaged file and restart the process. But for some reason (which we will not discuss here) the process was not getting killed and it continued running with a pex file which did not exist anymore (hence is_zipfile fails) because of the upgrade to new packaging.\n","permalink":"http://localhost:1313/posts/debug-running-python-process/","summary":"Only if it were as easy as installing debug symbols, attach the process with gdb and py-bt! So we have a python agent, which distributes files, running across the fleet. And on some random hosts, it went haywire! On those set of hosts, the process was using 100% of CPU and not doing anything meaningful work. Restarting the process fixes the problem. I had worked on debugging a stuck process, but this was the opposite.","title":"Debugging a Running Python Process"},{"content":"We faced a memory leak in production and I wrote about it in this blog post. A while back, I somewhere came across the open Call for Proposals for Pycon India 2019 and I submitted a talk titled Let's Hunt a Memory Leak. It got selected and I had to prepare! While learning python internals and especially memory related behaviour, I also wrote about werid behaviour with python 2 and integers. The PyCon finally happened and it was pretty fun to learn new things and to deliver it!\nLet\u0026rsquo;s Hunt a Memory Leak The talk\u0026rsquo;s description looked something like:\nPython being a high level interpreted language, it never bothers us to deal with garbage collection. However, because we do not explicitly free memory like we do in C language, python has to clean it for us and hence there is garbage collection involved with python. While garbage collection works most of the times and never becomes an issue, when there is a memory leak in the code, we have no choice but to dig deeper to uncover it. In this talk we will first look at how python manages memory and how does garbage collection works with python. We will look at some hands on examples to confirm the python memory management behavior and we will see why is it important to be aware about this behavior. Further, we will look at a simulated memory leak scenario which we faced in production with Flask environment and we will look at one of the ways we used to hunt it down.\nSlides and Recording Slides YouTube A couple of images :)\n","permalink":"http://localhost:1313/posts/pycon-lets-hunt-memory-leak/","summary":"We faced a memory leak in production and I wrote about it in this blog post. A while back, I somewhere came across the open Call for Proposals for Pycon India 2019 and I submitted a talk titled Let's Hunt a Memory Leak. It got selected and I had to prepare! While learning python internals and especially memory related behaviour, I also wrote about werid behaviour with python 2 and integers.","title":"PyCon19 India: Let's Hunt a Memory Leak"},{"content":"In Detecting Memory Leak in Python, scenarios were shown where python does not release memory when we created a huge list and then explicitly deleted it. The given explanation was that python caches these objects and does not release the memory back to OS. Let\u0026rsquo;s take a deeper look at what exactly happens!\nUpdate: I gave a talk at PyCon 2019 on a similar subject, if you prefer detailed explanation in video format, checkout PyCon19 India: Let\u0026rsquo;s Hunt a Memory Leak or just scroll down to the bottom of the page.\nPython 2.7.15 (default, Jan 12 2019, 21:07:57) [GCC 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import os, psutil, gc, time \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; l=[i for i in range(100000000)] \u0026gt;\u0026gt;\u0026gt; print(psutil.Process(os.getpid()).memory_info()) pmem(rss=3244871680L, vms=7824240640L, pfaults=1365384, pageins=460) \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; del l \u0026gt;\u0026gt;\u0026gt; print(psutil.Process(os.getpid()).memory_info()) pmem(rss=2509352960L, vms=6964514816L, pfaults=1381131, pageins=460) After deleting the list explicitly, the memory usage still is at 2.5G. That means the integers are still floating around. (see what I did there?)\nPython Objects Everything in python is an object, including our beloved integer.\n// Include/object.h /* Nothing is actually declared to be a PyObject, but every pointer to * a Python object can be cast to a PyObject*. This is inheritance built * by hand. Similarly every pointer to a variable-size Python object can, * in addition, be cast to PyVarObject*. */ typedef struct _object { PyObject_HEAD } PyObject; // Include/intobject.h typedef struct { PyObject_HEAD long ob_ival; } PyIntObject; So PyIntObject is just a wrapper around a C type long, with added python specific HEAD struct which contains additional information like reference count, pointer to various methods which will be called when we do print, delete, get, set, etc on the object.\nLet\u0026rsquo;s try to allocate an integer: We execute a really simple python assignment statement and we can see that the function PyObject * PyInt_FromLong(long ival), which takes a long variable and returns a PyObject (of course). From this point onwards, we will dive deep into two things:\nLook deeper into the said function above, and see how exactly an object is allocated Look into what happens when we delete it and try to understand the behavior we saw in the beginning Also: did you look at the call stack? The int object is created when the code is being compiled to bytecode! (The AST [abstract syntax tree] function calls) And not at during run time.\n1. Creating The Int Object PyObject * PyInt_FromLong(long ival) { register PyIntObject *v; // TRIMMED SOME CODE HERE if (free_list == NULL) { if ((free_list = fill_free_list()) == NULL) return NULL; } /* Inline PyObject_New */ v = free_list; free_list = (PyIntObject *)Py_TYPE(v); (void)PyObject_INIT(v, \u0026amp;PyInt_Type); v-\u0026gt;ob_ival = ival; return (PyObject *) v; } The code above is responsible for creating an integer object from given long value. The trimmed code in the snipped above handles small integers separately. Leaving some links in the bottom if you\u0026rsquo;re interested in the specifics.\nSo the first thing that happens here is filling some sort of free list. Let\u0026rsquo;s take a look at what it is.\n/* Integers are quite normal objects, to make object handling uniform. (Using odd pointers to represent integers would save much space but require extra checks for this special case throughout the code.) Since a typical Python program spends much of its time allocating and deallocating integers, these operations should be very fast. Therefore we use a dedicated allocation scheme with a much lower overhead (in space and time) than straight malloc(): a simple dedicated free list, filled when necessary with memory from malloc(). block_list is a singly-linked list of all PyIntBlocks ever allocated, linked via their next members. PyIntBlocks are never returned to the system before shutdown (PyInt_Fini). free_list is a singly-linked list of available PyIntObjects, linked via abuse of their ob_type members. */ #define BLOCK_SIZE 1000 /* 1K less typical malloc overhead */ #define BHEAD_SIZE 8 /* Enough for a 64-bit pointer */ #define N_INTOBJECTS ((BLOCK_SIZE - BHEAD_SIZE) / sizeof(PyIntObject)) struct _intblock { struct _intblock *next; PyIntObject objects[N_INTOBJECTS]; }; typedef struct _intblock PyIntBlock; static PyIntBlock *block_list = NULL; static PyIntObject *free_list = NULL; static PyIntObject * fill_free_list(void) { PyIntObject *p, *q; /* Python\u0026#39;s object allocator isn\u0026#39;t appropriate for large blocks. */ p = (PyIntObject *) PyMem_MALLOC(sizeof(PyIntBlock)); if (p == NULL) return (PyIntObject *) PyErr_NoMemory(); ((PyIntBlock *)p)-\u0026gt;next = block_list; block_list = (PyIntBlock *)p; /* Link the int objects together, from rear to front, then return the address of the last int object in the block. */ p = \u0026amp;((PyIntBlock *)p)-\u0026gt;objects[0]; q = p + N_INTOBJECTS; while (--q \u0026gt; p) Py_TYPE(q) = (struct _typeobject *)(q-1); Py_TYPE(q) = NULL; return p + N_INTOBJECTS - 1; } There are some spoilers in the comment in the code above. There are two new variables introduced above. The block_list and the free_list. block_list is a singly linked list of bunch of PytIntObjects. free_list is the pointer to next free object in this said bunch of Int objects in a block. So here is what happens in plain English:\nAllocate a new PyIntBlock Add the block at the beginning of the block_list linked list. Now link the linked list of those bunch of Int objects. Return the address of last Int Object. Here block_list is connected using *next and free_list is linked using *ob_type which is included in each Python object\u0026rsquo;s header.\nNow if we went back to the function PyInt_FromLong above, what we\u0026rsquo;re doing is attaching our new long variable to the next free slot in free_list and return it.\nWhy? Why do all these convoluted things? As the comment said, int objects are frequently allocated in python. So this approach allocates a bunch of int objects in one go (N_INTOBJECTS many, 24 in other words). So in one single alloc call, we reserve space for 24 objects. So that next 23 int allocations can go relatively faster!!\nPyIntBlock PyIntBlock +-----------------+ +-------------------+ | | | | | *next+-----------\u0026gt;+ *next +---------\u0026gt; block_list+-\u0026gt;+ objects | | | | + | | + | +-----------------+ +-------------------+ | | v | +-----+----------+ v NULL \u0026lt;---------+*ob_type | | ob_ival=NAN |PyIntObject | | +----+-----------+ ^ | + 21 similar objects ^ +*free_list | | +----+-----------+ | | *ob_type +\u0026lt;------+ | ob_ival=NAN |PyIntObject | | +----+-----------+ ^ | | +----+-----------+ | *ob_type | | ob_ival=11 |PyIntObject | | +----------------+ 2. Deleting The Int Object So that was how a python object is allocated. Let\u0026rsquo;s see what happens when we delete it We can see static void int_dealloc(PyIntObject *v) is called. But how does python know what function to call? It is stored in the definition of PyIntObject specifically in the HEAD part. Here\u0026rsquo;s the function:\nint_dealloc(PyIntObject *v) { if (PyInt_CheckExact(v)) { Py_TYPE(v) = (struct _typeobject *)free_list; free_list = v; } else Py_TYPE(v)-\u0026gt;tp_free((PyObject *)v); } It is relatively simple, if the given object is indeed int, then just move the free_list pointer to the deleting object.\nPyIntBlock PyIntBlock +-----------------+ +-------------------+ | | | | | *next+-----------\u0026gt;+ *next +---------\u0026gt; block_list+-\u0026gt;+ objects | | | | + | | + | +-----------------+ +-------------------+ | | v | +-----+----------+ v NULL \u0026lt;---------+*ob_type | | ob_ival=NAN |PyIntObject | | +----+-----------+ ^ | + 21 similar objects ^ | +----+-----------+ | *ob_type | | ob_ival=NAN |PyIntObject | | +----+-----------+ ^ | *free_list | + +----+-----------+ | | *ob_type +\u0026lt;----------+ | ob_ival=NAN |PyIntObject | | +----------------+ Bottomline: As we saw, we allocate a bunch of int objects (inside a block) when we create a new int and there\u0026rsquo;s not enough space in the current block. But when we delete, we just move the free_list pointer around. And not call actual dealloc function to free any memory. So that explains the initial behavior!\nAlso, floats work the same way.\nOther good reads on python2 and integers: Python Internals – Integer object pool Int Object in Python Recording of the talk: ","permalink":"http://localhost:1313/posts/python-2-integers/","summary":"In Detecting Memory Leak in Python, scenarios were shown where python does not release memory when we created a huge list and then explicitly deleted it. The given explanation was that python caches these objects and does not release the memory back to OS. Let\u0026rsquo;s take a deeper look at what exactly happens!\nUpdate: I gave a talk at PyCon 2019 on a similar subject, if you prefer detailed explanation in video format, checkout PyCon19 India: Let\u0026rsquo;s Hunt a Memory Leak or just scroll down to the bottom of the page.","title":"Curious Case of Python 2 and Integers"},{"content":"This happened a while back, sometime in the year of 2017. I was on-call for the week and it was the weekend. Usually, things are quiet over the weekends but not that weekend. Pages started coming frequently affecting different hosts. The alert was titled\nWARNING: Memory usage is more than 80% And that was just not one or set of hosts. This started coming from random hosts from across the infrastructure. Be it a data host, an app host, some ops specific hosts. Pretty random. Something was obviously wrong\nInitial Symptoms and Observations: Alerts were triggered from random hosts Checking Grafana memory usage graphs clearly indicated an increase in memory usage. This also included hosts like mail servers which had consistent memory usage for months and suddenly they too started using more memory The time when memory usage started going up across hosts was close enough. That means some change triggered this. Next Steps: There were two obvious things from the initial look at the situation.\nSomething is causing the memory leak Some change triggered this This is how it went finding answers for the above two questions\nWhat is using the memory? The most interesting hosts were the mail servers I mentioned earlier. They had nothing significant running on them and yet memory usage touch a GB. Tried finding processes using the highest amount of memory but they all were using a nominal amount of memory.\nsanket@tfs:~$ ps auxw --sort rss | tail Even tried totaling RSS of all the processes running on that host but that amount was still very very less than total memory used by the server\nsanket@tfs:~$ ps auxw | awk \u0026#39;BEGIN {sum=0} {sum +=$6} END {print sum/1024}\u0026#39; So this still wasn\u0026rsquo;t telling where exactly the memory is being used. Now, what tells you the comprehensive memory usage picture? /proc/meminfo\nsanket@tfs:~$ cat /proc/meminfo ... MemTotal: 3524440 kB MemFree: 477176 kB MemAvailable: 2270304 kB ... Slab: 538216 kB SReclaimable: 482496 kB SUnreclaim: 55720 kB ... So apparently, most of the leaking memory was listed under Slab section. Initially had no idea what does it mean but further searches on revealed that it\u0026rsquo;s a cache for kernel\u0026rsquo;s data structures. To see what specific data structures are taking that space\nsanket@tfs:~$ sudo slabtop --once ... OBJS ACTIVE USE OBJ SIZE SLABS OBJ/SLAB CACHE SIZE NAME 71883 49685 0% 0.19K 3423 21 313692K dentry ... So something called dentry was eating all the memory. What is it? It is a directory entry which is cached. When you look up for a file or a directory, the operation goes iteratively scanning each component of the path. For example, to get /etc/passwd, it has to first find dir / and list its contents, after finding etc in it, it has to list contents of etc and so on. When this lookup is finally done, the kernel will cache it because the above iterative resolution has to do expensive disk reads. (If you want to know more on this, checkout talk I gave on filesystem)\nNow, something was filling up the cache. Wasn\u0026rsquo;t sure what it was but having found what was using the memory, a fix was also found. Because it\u0026rsquo;s a cache, it should be (moderately?) safe to drop it.\nroot@tfs:~# echo 3 \u0026gt; /proc/sys/vm/drop_caches So this fixes the symptom at least. We did not need to restart the servers anymore to reclaim the memory.\nWho triggered this? One of the symptoms of the problem was that it was spread sitewide. What else works site-wide? The configuration management system. Saltstack.\nGoing through git logs of salt repo, there was one commit that was correlating with increase in memory usage.\nWhat was the change? An API that each host contacts to (called from custom salt grain module if I remember correctly) was moved from HTTP to HTTPS. But why would switching to HTTPS cause a memory leak?!?!\nThe Root Cause: Running that API in a loop on a host with ample free memory reproduced the problem. Putting it under strace revealed that it\u0026rsquo;s trying to access lot of non-existing files. Which also justifies the increase in dentry cache usage.\nWas not exactly sure why would it try to access those files but quick search on the internet landed me on StackExchange post Unusually high dentry cache usage which revealed that it was due to a bug in NSS (Network Security Services, library developed by Mozilla, used when we do SSL stuff)\nThe fix was to upgrade the library or temporarily adjust cache pressure.\nFin Guess who learned something new that weekend! The feeling after solving the mystery is so satisfying :D\nNote: The command executions shown above and its results are not from actual affected servers. The numbers are made up.\n","permalink":"http://localhost:1313/posts/site-wide-memory-leak-on-call-story/","summary":"This happened a while back, sometime in the year of 2017. I was on-call for the week and it was the weekend. Usually, things are quiet over the weekends but not that weekend. Pages started coming frequently affecting different hosts. The alert was titled\nWARNING: Memory usage is more than 80% And that was just not one or set of hosts. This started coming from random hosts from across the infrastructure.","title":"Site Wide Memory Leak: An On-Call Story"},{"content":"In the first post on this blog, I wrote about a tiny distributed filesystem I made in python for educational purpose. This year, I had a chance to use it in a talk delivered at SRECon 19 Asia. The title was\nLet\u0026rsquo;s Build a Distributed File System The talk was listed under something called Core Principles track and Talks in this track will focus on providing a deep understanding of how technologies we use everyday function and why it's important to know these details when supporting and scaling your infrastructure. If I were to pull up abstract submitted to Usenix\nLet\u0026rsquo;s explore something that we use and rely on every day. The file systems. Typical and distributed. We first will look at a typical file system, the architectural components and how they all work together when you perform a read or write. We then will take those components and evolve that into a distributed file system architecture. While the architectures we\u0026rsquo;ll explore will not be of a specific file system, they will be generic enough to be relatable with many file system implementations that exist today. We also will then implement a tiny distributed file system in Python to see all those components playing together in action. Please note that this will be a very simple, minimal example, not suitable for real usage. If you are a file system hacker, this session will be too basic for you.\nSlides GitHub and Recording Slides GitHub YouTube Couple of images :)\n","permalink":"http://localhost:1313/posts/lets-build-distributed-filesystem/","summary":"In the first post on this blog, I wrote about a tiny distributed filesystem I made in python for educational purpose. This year, I had a chance to use it in a talk delivered at SRECon 19 Asia. The title was\nLet\u0026rsquo;s Build a Distributed File System The talk was listed under something called Core Principles track and Talks in this track will focus on providing a deep understanding of how technologies we use everyday function and why it's important to know these details when supporting and scaling your infrastructure.","title":"SRECon19 Asia: Let's Build a Distributed File System"},{"content":"I have been attending LSPE [Large Scale Production Engineering] Meetup for last two years. And for the last one, I decided to give it back to the community. I conducted a hands-on session titled:\nServerless meets CI/CD The session briefly introduced what is Serverless and CD/CD and why should you be concerned about it. We then went hands-on with AWS Lambda as serverless platform and Bitbucket Pipelines for CI/CD. Started from making a Hello World! flask app and using zappa we deployed it on AWS Lambda. Then introduced Bitbucket Pipelines and using it, we enabled CI/CD. So what we achieved in the end was the fully automated pipeline that would deploy the flask app to AWS Lambda with just a commit in Bitbucket repo.\nAttaching a Wiki/HowTo step-by-step guide as well here along with slides.\nSlides: Serverless Meets CI/CD GitHub Wiki: github.com/sanketplus/serverless-python Couple of images :)\n","permalink":"http://localhost:1313/posts/serverless-meets-ci-cd/","summary":"I have been attending LSPE [Large Scale Production Engineering] Meetup for last two years. And for the last one, I decided to give it back to the community. I conducted a hands-on session titled:\nServerless meets CI/CD The session briefly introduced what is Serverless and CD/CD and why should you be concerned about it. We then went hands-on with AWS Lambda as serverless platform and Bitbucket Pipelines for CI/CD. Started from making a Hello World!","title":"Serverless Meets CI/CD"},{"content":"In production, a memory leak will not always bubble up. And there could be multiple reasons behind it. You may not be getting enough traffic. Frequent deployments. No hard memory usage limit set. Or mix of them.\nThe flask app we had to debug had same characteristics. It never had huge surge of traffic and there would be multiple deployments over week. Although it had cgroup memory usage limit, it had some room to grow and the leak never appeared. Until we decided to implement cache warmer which would be generating significant traffic and there it goes, uWSGI processes getting killed by OOM Killer!\nUpdate: I gave a talk at PyCon 2019 on the same subject, if you prefer detailed explanation in video format, checkout PyCon19 India: Let\u0026rsquo;s Hunt a Memory Leak or just scroll down to the bottom of the page.\nA Word on Python Memory Management: Python does memory management on its own and it\u0026rsquo;s completely abstracted from user. It generally is not needed to know how is it done internally but when your workers are dying, you gotta know.\nApparently, when certain primitive types of object goes out of scope or you delete it explicitly with del, the memory is not released back to OS and it would still be accounted for the python process. The now free objects would go to something called freelist and would still stay on heap. It is cleared only when garbage collection of highest generation happens. 1\nHere we are allocating list of ints and then explicitly deleting it. We will see mem usage with and without GC.\nimport os, psutil, gc, time l=[i for i in range(100000000)] print(psutil.Process(os.getpid()).memory_info()) del l #gc.collect() print(psutil.Process(os.getpid()).memory_info()) The results would look like:\n# without GC: pmem(rss=3268038656L, vms=7838482432L, pfaults=993628, pageins=140) pmem(rss=2571223040L, vms=6978756608L, pfaults=1018820, pageins=140) # with GC: pmem(rss=3268042752L, vms=7844773888L, pfaults=993636, pageins=0) pmem(rss=138530816L, vms=4552351744L, pfaults=1018828, pageins=0) Look that by deleting, we are going from 3.2G -\u0026gt; 2.5G so still a lots of stuff(mostly int objects) lying around heap. If we also trigger a GC, it goes from 3.2G -\u0026gt; 0.13G. So it memory was not given back to OS until a GC was triggered.\nThis is just an idea on how does python does memory management. Attaching some reference links as well for more details on how is memory management actually done.\nConfirm there\u0026rsquo;s a leak: To give bit more context on application which was leaking memory, it was a flask app with traffic mostly on one API endpoint with different parameters.\nWith the basic knowledge of how python does memory management, we triggered explicit GC with each response sent back. Something like this:\n@blueprint.route(\u0026#39;/app_metric/\u0026lt;app\u0026gt;\u0026#39;) def get_metric(app): response, status = get_metrics_for_app(app) gc.collect() return jsonify(data=response), status Even with this gc collection, memory was still gradually increasing with traffic. Meaning? IT\u0026rsquo;S A LEAK!!\nStarting with heap dump: So we had this uWSGI worker with high memory utilization. I was not aware of any memory profiler which would attach to a running python process and give real-time object allocations. I still am not aware of any such profiler. (A cool project idea?) So a heap dump was taken to analyze what all is lying there. Here\u0026rsquo;s how it was done:\n$\u0026gt; hexdump core.25867 | awk \u0026#39;{printf \u0026#34;%s%s%s%s\\n%s%s%s%s\\n\u0026#34;, $5,$4,$3,$2,$9,$8,$7,$6}\u0026#39; | sort | uniq -c | sort -nr | head 1209344 0000000000000000 748192 0000000000000001 200862 ffffffffffffffff 177362 00007f01104e72c0 169971 145219 00007f01104e0c70 140715 fffffffffffffffc 138963 fffffffffffffffa 136849 0000000000000002 99910 00007f01104d86a0 It is number of symbols and symbol address mapping. To know what that object is actually:\n$\u0026gt; gdb python core.25867 (gdb) info symbol 0x00007f01104e0c70 PyTuple_Type in section .data of /export/apps/python/3.6.1/lib/libpython3.6m.so.1.0 (gdb) info symbol 0x00007f01104d86a0 PyLong_Type in section .data of /export/apps/python/3.6.1/lib/libpython3.6m.so.1.0 So there are lot of tuples and longs (int objects) on the heap. So what? Heap dump does tell what is there on heap but it does not tell who put it there. So this was useless.\nLet\u0026rsquo;s track memory allocations: There was no other way but to track memory allocations. There are number of python modules available which helps you do that. But they need to be installed separately and since 3.4 python comes bundling tracemalloc 2. Tracemalloc tracks memory allocations and point it to line/module where object was allocated with size. You can also take snapshots at random point in code path and compare memory difference between those two points.\nPerfect! Since it\u0026rsquo;s a flask app, it\u0026rsquo;s supposed to be stateless and there should not be permanent memory allocations between API calls (which was not the case here). So how do we take snapshot of memory and track memory allocations between API calls, it\u0026rsquo;s stateless.\nFor love of monkey-patching and the lack of time on Friday evening, this was the best I could come up with: Pass a query parameter in HTTP request which would take a snapshot. Pass a different parameter which would take another snapshot and compare it with the first one! Neat? Here\u0026rsquo;s how it looks:\nimport tracemalloc tracemalloc.start() s1=None s2=None ... @blueprint.route(\u0026#39;/app_metric/\u0026lt;app\u0026gt;\u0026#39;) def get_metric(app): global s1,s2 trace = request.args.get(\u0026#39;trace\u0026#39;,None) response, status = get_metrics_for_app(app) if trace == \u0026#39;s2\u0026#39;: s2=tracemalloc.take_snapshot() for i in s2.compare_to(s1,\u0026#39;lineno\u0026#39;)[:10]: print(i) elif trace == \u0026#39;s1\u0026#39;: s1=tracemalloc.take_snapshot() return jsonify(data=response), status When trace=s1 is passed with request, a memory snapshot is taken. When trace=s2 is passed, another snapshot is taken and it is compared with the first snapshot. We will be printing the difference and that would tell who allocated how much memory between these two requests.\nHello, leak! The output of snapshot difference looked like this:\n/\u0026lt;some\u0026gt;/\u0026lt;path\u0026gt;/\u0026lt;here\u0026gt;/foo_module.py:65: size=3326 KiB (+2616 KiB), count=60631 (+30380), average=56 B /\u0026lt;another\u0026gt;/\u0026lt;path\u0026gt;/\u0026lt;here\u0026gt;/requests-2.18.4-py2.py3-none-any.whl.68063c775939721f06119bc4831f90dd94bb1355/requests-2.18.4-py2.py3-none-any.whl/requests/models.py:823: size=604 KiB (+604 KiB), count=4 (+3), average=151 KiB /export/apps/python/3.6/lib/python3.6/threading.py:884: size=50.9 KiB (+27.9 KiB), count=62 (+34), average=840 B /export/apps/python/3.6/lib/python3.6/threading.py:864: size=49.0 KiB (+26.2 KiB), count=59 (+31), average=851 B /export/apps/python/3.6/lib/python3.6/queue.py:164: size=38.0 KiB (+20.2 KiB), count=64 (+34), average=608 B /export/apps/python/3.6/lib/python3.6/threading.py:798: size=19.7 KiB (+19.7 KiB), count=35 (+35), average=576 B /export/apps/python/3.6/lib/python3.6/threading.py:364: size=18.6 KiB (+18.0 KiB), count=36 (+35), average=528 B /export/apps/python/3.6/lib/python3.6/multiprocessing/pool.py:108: size=27.8 KiB (+15.0 KiB), count=54 (+29), average=528 B /export/apps/python/3.6/lib/python3.6/threading.py:916: size=27.6 KiB (+14.5 KiB), count=57 (+30), average=496 B \u0026lt;unknown\u0026gt;:0: size=25.3 KiB (+12.4 KiB), count=53 (+26), average=488 B Turns out, we had a custom module which we were using to make downstream calls to get data for response. That custom modules override the threadpool module to get profiling data ie: how much time did it take to do the downstream call. And for some reason, the result of profiling was appended to a list which was class variable! Which was 2600KB in size (the first line) and this was done for every incoming request. It looked something like this:\nclass Profiler(object): ... results = [] ... def end(): timing = get_end_time() results.append(timing) ... Guess who had a happy Friday! :D\nInteresting reads:\nhttps://hbfs.wordpress.com/2013/01/01/python-memory-management-part-i/ https://hbfs.wordpress.com/2013/01/08/python-memory-management-part-ii/ https://rushter.com/blog/python-memory-managment https://rushter.com/blog/python-garbage-collector Recording of the talk: gcmodule.c\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTracemalloc\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/detect-memory-leak-python/","summary":"In production, a memory leak will not always bubble up. And there could be multiple reasons behind it. You may not be getting enough traffic. Frequent deployments. No hard memory usage limit set. Or mix of them.\nThe flask app we had to debug had same characteristics. It never had huge surge of traffic and there would be multiple deployments over week. Although it had cgroup memory usage limit, it had some room to grow and the leak never appeared.","title":"Detecting Memory Leak in Python"},{"content":"Last, and the most dreaded, comes the MySQL RDS. The most critical part of infrastructure which is responsible for auth, new sign-ups and other user related activities.\nThe task was dreaded because there is no way to enable encryption once RDS has been created and most ways, which we will discuss, incurred downtime.\nThe Task Encrypt existing MySQL RDS, which is also multi AZ, with near-zero downtime.\nThe RDS instance we had was a MySQL one which is also multi AZ. After lots of docs reading, googling around and chatting with AWS support we found following ways to go about it.\nThe Snapshot Way [huge downtime] The dump way. The DMS Way The Looong Way The Snapshot Way: Probably the easiest to follow, the one mentioned in AWS docs too. BUT. Huge downtime depending on the DB size. Because once the RDS instance has been created, there is no way to enable encryption, what AWS suggests is to do:\nTake a snapshot. Encrypt snapshot. Restore an RDS from encrypted snapshot. Clearly, if we don\u0026rsquo;t want to miss-out writes, we will have to shut the DB down and that is downtime. Not the way to go about it.\nThe Dump Way: Also has downtime bus lesser than the previous one. The idea is to take dump of whatever db you want to be encrypted only. And not whole DB instance. And upload it to newly created empty encrypted RDS. We tried taking dump and uploaded on new RDS and measured time. It was in minutes and not acceptable.\nThe DMS Way: AWS has this cool service called Database Migration Service [DMS] which lets you migrate databases across DB engines. And from same engine to same engine too ie from MySQL to MySQL.\nSo here we though the trick that should work is:\nCreate new RDS with encryption enabled (we can only do it while starting new one) Create DMS job to migrate existing DB to new one. Also choose replicate old data and continue to keep DBs in sync So what DMS basically does (I think) is you give them two DBs. Source and destination (encrypted here). And tell it to keep them in sync. So they will upload initial DB dump on new db and then continue replicating bin log on new one and thus keeping them in sync.\nThis should work, right? Wrong! It was all cool on test setup. It did replicate old db to new one and it also kept replicating live writes. And mind well that this was on dump of our production db that we wanted to encrypt. But when we tried to move this setup with staging infra. DMS replication started failing! Why? The JSON Columns. Turns out, DMS did not support replicating JSON columns so when one such cell is updated in source DB, it could not replicate the same on destination encrypted db.\nBUMMER! I thought life is easy. Nope! Hence,\nThe Looong Way: Running out of options we had to go this way:\nMake a read replica of existing non-encrypted DB Wait for replica to start and get in sync with source. Stop replication on read replica Note where replication was stopped. Take snapshot of read replica Encrypt snapshot Restore RDS from step 6 snapshot Start replication. Set RDS master as the original db and replication start point as noted in step 4 Now before you start, make sure binlog are enabled and is in row format (by default it is). Also increase bin log retention duration so that we have it to get replicated to new db.\nshow variables like \u0026#39;binlog_format\u0026#39;; # Create replication user and increase binlog retention to 24hrs: CREATE USER \u0026#39;rep_user\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;rep_user\u0026#39;; GRANT REPLICATION slave ON *.* TO \u0026#39;rep_user\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;rep_user\u0026#39;; FLUSH PRIVILEGES; # increase binlog retention call mysql.rds_set_configuration(\u0026#39;binlog retention hours\u0026#39;, 24); Now create a read replica of existing RDS from AWS console. Wait for it to get in sync and be available fully. Monitor the console and see when it becomes available. Also you can log-in to ReadReplica (RR) and do show slave status\\G;\nNow moving to step 3, we will stop replication and note the points.\nStop replication: CALL mysql.rds_stop_replication; # Note the replication state: show slave status\\G; And note-down Relay_Master_log_file and Exec_Master_log_pos :: this basically says in which file till which position replication has been done. We will use the same position while resuming replication.\nRDS page should say replication has been stopped using command in RED fonts. Take snapshot from RDS UI. Check for snapshot progress and once done encrypt snapshot using RDS UI only. Restore RDS from encrypted snapshot. It will have most settings as default, you can set rds name here. You can make whatever modification you want to have in your new encrypted DB.\nOnce RDS has been started it will have default SG (no access from EC2 or office), have backup disabled, default parameter group etc. Just modify this RDS to have same setting as source RDS. we will need to reboot this RDS to apply parameter group.\nNow we have encrypted RDS which has data similar to what ReadReplica had. Now we need to enable replication from the points we noted above.On new encrypted RDS\nCALL mysql.rds_set_external_master(\u0026#39;\u0026lt;source rds\u0026gt;\u0026#39;, 3306, \u0026#39;\u0026lt;replication user\u0026gt;\u0026#39;, \u0026#39;\u0026lt;that user\u0026#39;s passwd\u0026gt;\u0026#39;, \u0026#39;\u0026lt;Relay_Master_log_file\u0026gt;\u0026#39;, \u0026lt;Exec_Master_log_pos\u0026gt;, 0); CALL mysql.rds_start_replication; Now do show slave status\\G; and monitor Replication Delay. It would be in some thousands of seconds as it takes time to take and encrypt snapshot. Withing couple of minutes, new RDS should sync and delay should get to 0.\nNow you have both the RDS in sync, any writes to unencrypted one is replicated to encrypted one. You can now switch your application to use new one. While migrating we should make sure that all service instance are migrated at once , ie there should be no point where both RDS are in use by set of services. Why? Because this may break replication.\nExample: Say we have auto-increment column in a DB and we have written till column 100. Now if you write on slave (encrypted RDS here), you will be able to write and that column will be 101. If you write on master (unencrypted RDS here) you also will be able to write with column 101. But the replication will try to replicate master\u0026rsquo;s 101 column and because it already exists on slave replication would break and all DB will stop getting replicated :)\nSo let\u0026rsquo;s tear replication down as there are no writes on master. Do this on slave (new encrypted RDS):\nCALL mysql.rds_stop_replication; CALL mysql.rds_reset_external_master; And that is it! Again, couple things to remember:\nNeedless to say, try it all on staging setup. Try it couple of times While migrating, there should be no instance while both RDS are simultaneously being used by service. Try using DMS if you don\u0026rsquo;t have json columns. Ref:\nEncrypting Amazon RDS Resources ","permalink":"http://localhost:1313/posts/encrypt-existing-aws-rds/","summary":"Last, and the most dreaded, comes the MySQL RDS. The most critical part of infrastructure which is responsible for auth, new sign-ups and other user related activities.\nThe task was dreaded because there is no way to enable encryption once RDS has been created and most ways, which we will discuss, incurred downtime.\nThe Task Encrypt existing MySQL RDS, which is also multi AZ, with near-zero downtime.\nThe RDS instance we had was a MySQL one which is also multi AZ.","title":"Encrypt Existing AWS RDS : The GDPR Series"},{"content":"So next in line was S3 bucket. This too did not have encryption enabled, ie: data encryption at rest.\nThe Task Encrypt existing S3 bucket which contains user data with zero downtime.\nA word on encrypted S3 objects/buckets: By default there is no encryption involved when you create or put objects in an S3 bucket. However, you can enable default encryption on a bucket and any object put in the bucket will be encrypted by default. And there are ways to enforce it also. S3 at that moment had two types of server side encryption options available.\nSSE-S3 SSE-KMS In first option that is SSE-S3, you just ask S3 to encrypt your objects and the rest will be managed by them. Meaning you don\u0026rsquo;t have to specify a key using which objects will be encrypted. While going with the second option, you can choose your own KMS key which will be used for encryption.\nWe went with option 1 as we did not intend to use features provided by KMS keys like rotation and brining in your own key. Also KMS request rate is subject to S3 request rates so you have to be aware of that too.\nSo this is how we went with it.\nStep 1: Backup (of course) Goes without saying! Used S3 cli for this purpose. Be mindful of choosing number of threads/concurrency as S3 has some rate limit.1\naws s3 cp s3://bucket-name/ s3://backup-bucket-name/ --recursive Step 2: Enable Default Encryption In S3 console go to Properties Tab \u0026gt; Default Encryption. You have option to select SSE-S3 or SSE-KMS. After enabling this, every object put in bucket will be encrypted by default. Now all that we have to do is encrypt older objects.\nStep 3: Encrypt Older Objects This can be done two ways. You can either use S3 CLI or write your own little python/java program to do it. Using CLI it would look something like:\naws s3 cp s3://bucket-name/ s3://bucket-name/ --recursive --sse So what you\u0026rsquo;re doing here is copying objects in-place and while doing that, enable encryption also with --sse. What we did it Java way by creating threadpool of 20 or 50.\nAnd that is it! Again, couple things to remember:\nBe mindful of S3 request limits. So that real traffic does not get affected. You can contact AWS support beforehand. If you\u0026rsquo;re using SSE-KMS, be careful to not delete the key that was used to encrypt. Otherwise the data is as good as trash. :) If using S3 cli, do look into max_queue_size and max_concurrent_requests to limit the request rate or to increase it. 2 Ref:\nS3 Request Rate Considerations\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS3 CLI config\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/encrypt-s3-bucket/","summary":"So next in line was S3 bucket. This too did not have encryption enabled, ie: data encryption at rest.\nThe Task Encrypt existing S3 bucket which contains user data with zero downtime.\nA word on encrypted S3 objects/buckets: By default there is no encryption involved when you create or put objects in an S3 bucket. However, you can enable default encryption on a bucket and any object put in the bucket will be encrypted by default.","title":"Encrypt Existing S3 Bucket : The GDPR Series"},{"content":"It\u0026rsquo;s been some time since The GDPR has kicked in. And like every other ops person, I too had to work for compliance related tasks. Three major tasks that I took up, which involved mostly encrypting data at rest, were\nEncrypting existing EBS Encrypt S3 buckets Encrypt RDS In this GDPR series, I will be sharing my experiences and how did we went on with it.\nThe Task Encrypt EBS that is currently in use by a MongoDB cluster. With zero downtime.\nA word on encrypted EBS: EBS encryption makes data encrypted at rest and it is decrypted only on your physical hardware. So on wire ie. from EBS storage till your physical hardware where your instance runs, and on its storage, it is encrypted.\nThe setup was consisting 3 mongo node with data replicated on all. No fancy stuff like sharding. Because we had three node cluster, the goal of zero downtime was achievable. We could simply take one node down, do our stuff, put it back in cluster. We will discuss the \u0026lsquo;do our stuff\u0026rsquo; part here and not the mongo specific instructions. Ie how to take a node out of cluster and put in back in.\nStep 1 : Unmount the volume In mongo terms, take node out of cluster, meaning make sure no process is using that volume and it is free to unmount.\nsudo umount \u0026lt;mount point\u0026gt; If unmount fails stating volume is in use, you can find process using that volume using\nsudo lsof | grep \u0026lt;mount point\u0026gt; Step 2 : Encrypt it We had two options to go with this step:\nAttach new encrypted volume and rsync your data Encrypt existing volume I remember measuring time for both the options and they were comparable. We found option 2 to be faster because the whole copying-encrypting process is done by AWS and not on your instance. So we decided to go with second way of doing it, which would involve following steps:\nTake a snapshot of your EBS volume Copy snapshot with encryption enabled. Create a new EBS from copied encrypted snapshot All the steps mentioned above may take some time depending on size of volume.\nStep 3 : Mount it Now newly restored EBS can be attached to instance and mounted to older mount point. With mongo, we just start mongo back again and it will get in sync with cluster by replicating missed data.\nYes so that is how that one went. While encrypting master node of mongo, we had to fail master over to other cluster member and then we followed the same procedure. More detailed docs info can be found on this AWS Guide\n","permalink":"http://localhost:1313/posts/encrypt-aws-ebs/","summary":"It\u0026rsquo;s been some time since The GDPR has kicked in. And like every other ops person, I too had to work for compliance related tasks. Three major tasks that I took up, which involved mostly encrypting data at rest, were\nEncrypting existing EBS Encrypt S3 buckets Encrypt RDS In this GDPR series, I will be sharing my experiences and how did we went on with it.\nThe Task Encrypt EBS that is currently in use by a MongoDB cluster.","title":"Encrypting Existing AWS EBS : The GDPR Series"},{"content":"Most of our infrastructure and client facing services are in us-east-1 and we have lots of users connecting from different parts of the world including India. Of course there was a significant latency involved when users connect to US from other part of the world. And we wanted to test that, if a user from India connects to Mumbai region(faster handshake) and then that region uses VPC peering to us-east-1 to talk to other services. Here\u0026rsquo;s how it went:\nFirst thing we looked out for was AWS provided VPC peering, it does exist, but for limited number of regions (4, 3 US, 1 UK, at the time of writing this) and it did not include Mumbai. So we had to setup our own IPSec VPN tunnels.\nThe tool we chose for IPSec tunnel was OpenSwan. Found it easy to setup. We tried two different setups:\nEC2 \u0026lt;=== tunnel ===\u0026gt; EC2 EC2 \u0026lt;=== tunnel ===\u0026gt; AWS Managed VPN VPC Peering: EC2 \u0026lt;=\u0026gt; EC2 Following steps can be taken for setting up EC2, on both the regions.\nnet.ipv4.ip_forward = 1 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.all.send_redirects = 0 install OpenSwan: `sudo yum install openswan`\n`sudo vi /etc/ipsec.conf` and uncomment last line to include files from `ipsec.d folder.\nCreate conf files:\n(/etc/ipsec.d/us-mum.conf)\nconn us-mum type=tunnel authby=secret left=%defaultroute leftid=\u0026lt;your pub IP\u0026gt; leftnexthop=%defaultroute leftsubnet=\u0026lt;your VPC CIDR\u0026gt; right=\u0026lt;opposite side pub IP\u0026gt; rightsubnet=\u0026lt;opposite side VPC CIDR\u0026gt; pfs=yes auto=start (/etc/ipsec.d/us-mum.secrets)\n\u0026lt;your pub IP\u0026gt; \u0026lt;other side pub IP\u0026gt; : PSK \u0026#34;changemeplease\u0026#34; You would want to do similar setup in the other region\u0026rsquo;s VPC EC2. Obviously, the PSK will the same and new conf files will be created ie:`/etc/ipsec.d/us-mum.secrets` and `us-mum.conf` with values changed appropriately.\nEstablishing tunnel: sudo service ipsec start sudo ipsec verify sudo service ipsec status If you see any problems with `verify` output, you may want to rectify it. For example if you have not set `send_redirects` or not set it properly, you can do:\necho 0 \u0026gt; /proc/sys/net/ipv4/conf/all/send_redirects echo 0 \u0026gt; /proc/sys/net/ipv4/conf/default/send_redirects echo 0 \u0026gt; /proc/sys/net/ipv4/conf/eth0/send_redirects echo 0 \u0026gt; /proc/sys/net/ipv4/conf/lo/send_redirects VPC Peering: EC2 \u0026lt;=\u0026gt; AWS Managed VPN In this case, one instance will be taken care by AWS and one will be EC2 as setup above.\nTo setup AWS side VPN:\nOnce you create VPN connection, you will get two public IPs. Use one of them in our EC2 conf as a rightid. The rest settings should be same as EC2-EC2 setup and self explanatory.\nStill not working?\nRef: https://aws.amazon.com/articles/connecting-multiple-vpcs-with-ec2-instances-ipsec/\nLatency Tests: Here\u0026rsquo;s how we performed latency tests. We had a webserver running in US region. We had one instance in both US and Mumbai region, both had nginx, proxying requests to US region.\nHere are the results:\nConnecting Directly to US from Bangalore:\nConnecting to Mumbai from Bangalore , request will be tunneled to US:\nAs you can see, for the first request, handshake is much faster (approx 10x) to Mumbai as it is near to client. But when you have the socket established, it\u0026rsquo;s clear that if you take Mumbai route (VPN) to US instead of going directly US, it is approx 1.5x slower as we encounter penalty for VPN encryption and decryption operations.\nSo that\u0026rsquo;s how that one went. Let me know if any doubts or you\u0026rsquo;re stuck anywhere. Also I would love to know how did it work for you and what improvements you saw with your setup.\n","permalink":"http://localhost:1313/posts/aws-vpc-peering-latency-test/","summary":"Most of our infrastructure and client facing services are in us-east-1 and we have lots of users connecting from different parts of the world including India. Of course there was a significant latency involved when users connect to US from other part of the world. And we wanted to test that, if a user from India connects to Mumbai region(faster handshake) and then that region uses VPC peering to us-east-1 to talk to other services.","title":"Setting up Inter Region AWS VPC Peering and Latency Tests"},{"content":"About new Hadoop cluster we set up, the phoenix version bundled with HDP distribution(4.7) had some bugs which would make it impossible to use to run BI queries. There was no way provided by HDP to upgrade phoenix as we were using the latest version. Looking around on the internet, I found that manually we can replace the related jars and bins to have a new version in place.\nSo that\u0026rsquo;s what I tried. And it kind of worked. (It still is working)\nThese are the steps: download latest phoenix binaries (4.10 at that time) Find installed files (under /usr/hdp/current/phoenix) Correlate installed ones with the ones in new binary package/tar Replace older files with new ones and also rename them or make appropriate links. Here a very lame/lousy bash script I used:\nDisclaimer: It is not meant to use as a copy-paste script. Use it only for reference. Also we did it when cluster was not having production workloads. You might want to be extra cautious and read more around this if your cluster is having production workloads.\ncp -R /usr/hdp/2.5.3.0-37/phoenix /usr/hdp/2.5.3.0-37/phoenix-bk cd $NEW_PHOENIX cp phoenix-4.10.0-HBase-1.1-client.jar phoenix-4.10.0-HBase-1.1-hive.jar phoenix-4.10.0-HBase-1.1-queryserver.jar phoenix-4.10.0-HBase-1.1-server.jar phoenix-4.10.0-HBase-1.1-thin-client.jar /usr/hdp/2.5.3.0-37/phoenix/ rm -f /usr/hdp/2.5.3.0-37/phoenix/phoenix-4.7.0.2.5.3.0-37-client.jar /usr/hdp/2.5.3.0-37/phoenix/phoenix-4.7.0.2.5.3.0-37-hive.jar /usr/hdp/2.5.3.0-37/phoenix/phoenix-4.7.0.2.5.3.0-37-queryserver.jar /usr/hdp/2.5.3.0-37/phoenix/phoenix-4.7.0.2.5.3.0-37-server.jar /usr/hdp/2.5.3.0-37/phoenix/phoenix-4.7.0.2.5.3.0-37-thin-client.jar cd /usr/hdp/2.5.3.0-37/phoenix/ rm -f phoenix-client.jar phoenix-hive.jar phoenix-server.jar phoenix-thin-client.jar ln -s phoenix-4.10.0-HBase-1.1-client.jar phoenix-client.jar ln -s phoenix-4.10.0-HBase-1.1-hive.jar phoenix-hive.jar ln -s phoenix-4.10.0-HBase-1.1-server.jar phoenix-server.jar ln -s phoenix-4.10.0-HBase-1.1-thin-client.jar phoenix-thin-client.jar cd $NEW_PHOENIX cp phoenix-core-4.10.0-HBase-1.1.jar phoenix-core-4.10.0-HBase-1.1-sources.jar phoenix-flume-4.10.0-HBase-1.1.jar phoenix-hive-4.10.0-HBase-1.1.jar phoenix-hive-4.10.0-HBase-1.1-sources.jar phoenix-pherf-4.10.0-HBase-1.1.jar phoenix-pherf-4.10.0-HBase-1.1-minimal.jar phoenix-pherf-4.10.0-HBase-1.1-sources.jar phoenix-pig-4.10.0-HBase-1.1.jar phoenix-queryserver-4.10.0-HBase-1.1.jar phoenix-queryserver-4.10.0-HBase-1.1-sources.jar phoenix-queryserver-client-4.10.0-HBase-1.1.jar phoenix-spark-4.10.0-HBase-1.1.jar phoenix-spark-4.10.0-HBase-1.1-sources.jar /usr/hdp/2.5.3.0-37/phoenix/lib cd /usr/hdp/2.5.3.0-37/phoenix/lib rm -f phoenix-core-4.7.0.2.5.3.0-37.jar phoenix-core-4.7.0.2.5.3.0-37-sources.jar phoenix-flume-4.7.0.2.5.3.0-37.jar phoenix-hive-4.7.0.2.5.3.0-37.jar phoenix-hive-4.7.0.2.5.3.0-37-sources.jar phoenix-pherf-4.7.0.2.5.3.0-37.jar phoenix-pherf-4.7.0.2.5.3.0-37-minimal.jar phoenix-pherf-4.7.0.2.5.3.0-37-sources.jar phoenix-pig-4.7.0.2.5.3.0-37.jar phoenix-queryserver-4.7.0.2.5.3.0-37.jar phoenix-queryserver-4.7.0.2.5.3.0-37-sources.jar phoenix-queryserver-client-4.7.0.2.5.3.0-37.jar phoenix-spark-4.7.0.2.5.3.0-37.jar phoenix-spark-4.7.0.2.5.3.0-37-sources.jar cd /usr/hdp/2.5.3.0-37/phoenix/bin rm -f end2endTest.py performance.py pherf-cluster.py pherf-standalone.py pherf-standalone.py phoenix_utils.pyc queryserver.py sqlline.py sqlline-thin.py traceserver.py cd $NEW_PHOENIX/bin cp end2endTest.py performance.py pherf-cluster.py pherf-standalone.py queryserver.py sqlline.py sqlline-thin.py traceserver.py /usr/hdp/2.5.3.0-37/phoenix/bin You can see:\nI first copied new jars Deleted old ones Make links with the new jars Copied new jars to lib folder and removed older ones. Removed old binaries and copied new ones. That\u0026rsquo;s it!\nAlso you would want to add your zookeeper address in sqline.py which was there in the previous binary installed by HDP. Else you can pass it as cmdline.\nAnd restart the cluster [server and then clients] 🙂\nDo read: https://phoenix.apache.org/upgrading.html You might also want to look at Tuning Your HBase or maybe benchmark it.\n","permalink":"http://localhost:1313/posts/upgrading-apache-phoenix-hdp/","summary":"About new Hadoop cluster we set up, the phoenix version bundled with HDP distribution(4.7) had some bugs which would make it impossible to use to run BI queries. There was no way provided by HDP to upgrade phoenix as we were using the latest version. Looking around on the internet, I found that manually we can replace the related jars and bins to have a new version in place.\nSo that\u0026rsquo;s what I tried.","title":"Upgrading Apache Phoenix in HDP Cluster"},{"content":"Recently I was revisiting concepts of TCP protocol and that reminded me that there was also a thing called TCP Fast Open. Digging further on the same revealed a lot. We will briefly discuss how this enhancement works. What are the limitations. And later we will do the hands on and see how the TCP Fast Open drastically reduces the load time.\nWhat is TCP Fast Open? TCP Fast Open is an optimization over TCP which eliminates the need to wait for 3 way handshake before application can send data over it.\nHere\u0026rsquo;s roughly how it happens: First time 3 way handshake happens, server will generate a cookie and pass it to the client. Next time, in first step on 3 way handshake (SYN), client will send cookie+data along with SYN. If cookie stands valid, data is delivered to application, then application can process data and reply. Here we do not wait for 3 way handshake to complete next time.\nHere is the traditional 3 way handshake.\nSo here, if round trip time is 100ms, application will have to wait at least 200ms before it can send any data to server.\nIf client had set TCP Fast Option set (we will see server and client code soon), server will create a cookie and send back the cookie to client, as shown in the following diagram.\nNow from next time onward, client will send cookie in SYN and if it stands valid, server can immediately send reply back without waiting for handshake to complete as shown below.\nWe can see that 3 way handshake still happens but application does not need to wait for it as data is delivered to it immediately if valid cookie is present.\nThis can potentially halve the latency depending upon the application.\nProblems: Not all the devices support it. (Although they will at some point) Some middleware (firewall,NAT) can cause problems as this is relatively new changes to an old protocol. If data that client need to send with SYN is large (\u0026gt; ~1400 bytes), TCP Fast open does not optimize anything. Requests may get duplicated. (The first SYN packet) So applications need to handle that. In Action: We will build simple echo server with TCP Fast Open enabled and see the wireshark traces along with request time to see how fast does it get with TFO enabled.\nEcho server:\nimport socket def listen(): connection = socket.socket(socket.AF_INET, socket.SOCK_STREAM) connection.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) connection.setsockopt(socket.SOL_TCP, 23, 5) connection.bind((\u0026#39;0.0.0.0\u0026#39;, 8000)) connection.listen(10) while True: current_connection, address = connection.accept() while True: data = current_connection.recv(2048) if data: current_connection.send(data) print data current_connection.close() break if __name__ == \u0026#34;__main__\u0026#34;: try: listen() except KeyboardInterrupt: pass The only new/different we do here is the line `connection.setsockopt(socket.SOL_TCP, 23, 5)` here 23 is the protocol number of **TCP_FASTOPEN **it is not defined in socket module if python2 so writing manually here and 5 is the queue length for number of TFO request which are yet to complete 3 way handshake.\nEcho client:\nimport socket addr = (\u0026#34;ssh.movienight.gq\u0026#34;, 8000) s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) s.setsockopt(socket.SOL_TCP, 23, 5) s.sendto(\u0026#34;hello!\u0026#34;,536870912,addr) print s.recv(1000) Here we used `sendto` to send SYN packet along with data. second argument is protocol number for MSG_FASTOPEN. Again because of python2. Setting this will send a TFO request to server.\nTo enable TFO support in linux:\necho 3 \u0026gt; /proc/sys/net/ipv4/tcp_fastopen number 3 will add support for both TFO client and server.\nLet\u0026rsquo;s look at network traces to see TFO working in action:\nThis was trace for simple curl request to server. No TFO set. As we can see, first 3 way handshake happens [first 3 packets] then application (curl) sends HTTP request and after that it gets response.\nHere with –tcp-fastopen option to curl, in the fist packet itself [46645] client will send request data. and it was immediately delivered to application (echo server). Also note that the three way handshake still happens.\nAlso see how cookie was sent with SYN packet[46645] itself.\nHow fast was it? Let the results speak for themselves: [ okay, 270ms (TFO) vs 690ms (regular TCP) ]\nsanket@iamgroot /home/sanket/workspace/z_trivial/knetstat (0) (master)\u0026gt; time curl ssh.movienight.gq:8000 19:59:12 GET / HTTP/1.1 Host: ssh.movienight.gq:8000 User-Agent: curl/7.56.1 Accept: */* 0.00user 0.00system 0:00.69elapsed 1%CPU (0avgtext+0avgdata 8860maxresident)k 0inputs+0outputs (0major+492minor)pagefaults 0swaps sanket@iamgroot /home/sanket/workspace/z_trivial/knetstat (0) (master)\u0026gt; time curl ssh.movienight.gq:8000 --tcp-fastopen GET / HTTP/1.1 Host: ssh.movienight.gq:8000 User-Agent: curl/7.56.1 Accept: */* 0.00user 0.00system 0:00.27elapsed 1%CPU (0avgtext+0avgdata 8728maxresident)k 0inputs+0outputs (0major+494minor)pagefaults 0swaps Of course I did not tell the fine details. Please refer:\n[1] https://lwn.net/Articles/508865/\n[2] https://bradleyf.id.au/nix/shaving-your-rtt-wth-tfo/\nIf Websockets excites you, which is kinda long open TCP connection, speaking it\u0026rsquo;s own language, why don\u0026rsquo;t you look up to this article\n","permalink":"http://localhost:1313/posts/tcp-fast-open-python/","summary":"Recently I was revisiting concepts of TCP protocol and that reminded me that there was also a thing called TCP Fast Open. Digging further on the same revealed a lot. We will briefly discuss how this enhancement works. What are the limitations. And later we will do the hands on and see how the TCP Fast Open drastically reduces the load time.\nWhat is TCP Fast Open? TCP Fast Open is an optimization over TCP which eliminates the need to wait for 3 way handshake before application can send data over it.","title":"TCP Fast Open: In Action with Python"},{"content":"Echo websocket server implemented by hand on raw TCP Sockets.\nJourney to websocket was pretty long. I started with an idea to make an app which can play music in sync across the devices during college period. No wonder I couldn\u0026rsquo;t get through it. Later this year I stumbled upon this new thing called WebSockets and they were intriguing. I thought I could finish that app with websockets (and I did, with partial success). Spinned of another app out of it. And websockets were on a roll. It was time I digged further in and ended up writing a websocket server. (GitHub link at the bottom)\nUpdate: Links to production grade websocket implementations at bottom.\nSo what is a websocket server? A WebSocket server is a TCP application listening on any port of a server that follows a specific protocol, simple as that.\nHow does it work? : It uses HTTP protocol for handshake and after handshake is complete, it works over TCP protocol and exchanges data in its agreed-upon format called frames. Connections are bi-directional and any party can send message anytime. Unlike HTTP where new TCP connection is made every time you want to communicate, WebSockets maintains a connection using which any side can send message anytime, reducing the message delivery time by using the existing connection.\nThe WebSocket Server: We will be writing our server in 4 parts:\nWriting a TCP/HTTP server to identify a websocket request. Performing a handshake Decoding/Receiving data/frames Sending data/frames I will be discussing the protocol implementations as we go thru steps. You can also take a pause have a look at this awesome piece written by Mozilla on WebSocket Servers. It is a must read. Now or later.\n1. Writing a TCP/HTTP Server to Identify WebSocket Request We will be using python’s SocketServer library which ptovides simple TCP server. The client will send an HTTP request which looks something like this:\nGET /chat HTTP/1.1 Host: example.com:8000 Upgrade: websocket Connection: Upgrade Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ== Sec-WebSocket-Version: 13 So what we need to lookout for is that if the request is of type GET and it has these three headers namely `Upgrade: websocket` `Connection: Upgrade` and `Sec-WebSocket-key: `\nIf you find all this, we can proceed towards the next step which is completing the handshake. In our implementation we will check if all the three headers are present and we will proceed with the handshake. The request handler function will look something like this:\ndef handle(self): # self.request is the TCP socket connected to the client self.data = self.request.recv(1024).strip() headers = self.data.split(\u0026#34;\\r\\n\u0026#34;) # is it a websocket request? if \u0026#34;Connection: Upgrade\u0026#34; in self.data and \u0026#34;Upgrade: websocket\u0026#34; in self.data: # getting the websocket key out for h in headers: if \u0026#34;Sec-WebSocket-Key\u0026#34; in h: key = h.split(\u0026#34; \u0026#34;)[1] # let\u0026#39;s shake hands shall we? self.shake_hand(key) while True: payload = self.decode_frame(bytearray(self.request.recv(1024).strip())) decoded_payload = payload.decode(\u0026#39;utf-8\u0026#39;) self.send_frame(payload) if \u0026#34;bye\u0026#34; == decoded_payload.lower(): \u0026#34;Bidding goodbye to our client...\u0026#34; return else: self.request.sendall(\u0026#34;HTTP/1.1 400 Bad Request\\r\\n\u0026#34; + \\ \u0026#34;Content-Type: text/plain\\r\\n\u0026#34; + \\ \u0026#34;Connection: close\\r\\n\u0026#34; + \\ \u0026#34;\\r\\n\u0026#34; + \\ \u0026#34;Incorrect request\u0026#34;) This is the rough flow: If we find a valid websocket request, we proceed with handshake and then in while loop, we just do echo. ie sending back whatever we received. If it’s not a valid request we send HTTP 400 in response.\nPretty simple till now, innit?\n2. Performing a Handshake This is where the protocol details kicks in. You will need to send a specific HTTP response back to client in order to establish the bidirectional connection. The response will look something like this:\nHTTP/1.1 101 Switching Protocols Upgrade: websocket Connection: Upgrade Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo= You see there a new header called Sec-WebSocket-Accept with some random looking characters. Now there\u0026rsquo;s a method to calculate this. As per protocol, you concatenate the key you received in request header (‘dGhlIHNhb…’) and the magic string (“258EAFA5-E914-47DA-95CA-C5AB0DC85B11”) , calcualte SHA1 hash of them and send back the base64 encoding of the hash (which is ‘s3pPLMB…’) This is done so that client can also confirm that the server understands the protocol. So handshake is basically HTTP response with a header containg SHA1 of the key and magic-string and key client sent and those same two headers.\nHere\u0026rsquo;s how it\u0026rsquo;s done in python:\ndef shake_hand(self,key): # calculating response as per protocol RFC key = key + WS_MAGIC_STRING resp_key = base64.standard_b64encode(hashlib.sha1(key).digest()) resp=\u0026#34;HTTP/1.1 101 Switching Protocols\\r\\n\u0026#34; + \\ \u0026#34;Upgrade: websocket\\r\\n\u0026#34; + \\ \u0026#34;Connection: Upgrade\\r\\n\u0026#34; + \\ \u0026#34;Sec-WebSocket-Accept: %s\\r\\n\\r\\n\u0026#34;%(resp_key) Here we send the key we received in request header as an argument and we use hashlib to calculate SHA1 and base64 to encode it.\n3. Decoding an Incoming Frame Now that the connection is established, client/the other side can send us data. Now the data won’t be in plain-text. It is using a special frame format defined in protocol. A frame looks something like this:\n0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-------+-+-------------+-------------------------------+ |F|R|R|R| opcode|M| Payload len | Extended payload length | |I|S|S|S| (4) |A| (7) | (16/64) | |N|V|V|V| |S| | (if payload len==126/127) | | |1|2|3| |K| | | +-+-+-+-+-------+-+-------------+ - - - - - - - - - - - - - - - + | Extended payload length continued, if payload len == 127 | + - - - - - - - - - - - - - - - +-------------------------------+ | |Masking-key, if MASK set to 1 | +-------------------------------+-------------------------------+ | Masking-key (continued) | Payload Data | +-------------------------------- - - - - - - - - - - - - - - - + : Payload Data continued ... : + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + | Payload Data continued ... | +---------------------------------------------------------------+ I will discuss the fields which we will be using here. Please read that Mozilla article I mentioned before to get more idea around this.\nThe FIN bit suggests that this is the last frame.We will assume/set it to 1 as we will be sending small amount of data only. Next 3 bits are reserved. The opcode suggests what kind of operation is this. 0x0 for continuation, 0x1 for text, 0x2 for binary etc. We will be using 0x1. The MASK bit we will discuss shortly.\nPayload length is somewhat tricky. I am quoting the Mozilla Article here:\nDecoding Payload Length To read the payload data, you must know when to stop reading. That\u0026rsquo;s why the payload length is important to know. Unfortunately, this is somewhat complicated. To read it, follow these steps:\nRead bits 9-15 (inclusive) and interpret that as an unsigned integer. If it\u0026rsquo;s 125 or less, then that\u0026rsquo;s the length; you\u0026rsquo;re done. If it\u0026rsquo;s 126, go to step 2. If it\u0026rsquo;s 127, go to step 3. Read the next 16 bits and interpret those as an unsigned integer. You\u0026rsquo;re done. Read the next 64 bits and interpret those as an unsigned integer (The most significant bit MUST be 0). You\u0026rsquo;re done. We are assuming it to be \u0026lt;125. That will leave byte 2 to 6 as masking bytes. If you are using web browser console as a client (which we will) it will set the mask bit to 1. Hence the payload will be masked. You can use XOR operation with the mask to get the original data back. The code to help you understand it better:\ndef decode_frame(self,frame): opcode_and_fin = frame[0] # assuming it\u0026#39;s masked, hence removing the mask bit(MSB) to get len. also assuming len is \u0026lt;125 payload_len = frame[1] - 128 mask = frame [2:6] encrypted_payload = frame [6: 6+payload_len] payload = bytearray([ encrypted_payload[i] ^ mask[i%4] for i in range(payload_len)]) return payload We sent frame as a bytearray as you noticed in the first function handle. The operations are quite self explanatory. To get the payload length, we are subtracting 128 (the mask bit) from byte 1. (look at the frame structure and you\u0026rsquo;ll have a clear picture) Encrypted payload XORed with the mask will give us the decrypted payload.\n4. Sending Frames While sending frames, we will do nothing fancy. We will not set MASK bit and we will send data unmasked i.e. in plain text. So that will leave us with filling the FIN bit, the OPCODE, the LEN and finally the payload. Have a look:\ndef send_frame(self, payload): # setting fin to 1 and opcpde to 0x1 frame = [129] # adding len. no masking hence not doing +128 frame += [len(payload)] # adding payload frame_to_send = bytearray(frame) + payload self.request.sendall(frame_to_send) Yep, that easy. So that wraps up our server. Now let\u0026rsquo;s have a look at how can we make it on roll. Fire up a web browser console and try these out:\nWe asked our browser side websocket to print whatever it receives in console. And our server is sending back whatever the client sends. So there you are. The mighty WebSockets with \u0026lt;80 lines of python code 😀 Check it out on GitHub.\nSome interesting links which helped me get here:\nhttps://www.fullstackpython.com/websockets.html https://blog.pusher.com/websockets-from-scratch If you want production grade websocket implementations:\nAutobahn-Python aiohttp (I have used this personally) ","permalink":"http://localhost:1313/posts/websocket-server-python/","summary":"Echo websocket server implemented by hand on raw TCP Sockets.\nJourney to websocket was pretty long. I started with an idea to make an app which can play music in sync across the devices during college period. No wonder I couldn\u0026rsquo;t get through it. Later this year I stumbled upon this new thing called WebSockets and they were intriguing. I thought I could finish that app with websockets (and I did, with partial success).","title":"Writing Simple WebSocket Server in Python: PyWSocket"},{"content":"So the CDH Cluster was replaced by HDP Cluster and everything was going smooth for time being. Until the time when I started getting a dead RegionServer. Frequently. So a deep dive was needed to dig out what indeed was happening. And it turned out to be a long dive.\nThe following was the logline:\n2017-05-23 06:59:22,173 FATAL [regionserver/\u0026lt;hostname\u0026gt;/10.10.205.55:16020] regionserver.HRegionServer: ABORTING region server \u0026lt;hostname\u0026gt;,16020,1493962926376: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing\u0026lt;hostname\u0026gt;,16020,1493962926376 as dead server This alone did not tell much. Further scrolling up in logs, I found this:\n2017-05-24 04:55:34,712 INFO [RS_OPEN_REGION-hdps01:16020-2-SendThread(\u0026lt;zkhost\u0026gt;:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 31947ms for sessionid 0x15be7e4d09e1c4c, closing socket connection and attempting reconnect 2017-05-24 04:55:34,713 WARN [regionserver/\u0026lt;rs-host\u0026gt;/10.10.205.55:16020] util.Sleeper: We slept 16865ms instead of 3000ms, this is likely due to a long garbage collecting pause and it\u0026#39;s usually bad, see //hbase.apache.org/book.html#trouble.rs.runtime.zkexpired 2017-05-24 04:55:34,718 WARN [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 15598ms 1434184 No GCs detected So according to these lines, something paused the JVM and it was not able to send hart-beat to zookeeper. The node there in zookeeper expired and the HBase Master marked it as dead regionserver. They also have given a link in log line which points out to a solution. Go ahead and have a look at it. So basically it says these could be the reasons behind:\nNot enough RAM while running large imports. Swap partition enabled Something hogging the CPU Low ZooKeeper timeout. So first three options were ruled out for my dead regionserver. What I had to do was increase zookeeper timeout. BUT! I already had set the zookeeper session timeout (zookeeper.session.timeout) to 30min. And dead regionserver appeared only after 30sec or so.\nAs suggested in link above, I tried to set tickTime value (hbase.zookeeper.property.tickTime) to 6s. The calculation is something like this:\nMin timeout = 2 * tickTime\nMax timeout = 20 * tickTime\nSo here it must be 120 sec as a zookeeper timeout. But still timeouts were occurring about 40s after.I pulled up GC longs, there was no such long GC. So I went in and checked zookeeper config. Zookeeper also had tickTime value. Confusing it was and now I was not sure which tickTime was be applicable to the dead regionserver. There were two different values of ticktime. One in ZK and one in HBase.\nAs confusing as it was, I went to check zookeeper logs. There I found the cause; the sessions negotiated with zookeeper from dead regionserver were of 40s (20*2s of original ticktime) So clearly the timeout and ticktime we set in regionserver on hbase side were not taking effect. Strange! Roaming around in HDP Server\u0026rsquo;s UI, I found this glorious piece of help-text. (Could not take a screenshot as that pop-up appeared only when I hover 😛 )\nNow it was evident that if you are using different set of zookeeper quorum, the value set in hbase wont affect! **What? Why do you not print this in bold on some heading?? **So this was the cause and the ticktime was needed to set in zookeeper (as done already: see the screenshot ^^). Many times GC cause long delays. Apart from increasing timeout, you may also want to tune your GC. Also you may want to look into Tuning Your HBase or maybe benchmark it.\nAfter increasing tickTime in zookeeper, it\u0026rsquo;s running fine and it\u0026rsquo;s been two days. Let\u0026rsquo;s hope it just stays that way 🙂\n","permalink":"http://localhost:1313/posts/hbase-dead-regionserver/","summary":"So the CDH Cluster was replaced by HDP Cluster and everything was going smooth for time being. Until the time when I started getting a dead RegionServer. Frequently. So a deep dive was needed to dig out what indeed was happening. And it turned out to be a long dive.\nThe following was the logline:\n2017-05-23 06:59:22,173 FATAL [regionserver/\u0026lt;hostname\u0026gt;/10.10.205.55:16020] regionserver.HRegionServer: ABORTING region server \u0026lt;hostname\u0026gt;,16020,1493962926376: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing\u0026lt;hostname\u0026gt;,16020,1493962926376 as dead server This alone did not tell much.","title":"HBase YouAreDeadException: Dead RegionServer due to GC Pause"},{"content":"You would have wondered if i\u0026rsquo;s possible to reconfigure the PL part without any interruption while PS is running Linux. Well, i\u0026rsquo;s possible and as simple as,\necho \u0026#39;0\u0026#39; \u0026gt; /sys/devices/soc0/amba/f8007000.devcfg/is_partial_bitstream #echo \u0026#39;1\u0026#39; for partial bitstreams cat whatever_the_bit_file_name_is.bit \u0026amp;gt; /dev/xdevcfg Yeah, tha\u0026rsquo;s it! Make sure you\u0026rsquo;re running it as root.\nDon\u0026rsquo;t have a nice Linux running on ZedBoard yet? have a look at PYNQ Linux on ZedBoard\n","permalink":"http://localhost:1313/posts/reconfigure-zynq-pl-go/","summary":"You would have wondered if i\u0026rsquo;s possible to reconfigure the PL part without any interruption while PS is running Linux. Well, i\u0026rsquo;s possible and as simple as,\necho \u0026#39;0\u0026#39; \u0026gt; /sys/devices/soc0/amba/f8007000.devcfg/is_partial_bitstream #echo \u0026#39;1\u0026#39; for partial bitstreams cat whatever_the_bit_file_name_is.bit \u0026amp;gt; /dev/xdevcfg Yeah, tha\u0026rsquo;s it! Make sure you\u0026rsquo;re running it as root.\nDon\u0026rsquo;t have a nice Linux running on ZedBoard yet? have a look at PYNQ Linux on ZedBoard","title":"How to reconfigure Zynq-PL on-the-go?"},{"content":"Hi There!\nThe PYNQ Linux is a fun, easy and maker-friendly Ubuntu 15.04 rootfs. It comes bundled with the PYNQ-Z1 board, and the official documentations doesn\u0026rsquo;t even utter a word on how to build or port this image on any other Zynq. Maybe because it\u0026rsquo;s too obvious how to do so.\nWhat you need to run Linux on any ARM board?\nBOOT image (BOOT.bin) kernel image (uImage) devicetree blob (devicetree.dtb) rootfs What we need to worry about? everything but the rootfs.\nTL;DR Write the image file on SD card same as in the pynq tutorial, and replace the files in BOOT partition with these… ZedBoard BOOT files\nUpdate:\nuse older image of pynq from here\nDetailed Explanation (as if you asked for it) 0. Set up environment. You\u0026rsquo;ll obviously need to do this all a Desktop Linux system. I\u0026rsquo;ve tried doing onWindows, doesn\u0026rsquo;t work well with MinGW. Haven\u0026rsquo;t tried CygWin or Linux Subsystem for Windows, you may give it a try if you\u0026rsquo;re brave enough.\n0.1 Set up the Bash environment to work with Xilinx SDK tools, source /opt/Xilinx/SDK/2016.3/settings64.sh source /opt/Xilinx/Vivado/2016.3/settings64.sh # For Older SDKs before 2017.1 export CROSS_COMPILE=arm-xilinx-linux-gnueabi- Update:\nFor latest SDK version 2017.3 the cross compiler is arm-linux-gnueabihf-gcc.\n# For new SDKs on or after 2017.1 export CROSS_COMPILE=arm-linux-gnueabihf-gcc- 0.2 SD Card Patitioning you don\u0026rsquo;t really need to create partitions of the SD card because loading the image file from the PYNQ linux will do that for you, however you can do it anyways which should look like… (Use GParted or something)\nPartition-1:\nType: fat32 Free Space Preceding: 4MB (IMPORTANT!!!) Size: 52MB Label: BOOT (Optional) Partition-2:\nType: ext4 Size: whatever is left Label: rootfs (Optional) Notes DO NOT REMOVE SD CARD WITHOUT UNMOUNTING IT! Really, don\u0026rsquo;t. It creates badblocks in the SD card which might cause issues booting up the Zynq. You can check my GitHub @parthpower for the modified u-boot and Linux codes to work around this. 1. Make BOOT.bin -\u0026gt; To create BOOT.bin, you\u0026rsquo;ll need 3 things (which you\u0026rsquo;d have heard everywhere).\nFirst Stage Bootloader to dump the bitstream and call the second stage bootloader bitstream Second Stage Bootloader (u-boot.elf) I\u0026rsquo;m not going in details of first two things, as they are simple and you can just google them… Come on now! Ok, in short, make Vivado project with only PS, export it to the SDK, create FSLB application project and you\u0026rsquo;re good!\nLet\u0026rsquo;s get to the main part, MAKING u-boot.elf!!!!\nClone the Xilinx UBoot repo from the Xilinx github, checkout to the latest stable release (I used xilinx-v2016.4) (in case you need code)\ngit clone //github.com/Xilinx/u-boot-xlnx.git cd u-boot-xlnx git checkout xilinx-v2016.4 Now, you got to change some stuff in u-boot-xlnx/include/configs/zynq-common.h, find this line “sdboot=if mmcinfo;” and make it look like as below\n\u0026#34;sdboot=if mmcinfo; then \u0026#34; \\ \u0026#34;run uenvboot; \u0026#34; \\ \u0026#34;echo Copying Linux from SD to RAM...RFS in ext4 \u0026amp;\u0026amp; \u0026#34; \\ \u0026#34;load mmc 0 ${kernel_load_address} ${kernel_image} \u0026amp;\u0026amp; \u0026#34; \\ \u0026#34;load mmc 0 ${devicetree_load_address} ${devicetree_image} \u0026amp;\u0026amp; \u0026#34; \\ \u0026#34;bootm ${kernel_load_address} - ${devicetree_load_address}; \u0026#34; \\ Wait, what did we just messed with? Like what the hell happened?\nIf you go through the “Ubuntu on ZedBoard” tutorial by Avnet, they explain it\u0026rsquo;s for using the rootfs from the filesystem instead of a RAM disk.\nTo make u-boot,\nmake zynq_zed_config make You should add _u-boot-xlnx/tools _to the PATH. It\u0026rsquo;ll be useful making the Linux Image later on.\nexport PATH=\u0026#39;uboot-xlnx/tools:\u0026#39;$PATH Get the “u-boot” file from the root of the u-boot source directory, rename it to u-boot.elf. Now just make the BOOT.bin, open “Create Boot Image” in SDK use the FSBL.elf, bit file and the u-boot.elf. Mind the order of the files because that\u0026rsquo;s how the Zynq will be booted. FSBL configures the PL with the bit file then loads the second stage bootloader; u-boot.\n2. Make Linux Kernel Image The Linux Kernel Image is the actual Kernel being loaded by the u-boot and which has the rootfs of pynq distribution. In less technical words, Linux Kernel is the base system which loads the higher-level root filesystem from the mount point. Now we\u0026rsquo;re done with the theory part, Let\u0026rsquo;s make the kernel!\nFetch the sources,\ngit clone //github.com/Xilinx/linux-xlnx.git git checkout -b xlnx_3.17\u0026lt;/pre\u0026gt; Just default config, and make it!\nmake ARCH=arm xilinx_zynq_defconfig make ARCH=arm UIMAGE_LOADADDR=0x8000 uImage That\u0026rsquo;s it and you have your Linux kernel image ready at _linux-xlnx/arch/arm/boot/uImage _save it somewhere safe.\n3. Device Tree Blob What? Device tree blob is a “block-of-binary” which specifies memory location of the peripherals, their compatible driver names, and some configurations of the peripherals. It\u0026rsquo;s lots of information about the hardware specification of the SoC.\nThe dtb is compiled from dts (device tree string). For a PS-only design, the dts is already there in the linux-xlnx repository at /linux-xlnx/arch/arm/boot/dts/zynq-zed.dts\nWe need to modify it a little bit to load the rootfs, (yeah, lots of efforts just to load the rootfs)\nfind the line starting with _bootargs _and change it to,\nBy default, it has bootargs are set to load the rootfs from a ramdisk. But we configured the u-boot not to load the rootfs as ramdisk. So we are mounting it from /dev/mmcblk0p2 (partition 2 of the memory card)\nNow go to the root of linux-xlnx and\nand you have the dtb file at /linux-xlnx/arch/arm/boot/dts/zynq-zed.dtb rename it to devicetree.dtb and copy it somewhere safe.\n4. rootfs The root file system we want to use if the one provided by the PYNQ. Download the latest version from //pynq.io. Extract it, and write the image to the sd card. Use _dd _or Win32DiskImage tool or whatever you wish.\nUpdate:\nThe latest PYNQ distribution has kernel v4.6. You can use older image from,\npynq_z1_image_2016_09_14.zip\nMount the SD card, look into the BOOT partition, replace all the files, BOOT.bin, uImage, devicetree.dtb with the ones we\u0026rsquo;ve created, and you\u0026rsquo;re done. Seriously, you\u0026rsquo;re done!\nGood thing is, you can put any Linux distribution as long as it is made up on Linux Kernel 3.17 in the rootfs file system and it should work. I tried Linaro , it quite didn\u0026rsquo;t work well with me. Try playing around, let me know what works and what don\u0026rsquo;t work!\nHappy Hacking!\nSome Really Good References,\nEmbedded-Linux-Tutorial-Zybo\nUbuntu_on_Zynq_Tutorial_03.pdf Yet Another Guide to Running Linaro Ubuntu Linux Desktop on Xilinx Zynq on the ZedBoard www.wiki.xilinx.com/\nUpdate PYNQ has recently updated their repository and included instructions to make sd card image section. pynq_sd_card Thanks to Cathal McCabe for this update. ","permalink":"http://localhost:1313/posts/pynq-linux-on-zedboard/","summary":"Hi There!\nThe PYNQ Linux is a fun, easy and maker-friendly Ubuntu 15.04 rootfs. It comes bundled with the PYNQ-Z1 board, and the official documentations doesn\u0026rsquo;t even utter a word on how to build or port this image on any other Zynq. Maybe because it\u0026rsquo;s too obvious how to do so.\nWhat you need to run Linux on any ARM board?\nBOOT image (BOOT.bin) kernel image (uImage) devicetree blob (devicetree.dtb) rootfs What we need to worry about?","title":"PYNQ Linux on ZedBoard"},{"content":"As a part of migration from CDH cluster to HDP cluster, we also had to migrate OpenTSDB which was running on CDH cluster. There are many methods to copy/transfer data between clusters and what we used here was ExportSnapshot.\nSo these are the steps roughly:\nStop TSDs Take snapshot(s) Transfer snapshots Restore snapshots Modify and start TSDs Steps 1 and 5 are self understood. We will look at how to take,transfer and restore snapshots.\nSnapshot OpenTSDB Tables: Fire up hbase shell and take snapshot of opentsdb tables. You can choose any name for snapshot to be created.\nhbase\u0026gt; snapshot \u0026#39;tsdb\u0026#39;, \u0026#39;tsdb-\u0026lt;date\u0026gt;\u0026#39; hbase\u0026gt; snapshot \u0026#39;tsdb-uid\u0026#39;, \u0026#39;tsdb-uid-\u0026lt;date\u0026gt;\u0026#39; hbase\u0026gt; snapshot \u0026#39;tsdb-meta\u0026#39;, \u0026#39;tsdb-meta-\u0026lt;date\u0026gt;\u0026#39; hbase\u0026gt; snapshot \u0026#39;tsdb-tree\u0026#39;, \u0026#39;tsdb-tree-\u0026lt;date\u0026gt;\u0026#39; Transfer OpenTSDB Tables to New HBase Cluster Here we will use ExportSnapshot utility provided by HBase. It takes snapshot name, destination cluster address as primary arguments. As it will run as a map-reduce job, you can also specify number of mapper to use.\n$ sudo su - hdfs export HADOOP_MAPRED_HOME=/usr/lib/hadoop-0.20-mapreduce/ export HADOOP_HOME=/usr/lib/hadoop export HBASE_HOME=/usr/lib/hbase $ hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot tsdb-\u0026lt;date\u0026gt; -copy-to hdfs://\u0026lt;dest-hdfs\u0026gt;:8020/hbase -mappers 4 Repeat last line for other three opentsdb tables also. This will copy all the snapshot meta data and related HFiles to destination cluster. Make sure you set correct hbase path in destination cluster.\nOnce done you can check for copied snapshot files in destination cluster. You also want to check owner of the copied files so that HBase can access it properly. In case of HDP cluster it needed to get changed to _hbase:hdfs _and here is how to do it.\n$ sudo su - hdfs hadoop fs -chown -R hbase:hdfs /apps/hbase/data/archive hadoop fs -chown -R hbase:hdfs /apps/hbase/data/.hbase-snapshot/tsdb-\u0026lt;date\u0026gt; You may want to repeat last line for all four opentsdb tables snapshots.\nRestoring OpenTSDB Table Snapshots Now to list and restore snapshots on destination cluster, you can do the following in HBase shell. The first command should list all the snapshot and that should include all four opentsdb table snapshots we just transferred. The second line will restore a snapshot into a table.\nhbase\u0026gt; list_snapshots hbase\u0026gt; restore_snapshot \u0026#39;tsdb-\u0026lt;date\u0026gt;\u0026#39; If you have already created tables in destination cluster then you may want to disable table first and then restore snapshot and then enable it. Repeat the second line for all four tsdb tables.\nOnce all tables are restored and you have verified it, you can change HBase address in TSDs and start them.\nUpdate: It is also recommended to run major compaction on tables to reattain data locality.\n","permalink":"http://localhost:1313/posts/migrate-opentsdb-hbase/","summary":"As a part of migration from CDH cluster to HDP cluster, we also had to migrate OpenTSDB which was running on CDH cluster. There are many methods to copy/transfer data between clusters and what we used here was ExportSnapshot.\nSo these are the steps roughly:\nStop TSDs Take snapshot(s) Transfer snapshots Restore snapshots Modify and start TSDs Steps 1 and 5 are self understood. We will look at how to take,transfer and restore snapshots.","title":"Migrating OpenTSDB to Another HBase Cluster"},{"content":"Hi,\nIf you\u0026rsquo;re an FPGA fan or someone who\u0026rsquo;s got PYNQ board for fun, you might be having a hard time making it run Vivado SDK projects. That\u0026rsquo;s because, the PYNQ-Z1, the cheap Zynq-7020 board doesn\u0026rsquo;t have any popular DDR ram on board. You need to configure it by hand, however, tcl is at your rescue.\nWhen you create a project and include Zynq-PS system to the block diagram, most of the time you don\u0026rsquo;t need to change the DDR timing properties because of most of the popular boards ie. Zedboard and Zybo use standard memory parts which are pre-configured in the Vivado. However, if you\u0026rsquo;ve created an Overlay and used it with the Pynq Linux, it doesn\u0026rsquo;t make any difference because overlays are PL configurations only, they don\u0026rsquo;t mess with PS part or DDR interface of PS part.\nTL;DR To configure the memory on the PYNQ just do this copy paste in tcl console and you\u0026rsquo;re all set for SDK.\nset_property -dict [ list \\\rCONFIG.PCW_USE_M_AXI_GP0 {1}\\\rCONFIG.PCW_DDR_RAM_BASEADDR {0x00100000} \\\rCONFIG.PCW_DDR_RAM_HIGHADDR {0x1FFFFFFF} \\\rCONFIG.PCW_UART0_BASEADDR {0xE0000000} \\\rCONFIG.PCW_UART0_HIGHADDR {0xE0000FFF} \\\rCONFIG.PCW_UART1_BASEADDR {0xE0001000} \\\rCONFIG.PCW_UART1_HIGHADDR {0xE0001FFF} \\\rCONFIG.PCW_I2C0_BASEADDR {0xE0004000} \\\rCONFIG.PCW_I2C0_HIGHADDR {0xE0004FFF} \\\rCONFIG.PCW_I2C1_BASEADDR {0xE0005000} \\\rCONFIG.PCW_I2C1_HIGHADDR {0xE0005FFF} \\\rCONFIG.PCW_SPI0_BASEADDR {0xE0006000} \\\rCONFIG.PCW_SPI0_HIGHADDR {0xE0006FFF} \\\rCONFIG.PCW_SPI1_BASEADDR {0xE0007000} \\\rCONFIG.PCW_SPI1_HIGHADDR {0xE0007FFF} \\\rCONFIG.PCW_CAN0_BASEADDR {0xE0008000} \\\rCONFIG.PCW_CAN0_HIGHADDR {0xE0008FFF} \\\rCONFIG.PCW_CAN1_BASEADDR {0xE0009000} \\\rCONFIG.PCW_CAN1_HIGHADDR {0xE0009FFF} \\\rCONFIG.PCW_GPIO_BASEADDR {0xE000A000} \\\rCONFIG.PCW_GPIO_HIGHADDR {0xE000AFFF} \\\rCONFIG.PCW_ENET0_BASEADDR {0xE000B000} \\\rCONFIG.PCW_ENET0_HIGHADDR {0xE000BFFF} \\\rCONFIG.PCW_ENET1_BASEADDR {0xE000C000} \\\rCONFIG.PCW_ENET1_HIGHADDR {0xE000CFFF} \\\rCONFIG.PCW_SDIO0_BASEADDR {0xE0100000} \\\rCONFIG.PCW_SDIO0_HIGHADDR {0xE0100FFF} \\\rCONFIG.PCW_SDIO1_BASEADDR {0xE0101000} \\\rCONFIG.PCW_SDIO1_HIGHADDR {0xE0101FFF} \\\rCONFIG.PCW_USB0_BASEADDR {0xE0102000} \\\rCONFIG.PCW_USB0_HIGHADDR {0xE0102fff} \\\rCONFIG.PCW_USB1_BASEADDR {0xE0103000} \\\rCONFIG.PCW_USB1_HIGHADDR {0xE0103fff} \\\rCONFIG.PCW_TTC0_BASEADDR {0xE0104000} \\\rCONFIG.PCW_TTC0_HIGHADDR {0xE0104fff} \\\rCONFIG.PCW_TTC1_BASEADDR {0xE0105000} \\\rCONFIG.PCW_TTC1_HIGHADDR {0xE0105fff} \\\rCONFIG.PCW_FCLK_CLK0_BUF {true} \\\rCONFIG.PCW_FCLK_CLK1_BUF {false} \\\rCONFIG.PCW_FCLK_CLK2_BUF {false} \\\rCONFIG.PCW_FCLK_CLK3_BUF {false} \\\rCONFIG.PCW_UIPARAM_DDR_FREQ_MHZ {525} \\\rCONFIG.PCW_UIPARAM_DDR_BANK_ADDR_COUNT {3} \\\rCONFIG.PCW_UIPARAM_DDR_ROW_ADDR_COUNT {15} \\\rCONFIG.PCW_UIPARAM_DDR_COL_ADDR_COUNT {10} \\\rCONFIG.PCW_UIPARAM_DDR_CL {7} \\\rCONFIG.PCW_UIPARAM_DDR_CWL {6} \\\rCONFIG.PCW_UIPARAM_DDR_T_RCD {7} \\\rCONFIG.PCW_UIPARAM_DDR_T_RP {7} \\\rCONFIG.PCW_UIPARAM_DDR_T_RC {48.91} \\\rCONFIG.PCW_UIPARAM_DDR_T_RAS_MIN {35.0} \\\rCONFIG.PCW_UIPARAM_DDR_T_FAW {40.0} \\\rCONFIG.PCW_UIPARAM_DDR_AL {0} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_TO_CLK_DELAY_0 {0.040} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_TO_CLK_DELAY_1 {0.058} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_TO_CLK_DELAY_2 {-0.009} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_TO_CLK_DELAY_3 {-0.033} \\\rCONFIG.PCW_UIPARAM_DDR_BOARD_DELAY0 {0.223} \\\rCONFIG.PCW_UIPARAM_DDR_BOARD_DELAY1 {0.212} \\\rCONFIG.PCW_UIPARAM_DDR_BOARD_DELAY2 {0.085} \\\rCONFIG.PCW_UIPARAM_DDR_BOARD_DELAY3 {0.092} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_0_LENGTH_MM {15.6} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_1_LENGTH_MM {18.8} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_2_LENGTH_MM {0} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_3_LENGTH_MM {0} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_0_LENGTH_MM {16.5} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_1_LENGTH_MM {18} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_2_LENGTH_MM {0} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_3_LENGTH_MM {0} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_0_LENGTH_MM {25.8} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_1_LENGTH_MM {25.8} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_2_LENGTH_MM {0} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_3_LENGTH_MM {0} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_0_PACKAGE_LENGTH {105.056} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_1_PACKAGE_LENGTH {66.904} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_2_PACKAGE_LENGTH {89.1715} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_3_PACKAGE_LENGTH {113.63} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_0_PACKAGE_LENGTH {98.503} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_1_PACKAGE_LENGTH {68.5855} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_2_PACKAGE_LENGTH {90.295} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_3_PACKAGE_LENGTH {103.977} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_0_PACKAGE_LENGTH {80.4535} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_1_PACKAGE_LENGTH {80.4535} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_2_PACKAGE_LENGTH {80.4535} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_3_PACKAGE_LENGTH {80.4535} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_0_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_1_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_2_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_UIPARAM_DDR_DQS_3_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_0_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_1_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_2_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_UIPARAM_DDR_DQ_3_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_0_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_1_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_2_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_3_PROPOGATION_DELAY {160} \\\rCONFIG.PCW_PACKAGE_DDR_DQS_TO_CLK_DELAY_0 {0.040} \\\rCONFIG.PCW_PACKAGE_DDR_DQS_TO_CLK_DELAY_1 {0.058} \\\rCONFIG.PCW_PACKAGE_DDR_DQS_TO_CLK_DELAY_2 {-0.009} \\\rCONFIG.PCW_PACKAGE_DDR_DQS_TO_CLK_DELAY_3 {-0.033} \\\rCONFIG.PCW_PACKAGE_DDR_BOARD_DELAY0 {0.223} \\\rCONFIG.PCW_PACKAGE_DDR_BOARD_DELAY1 {0.212} \\\rCONFIG.PCW_PACKAGE_DDR_BOARD_DELAY2 {0.085} \\\rCONFIG.PCW_PACKAGE_DDR_BOARD_DELAY3 {0.092} \\\rCONFIG.PCW_CPU_CPU_6X4X_MAX_RANGE {667} \\\rCONFIG.PCW_CRYSTAL_PERIPHERAL_FREQMHZ {50} \\\rCONFIG.PCW_APU_PERIPHERAL_FREQMHZ {650} \\\rCONFIG.PCW_DCI_PERIPHERAL_FREQMHZ {10.159} \\\rCONFIG.PCW_QSPI_PERIPHERAL_FREQMHZ {200} \\\rCONFIG.PCW_SMC_PERIPHERAL_FREQMHZ {100} \\\rCONFIG.PCW_USB0_PERIPHERAL_FREQMHZ {60} \\\rCONFIG.PCW_USB1_PERIPHERAL_FREQMHZ {60} \\\rCONFIG.PCW_SDIO_PERIPHERAL_FREQMHZ {50} \\\rCONFIG.PCW_UART_PERIPHERAL_FREQMHZ {100} \\\rCONFIG.PCW_SPI_PERIPHERAL_FREQMHZ {166.666666} \\\rCONFIG.PCW_CAN_PERIPHERAL_FREQMHZ {100} \\\rCONFIG.PCW_CAN0_PERIPHERAL_FREQMHZ {-1} \\\rCONFIG.PCW_CAN1_PERIPHERAL_FREQMHZ {-1} \\\rCONFIG.PCW_I2C_PERIPHERAL_FREQMHZ {25} \\\rCONFIG.PCW_WDT_PERIPHERAL_FREQMHZ {133.333333} \\\rCONFIG.PCW_TTC_PERIPHERAL_FREQMHZ {50} \\\rCONFIG.PCW_TTC0_CLK0_PERIPHERAL_FREQMHZ {133.333333} \\\rCONFIG.PCW_TTC0_CLK1_PERIPHERAL_FREQMHZ {133.333333} \\\rCONFIG.PCW_TTC0_CLK2_PERIPHERAL_FREQMHZ {133.333333} \\\rCONFIG.PCW_TTC1_CLK0_PERIPHERAL_FREQMHZ {133.333333} \\\rCONFIG.PCW_TTC1_CLK1_PERIPHERAL_FREQMHZ {133.333333} \\\rCONFIG.PCW_TTC1_CLK2_PERIPHERAL_FREQMHZ {133.333333} \\\rCONFIG.PCW_PCAP_PERIPHERAL_FREQMHZ {200} \\\rCONFIG.PCW_TPIU_PERIPHERAL_FREQMHZ {200} \\\rCONFIG.PCW_FPGA0_PERIPHERAL_FREQMHZ {100} \\\rCONFIG.PCW_FPGA1_PERIPHERAL_FREQMHZ {50} \\\rCONFIG.PCW_FPGA2_PERIPHERAL_FREQMHZ {50} \\\rCONFIG.PCW_FPGA3_PERIPHERAL_FREQMHZ {50} \\\rCONFIG.PCW_ACT_APU_PERIPHERAL_FREQMHZ {650.000000} \\\rCONFIG.PCW_UIPARAM_ACT_DDR_FREQ_MHZ {525.000000} \\\rCONFIG.PCW_ACT_DCI_PERIPHERAL_FREQMHZ {10.096154} \\\rCONFIG.PCW_ACT_QSPI_PERIPHERAL_FREQMHZ {200.000000} \\\rCONFIG.PCW_ACT_SMC_PERIPHERAL_FREQMHZ {10.000000} \\\rCONFIG.PCW_ACT_ENET0_PERIPHERAL_FREQMHZ {125.000000} \\\rCONFIG.PCW_ACT_ENET1_PERIPHERAL_FREQMHZ {10.000000} \\\rCONFIG.PCW_ACT_USB0_PERIPHERAL_FREQMHZ {60} \\\rCONFIG.PCW_ACT_USB1_PERIPHERAL_FREQMHZ {60} \\\rCONFIG.PCW_ACT_SDIO_PERIPHERAL_FREQMHZ {50.000000} \\\rCONFIG.PCW_ACT_UART_PERIPHERAL_FREQMHZ {100.000000} \\\rCONFIG.PCW_ACT_SPI_PERIPHERAL_FREQMHZ {10.000000} \\\rCONFIG.PCW_ACT_CAN_PERIPHERAL_FREQMHZ {10.000000} \\\rCONFIG.PCW_ACT_CAN0_PERIPHERAL_FREQMHZ {23.8095} \\\rCONFIG.PCW_ACT_CAN1_PERIPHERAL_FREQMHZ {23.8095} \\\rCONFIG.PCW_ACT_I2C_PERIPHERAL_FREQMHZ {50} \\\rCONFIG.PCW_ACT_WDT_PERIPHERAL_FREQMHZ {108.333336} \\\rCONFIG.PCW_ACT_TTC_PERIPHERAL_FREQMHZ {50} \\\rCONFIG.PCW_ACT_PCAP_PERIPHERAL_FREQMHZ {200.000000} \\\rCONFIG.PCW_ACT_TPIU_PERIPHERAL_FREQMHZ {200.000000} \\\rCONFIG.PCW_ACT_FPGA0_PERIPHERAL_FREQMHZ {100.000000} \\\rCONFIG.PCW_ACT_FPGA1_PERIPHERAL_FREQMHZ {50.000000} \\\rCONFIG.PCW_ACT_FPGA2_PERIPHERAL_FREQMHZ {50.000000} \\\rCONFIG.PCW_ACT_FPGA3_PERIPHERAL_FREQMHZ {50.000000} \\\rCONFIG.PCW_ACT_TTC0_CLK0_PERIPHERAL_FREQMHZ {108.333336} \\\rCONFIG.PCW_ACT_TTC0_CLK1_PERIPHERAL_FREQMHZ {108.333336} \\\rCONFIG.PCW_ACT_TTC0_CLK2_PERIPHERAL_FREQMHZ {108.333336} \\\rCONFIG.PCW_ACT_TTC1_CLK0_PERIPHERAL_FREQMHZ {108.333336} \\\rCONFIG.PCW_ACT_TTC1_CLK1_PERIPHERAL_FREQMHZ {108.333336} \\\rCONFIG.PCW_ACT_TTC1_CLK2_PERIPHERAL_FREQMHZ {108.333336} \\\rCONFIG.PCW_CLK0_FREQ {100000000} \\\rCONFIG.PCW_CLK1_FREQ {50000000} \\\rCONFIG.PCW_CLK2_FREQ {50000000} \\\rCONFIG.PCW_CLK3_FREQ {50000000} \\\rCONFIG.PCW_OVERRIDE_BASIC_CLOCK {0} \\\rCONFIG.PCW_CPU_PERIPHERAL_DIVISOR0 {2} \\\rCONFIG.PCW_DDR_PERIPHERAL_DIVISOR0 {2} \\\rCONFIG.PCW_SMC_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_QSPI_PERIPHERAL_DIVISOR0 {5} \\\rCONFIG.PCW_SDIO_PERIPHERAL_DIVISOR0 {20} \\\rCONFIG.PCW_UART_PERIPHERAL_DIVISOR0 {10} \\\rCONFIG.PCW_SPI_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_CAN_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_CAN_PERIPHERAL_DIVISOR1 {1} \\\rCONFIG.PCW_FCLK0_PERIPHERAL_DIVISOR0 {10} \\\rCONFIG.PCW_FCLK1_PERIPHERAL_DIVISOR0 {20} \\\rCONFIG.PCW_FCLK2_PERIPHERAL_DIVISOR0 {20} \\\rCONFIG.PCW_FCLK3_PERIPHERAL_DIVISOR0 {20} \\\rCONFIG.PCW_FCLK0_PERIPHERAL_DIVISOR1 {1} \\\rCONFIG.PCW_FCLK1_PERIPHERAL_DIVISOR1 {1} \\\rCONFIG.PCW_FCLK2_PERIPHERAL_DIVISOR1 {1} \\\rCONFIG.PCW_FCLK3_PERIPHERAL_DIVISOR1 {1} \\\rCONFIG.PCW_ENET0_PERIPHERAL_DIVISOR0 {8} \\\rCONFIG.PCW_ENET1_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_ENET0_PERIPHERAL_DIVISOR1 {1} \\\rCONFIG.PCW_ENET1_PERIPHERAL_DIVISOR1 {1} \\\rCONFIG.PCW_TPIU_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_DCI_PERIPHERAL_DIVISOR0 {52} \\\rCONFIG.PCW_DCI_PERIPHERAL_DIVISOR1 {2} \\\rCONFIG.PCW_PCAP_PERIPHERAL_DIVISOR0 {5} \\\rCONFIG.PCW_TTC0_CLK0_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_TTC0_CLK1_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_TTC0_CLK2_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_TTC1_CLK0_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_TTC1_CLK1_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_TTC1_CLK2_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_WDT_PERIPHERAL_DIVISOR0 {1} \\\rCONFIG.PCW_ARMPLL_CTRL_FBDIV {26} \\\rCONFIG.PCW_IOPLL_CTRL_FBDIV {20} \\\rCONFIG.PCW_DDRPLL_CTRL_FBDIV {21} \\\rCONFIG.PCW_CPU_CPU_PLL_FREQMHZ {1300.000} \\\rCONFIG.PCW_IO_IO_PLL_FREQMHZ {1000.000} \\\rCONFIG.PCW_DDR_DDR_PLL_FREQMHZ {1050.000} \\\rCONFIG.PCW_SMC_PERIPHERAL_VALID {0} \\\rCONFIG.PCW_SDIO_PERIPHERAL_VALID {1} \\\rCONFIG.PCW_SPI_PERIPHERAL_VALID {0} \\\rCONFIG.PCW_CAN_PERIPHERAL_VALID {0} \\\rCONFIG.PCW_UART_PERIPHERAL_VALID {1} \\\rCONFIG.PCW_EN_EMIO_CAN0 {0} \\\rCONFIG.PCW_EN_EMIO_CAN1 {0} \\\rCONFIG.PCW_EN_EMIO_ENET0 {0} \\\rCONFIG.PCW_EN_EMIO_ENET1 {0} \\\rCONFIG.PCW_EN_PTP_ENET0 {0} \\\rCONFIG.PCW_EN_PTP_ENET1 {0} \\\rCONFIG.PCW_EN_EMIO_GPIO {0} \\\rCONFIG.PCW_EN_EMIO_I2C0 {0} \\\rCONFIG.PCW_EN_EMIO_I2C1 {0} \\\rCONFIG.PCW_EN_EMIO_PJTAG {0} \\\rCONFIG.PCW_EN_EMIO_SDIO0 {0} \\\rCONFIG.PCW_EN_EMIO_CD_SDIO0 {0} \\\rCONFIG.PCW_EN_EMIO_WP_SDIO0 {0} \\\rCONFIG.PCW_EN_EMIO_SDIO1 {0} \\\rCONFIG.PCW_EN_EMIO_CD_SDIO1 {0} \\\rCONFIG.PCW_EN_EMIO_WP_SDIO1 {0} \\\rCONFIG.PCW_EN_EMIO_SPI0 {0} \\\rCONFIG.PCW_EN_EMIO_SPI1 {0} \\\rCONFIG.PCW_EN_EMIO_UART0 {0} \\\rCONFIG.PCW_EN_EMIO_UART1 {0} \\\rCONFIG.PCW_EN_EMIO_MODEM_UART0 {0} \\\rCONFIG.PCW_EN_EMIO_MODEM_UART1 {0} \\\rCONFIG.PCW_EN_EMIO_TTC0 {0} \\\rCONFIG.PCW_EN_EMIO_TTC1 {0} \\\rCONFIG.PCW_EN_EMIO_WDT {0} \\\rCONFIG.PCW_EN_EMIO_TRACE {0} \\\rCONFIG.PCW_USE_AXI_NONSECURE {0} \\\rCONFIG.PCW_USE_M_AXI_GP0 {0} \\\rCONFIG.PCW_USE_M_AXI_GP1 {0} \\\rCONFIG.PCW_USE_S_AXI_GP0 {0} \\\rCONFIG.PCW_USE_S_AXI_GP1 {0} \\\rCONFIG.PCW_USE_S_AXI_ACP {0} \\\rCONFIG.PCW_USE_S_AXI_HP0 {0} \\\rCONFIG.PCW_USE_S_AXI_HP1 {0} \\\rCONFIG.PCW_USE_S_AXI_HP2 {0} \\\rCONFIG.PCW_USE_S_AXI_HP3 {0} \\\rCONFIG.PCW_M_AXI_GP0_FREQMHZ {10} \\\rCONFIG.PCW_M_AXI_GP1_FREQMHZ {10} \\\rCONFIG.PCW_S_AXI_GP0_FREQMHZ {10} \\\rCONFIG.PCW_S_AXI_GP1_FREQMHZ {10} \\\rCONFIG.PCW_S_AXI_ACP_FREQMHZ {10} \\\rCONFIG.PCW_S_AXI_HP0_FREQMHZ {10} \\\rCONFIG.PCW_S_AXI_HP1_FREQMHZ {10} \\\rCONFIG.PCW_S_AXI_HP2_FREQMHZ {10} \\\rCONFIG.PCW_S_AXI_HP3_FREQMHZ {10} \\\rCONFIG.PCW_USE_DMA0 {0} \\\rCONFIG.PCW_USE_DMA1 {0} \\\rCONFIG.PCW_USE_DMA2 {0} \\\rCONFIG.PCW_USE_DMA3 {0} \\\rCONFIG.PCW_USE_TRACE {0} \\\rCONFIG.PCW_TRACE_PIPELINE_WIDTH {8} \\\rCONFIG.PCW_INCLUDE_TRACE_BUFFER {0} \\\rCONFIG.PCW_TRACE_BUFFER_FIFO_SIZE {128} \\\rCONFIG.PCW_USE_TRACE_DATA_EDGE_DETECTOR {0} \\\rCONFIG.PCW_TRACE_BUFFER_CLOCK_DELAY {12} \\\rCONFIG.PCW_USE_CROSS_TRIGGER {0} \\\rCONFIG.PCW_FTM_CTI_IN0 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_FTM_CTI_IN1 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_FTM_CTI_IN2 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_FTM_CTI_IN3 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_FTM_CTI_OUT0 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_FTM_CTI_OUT1 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_FTM_CTI_OUT2 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_FTM_CTI_OUT3 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_USE_DEBUG {0} \\\rCONFIG.PCW_USE_CR_FABRIC {1} \\\rCONFIG.PCW_USE_AXI_FABRIC_IDLE {0} \\\rCONFIG.PCW_USE_DDR_BYPASS {0} \\\rCONFIG.PCW_USE_FABRIC_INTERRUPT {0} \\\rCONFIG.PCW_USE_PROC_EVENT_BUS {0} \\\rCONFIG.PCW_USE_EXPANDED_IOP {0} \\\rCONFIG.PCW_USE_HIGH_OCM {0} \\\rCONFIG.PCW_USE_PS_SLCR_REGISTERS {0} \\\rCONFIG.PCW_USE_EXPANDED_PS_SLCR_REGISTERS {0} \\\rCONFIG.PCW_USE_CORESIGHT {0} \\\rCONFIG.PCW_EN_EMIO_SRAM_INT {0} \\\rCONFIG.PCW_GPIO_EMIO_GPIO_WIDTH {64} \\\rCONFIG.PCW_UART0_BAUD_RATE {115200} \\\rCONFIG.PCW_UART1_BAUD_RATE {115200} \\\rCONFIG.PCW_EN_4K_TIMER {0} \\\rCONFIG.PCW_M_AXI_GP0_ID_WIDTH {12} \\\rCONFIG.PCW_M_AXI_GP0_ENABLE_STATIC_REMAP {0} \\\rCONFIG.PCW_M_AXI_GP0_SUPPORT_NARROW_BURST {0} \\\rCONFIG.PCW_M_AXI_GP0_THREAD_ID_WIDTH {12} \\\rCONFIG.PCW_M_AXI_GP1_ID_WIDTH {12} \\\rCONFIG.PCW_M_AXI_GP1_ENABLE_STATIC_REMAP {0} \\\rCONFIG.PCW_M_AXI_GP1_SUPPORT_NARROW_BURST {0} \\\rCONFIG.PCW_M_AXI_GP1_THREAD_ID_WIDTH {12} \\\rCONFIG.PCW_S_AXI_GP0_ID_WIDTH {6} \\\rCONFIG.PCW_S_AXI_GP1_ID_WIDTH {6} \\\rCONFIG.PCW_S_AXI_ACP_ID_WIDTH {3} \\\rCONFIG.PCW_INCLUDE_ACP_TRANS_CHECK {0} \\\rCONFIG.PCW_USE_DEFAULT_ACP_USER_VAL {0} \\\rCONFIG.PCW_S_AXI_ACP_ARUSER_VAL {31} \\\rCONFIG.PCW_S_AXI_ACP_AWUSER_VAL {31} \\\rCONFIG.PCW_S_AXI_HP0_ID_WIDTH {6} \\\rCONFIG.PCW_S_AXI_HP0_DATA_WIDTH {64} \\\rCONFIG.PCW_S_AXI_HP1_ID_WIDTH {6} \\\rCONFIG.PCW_S_AXI_HP1_DATA_WIDTH {64} \\\rCONFIG.PCW_S_AXI_HP2_ID_WIDTH {6} \\\rCONFIG.PCW_S_AXI_HP2_DATA_WIDTH {64} \\\rCONFIG.PCW_S_AXI_HP3_ID_WIDTH {6} \\\rCONFIG.PCW_S_AXI_HP3_DATA_WIDTH {64} \\\rCONFIG.PCW_EN_DDR {1} \\\rCONFIG.PCW_EN_SMC {0} \\\rCONFIG.PCW_EN_QSPI {1} \\\rCONFIG.PCW_EN_CAN0 {0} \\\rCONFIG.PCW_EN_CAN1 {0} \\\rCONFIG.PCW_EN_ENET0 {1} \\\rCONFIG.PCW_EN_ENET1 {0} \\\rCONFIG.PCW_EN_GPIO {1} \\\rCONFIG.PCW_EN_I2C0 {0} \\\rCONFIG.PCW_EN_I2C1 {0} \\\rCONFIG.PCW_EN_PJTAG {0} \\\rCONFIG.PCW_EN_SDIO0 {1} \\\rCONFIG.PCW_EN_SDIO1 {0} \\\rCONFIG.PCW_EN_SPI0 {0} \\\rCONFIG.PCW_EN_SPI1 {0} \\\rCONFIG.PCW_EN_UART0 {1} \\\rCONFIG.PCW_EN_UART1 {0} \\\rCONFIG.PCW_EN_MODEM_UART0 {0} \\\rCONFIG.PCW_EN_MODEM_UART1 {0} \\\rCONFIG.PCW_EN_TTC0 {0} \\\rCONFIG.PCW_EN_TTC1 {0} \\\rCONFIG.PCW_EN_WDT {0} \\\rCONFIG.PCW_EN_TRACE {0} \\\rCONFIG.PCW_EN_USB0 {1} \\\rCONFIG.PCW_EN_USB1 {0} \\\rCONFIG.PCW_DQ_WIDTH {32} \\\rCONFIG.PCW_DQS_WIDTH {4} \\\rCONFIG.PCW_DM_WIDTH {4} \\\rCONFIG.PCW_MIO_PRIMITIVE {54} \\\rCONFIG.PCW_EN_CLK0_PORT {1} \\\rCONFIG.PCW_EN_CLK1_PORT {0} \\\rCONFIG.PCW_EN_CLK2_PORT {0} \\\rCONFIG.PCW_EN_CLK3_PORT {0} \\\rCONFIG.PCW_EN_RST0_PORT {1} \\\rCONFIG.PCW_EN_RST1_PORT {0} \\\rCONFIG.PCW_EN_RST2_PORT {0} \\\rCONFIG.PCW_EN_RST3_PORT {0} \\\rCONFIG.PCW_EN_CLKTRIG0_PORT {0} \\\rCONFIG.PCW_EN_CLKTRIG1_PORT {0} \\\rCONFIG.PCW_EN_CLKTRIG2_PORT {0} \\\rCONFIG.PCW_EN_CLKTRIG3_PORT {0} \\\rCONFIG.PCW_P2F_DMAC_ABORT_INTR {0} \\\rCONFIG.PCW_P2F_DMAC0_INTR {0} \\\rCONFIG.PCW_P2F_DMAC1_INTR {0} \\\rCONFIG.PCW_P2F_DMAC2_INTR {0} \\\rCONFIG.PCW_P2F_DMAC3_INTR {0} \\\rCONFIG.PCW_P2F_DMAC4_INTR {0} \\\rCONFIG.PCW_P2F_DMAC5_INTR {0} \\\rCONFIG.PCW_P2F_DMAC6_INTR {0} \\\rCONFIG.PCW_P2F_DMAC7_INTR {0} \\\rCONFIG.PCW_P2F_SMC_INTR {0} \\\rCONFIG.PCW_P2F_QSPI_INTR {0} \\\rCONFIG.PCW_P2F_CTI_INTR {0} \\\rCONFIG.PCW_P2F_GPIO_INTR {0} \\\rCONFIG.PCW_P2F_USB0_INTR {0} \\\rCONFIG.PCW_P2F_ENET0_INTR {0} \\\rCONFIG.PCW_P2F_SDIO0_INTR {0} \\\rCONFIG.PCW_P2F_I2C0_INTR {0} \\\rCONFIG.PCW_P2F_SPI0_INTR {0} \\\rCONFIG.PCW_P2F_UART0_INTR {0} \\\rCONFIG.PCW_P2F_CAN0_INTR {0} \\\rCONFIG.PCW_P2F_USB1_INTR {0} \\\rCONFIG.PCW_P2F_ENET1_INTR {0} \\\rCONFIG.PCW_P2F_SDIO1_INTR {0} \\\rCONFIG.PCW_P2F_I2C1_INTR {0} \\\rCONFIG.PCW_P2F_SPI1_INTR {0} \\\rCONFIG.PCW_P2F_UART1_INTR {0} \\\rCONFIG.PCW_P2F_CAN1_INTR {0} \\\rCONFIG.PCW_IRQ_F2P_INTR {0} \\\rCONFIG.PCW_IRQ_F2P_MODE {DIRECT} \\\rCONFIG.PCW_CORE0_FIQ_INTR {0} \\\rCONFIG.PCW_CORE0_IRQ_INTR {0} \\\rCONFIG.PCW_CORE1_FIQ_INTR {0} \\\rCONFIG.PCW_CORE1_IRQ_INTR {0} \\\rCONFIG.PCW_VALUE_SILVERSION {3} \\\rCONFIG.PCW_IMPORT_BOARD_PRESET {None} \\\rCONFIG.PCW_PERIPHERAL_BOARD_PRESET {None} \\\rCONFIG.PCW_PRESET_BANK0_VOLTAGE {LVCMOS 3.3V} \\\rCONFIG.PCW_PRESET_BANK1_VOLTAGE {LVCMOS 1.8V} \\\rCONFIG.PCW_UIPARAM_DDR_ENABLE {1} \\\rCONFIG.PCW_UIPARAM_DDR_ADV_ENABLE {0} \\\rCONFIG.PCW_UIPARAM_DDR_MEMORY_TYPE {DDR 3} \\\rCONFIG.PCW_UIPARAM_DDR_ECC {Disabled} \\\rCONFIG.PCW_UIPARAM_DDR_BUS_WIDTH {16 Bit} \\\rCONFIG.PCW_UIPARAM_DDR_BL {8} \\\rCONFIG.PCW_UIPARAM_DDR_HIGH_TEMP {Normal (0-85)} \\\rCONFIG.PCW_UIPARAM_DDR_PARTNO {MT41J256M16 RE-125} \\\rCONFIG.PCW_UIPARAM_DDR_DRAM_WIDTH {16 Bits} \\\rCONFIG.PCW_UIPARAM_DDR_DEVICE_CAPACITY {4096 MBits} \\\rCONFIG.PCW_UIPARAM_DDR_SPEED_BIN {DDR3_1066F} \\\rCONFIG.PCW_UIPARAM_DDR_TRAIN_WRITE_LEVEL {1} \\\rCONFIG.PCW_UIPARAM_DDR_TRAIN_READ_GATE {1} \\\rCONFIG.PCW_UIPARAM_DDR_TRAIN_DATA_EYE {1} \\\rCONFIG.PCW_UIPARAM_DDR_CLOCK_STOP_EN {0} \\\rCONFIG.PCW_UIPARAM_DDR_USE_INTERNAL_VREF {0} \\\rCONFIG.PCW_DDR_PRIORITY_WRITEPORT_0 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_DDR_PRIORITY_WRITEPORT_1 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_DDR_PRIORITY_WRITEPORT_2 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_DDR_PRIORITY_WRITEPORT_3 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_DDR_PRIORITY_READPORT_0 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_DDR_PRIORITY_READPORT_1 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_DDR_PRIORITY_READPORT_2 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_DDR_PRIORITY_READPORT_3 {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_DDR_PORT0_HPR_ENABLE {0} \\\rCONFIG.PCW_DDR_PORT1_HPR_ENABLE {0} \\\rCONFIG.PCW_DDR_PORT2_HPR_ENABLE {0} \\\rCONFIG.PCW_DDR_PORT3_HPR_ENABLE {0} \\\rCONFIG.PCW_DDR_HPRLPR_QUEUE_PARTITION {HPR(0)/LPR(32)} \\\rCONFIG.PCW_DDR_LPR_TO_CRITICAL_PRIORITY_LEVEL {2} \\\rCONFIG.PCW_DDR_HPR_TO_CRITICAL_PRIORITY_LEVEL {15} \\\rCONFIG.PCW_DDR_WRITE_TO_CRITICAL_PRIORITY_LEVEL {2} \\\rCONFIG.PCW_NAND_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_NAND_NAND_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_NAND_GRP_D8_ENABLE {0} \\\rCONFIG.PCW_NAND_GRP_D8_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_NOR_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_NOR_NOR_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_NOR_GRP_A25_ENABLE {0} \\\rCONFIG.PCW_NOR_GRP_A25_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_NOR_GRP_CS0_ENABLE {0} \\\rCONFIG.PCW_NOR_GRP_CS0_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_NOR_GRP_SRAM_CS0_ENABLE {0} \\\rCONFIG.PCW_NOR_GRP_SRAM_CS0_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_NOR_GRP_CS1_ENABLE {0} \\\rCONFIG.PCW_NOR_GRP_CS1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_NOR_GRP_SRAM_CS1_ENABLE {0} \\\rCONFIG.PCW_NOR_GRP_SRAM_CS1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_NOR_GRP_SRAM_INT_ENABLE {0} \\\rCONFIG.PCW_NOR_GRP_SRAM_INT_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_QSPI_PERIPHERAL_ENABLE {1} \\\rCONFIG.PCW_QSPI_QSPI_IO {MIO 1 .. 6} \\\rCONFIG.PCW_QSPI_GRP_SINGLE_SS_ENABLE {1} \\\rCONFIG.PCW_QSPI_GRP_SINGLE_SS_IO {MIO 1 .. 6} \\\rCONFIG.PCW_QSPI_GRP_SS1_ENABLE {0} \\\rCONFIG.PCW_QSPI_GRP_SS1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_QSPI_GRP_IO1_ENABLE {0} \\\rCONFIG.PCW_QSPI_GRP_IO1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_QSPI_GRP_FBCLK_ENABLE {1} \\\rCONFIG.PCW_QSPI_GRP_FBCLK_IO {MIO 8} \\\rCONFIG.PCW_QSPI_INTERNAL_HIGHADDRESS {0xFCFFFFFF} \\\rCONFIG.PCW_ENET0_PERIPHERAL_ENABLE {1} \\\rCONFIG.PCW_ENET0_ENET0_IO {MIO 16 .. 27} \\\rCONFIG.PCW_ENET0_GRP_MDIO_ENABLE {1} \\\rCONFIG.PCW_ENET0_GRP_MDIO_IO {MIO 52 .. 53} \\\rCONFIG.PCW_ENET_RESET_ENABLE {1} \\\rCONFIG.PCW_ENET_RESET_SELECT {Share reset pin} \\\rCONFIG.PCW_ENET0_RESET_ENABLE {1} \\\rCONFIG.PCW_ENET0_RESET_IO {MIO 9} \\\rCONFIG.PCW_ENET1_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_ENET1_ENET1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_ENET1_GRP_MDIO_ENABLE {0} \\\rCONFIG.PCW_ENET1_GRP_MDIO_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_ENET1_RESET_ENABLE {0} \\\rCONFIG.PCW_ENET1_RESET_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SD0_PERIPHERAL_ENABLE {1} \\\rCONFIG.PCW_SD0_SD0_IO {MIO 40 .. 45} \\\rCONFIG.PCW_SD0_GRP_CD_ENABLE {1} \\\rCONFIG.PCW_SD0_GRP_CD_IO {MIO 47} \\\rCONFIG.PCW_SD0_GRP_WP_ENABLE {0} \\\rCONFIG.PCW_SD0_GRP_WP_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SD0_GRP_POW_ENABLE {0} \\\rCONFIG.PCW_SD0_GRP_POW_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SD1_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_SD1_SD1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SD1_GRP_CD_ENABLE {0} \\\rCONFIG.PCW_SD1_GRP_CD_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SD1_GRP_WP_ENABLE {0} \\\rCONFIG.PCW_SD1_GRP_WP_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SD1_GRP_POW_ENABLE {0} \\\rCONFIG.PCW_SD1_GRP_POW_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_UART0_PERIPHERAL_ENABLE {1} \\\rCONFIG.PCW_UART0_UART0_IO {MIO 14 .. 15} \\\rCONFIG.PCW_UART0_GRP_FULL_ENABLE {0} \\\rCONFIG.PCW_UART0_GRP_FULL_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_UART1_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_UART1_UART1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_UART1_GRP_FULL_ENABLE {0} \\\rCONFIG.PCW_UART1_GRP_FULL_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SPI0_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_SPI0_SPI0_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SPI0_GRP_SS0_ENABLE {0} \\\rCONFIG.PCW_SPI0_GRP_SS0_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SPI0_GRP_SS1_ENABLE {0} \\\rCONFIG.PCW_SPI0_GRP_SS1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SPI0_GRP_SS2_ENABLE {0} \\\rCONFIG.PCW_SPI0_GRP_SS2_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SPI1_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_SPI1_SPI1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SPI1_GRP_SS0_ENABLE {0} \\\rCONFIG.PCW_SPI1_GRP_SS0_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SPI1_GRP_SS1_ENABLE {0} \\\rCONFIG.PCW_SPI1_GRP_SS1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_SPI1_GRP_SS2_ENABLE {0} \\\rCONFIG.PCW_SPI1_GRP_SS2_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_CAN0_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_CAN0_CAN0_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_CAN0_GRP_CLK_ENABLE {0} \\\rCONFIG.PCW_CAN0_GRP_CLK_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_CAN1_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_CAN1_CAN1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_CAN1_GRP_CLK_ENABLE {0} \\\rCONFIG.PCW_CAN1_GRP_CLK_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_TRACE_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_TRACE_TRACE_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_TRACE_GRP_2BIT_ENABLE {0} \\\rCONFIG.PCW_TRACE_GRP_2BIT_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_TRACE_GRP_4BIT_ENABLE {0} \\\rCONFIG.PCW_TRACE_GRP_4BIT_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_TRACE_GRP_8BIT_ENABLE {0} \\\rCONFIG.PCW_TRACE_GRP_8BIT_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_TRACE_GRP_16BIT_ENABLE {0} \\\rCONFIG.PCW_TRACE_GRP_16BIT_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_TRACE_GRP_32BIT_ENABLE {0} \\\rCONFIG.PCW_TRACE_GRP_32BIT_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_TRACE_INTERNAL_WIDTH {2} \\\rCONFIG.PCW_WDT_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_WDT_WDT_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_TTC0_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_TTC0_TTC0_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_TTC1_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_TTC1_TTC1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_PJTAG_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_PJTAG_PJTAG_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_USB0_PERIPHERAL_ENABLE {1} \\\rCONFIG.PCW_USB0_USB0_IO {MIO 28 .. 39} \\\rCONFIG.PCW_USB_RESET_ENABLE {1} \\\rCONFIG.PCW_USB_RESET_SELECT {Share reset pin} \\\rCONFIG.PCW_USB0_RESET_ENABLE {1} \\\rCONFIG.PCW_USB0_RESET_IO {MIO 46} \\\rCONFIG.PCW_USB1_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_USB1_USB1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_USB1_RESET_ENABLE {0} \\\rCONFIG.PCW_USB1_RESET_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_I2C0_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_I2C0_I2C0_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_I2C0_GRP_INT_ENABLE {0} \\\rCONFIG.PCW_I2C0_GRP_INT_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_I2C0_RESET_ENABLE {0} \\\rCONFIG.PCW_I2C0_RESET_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_I2C1_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_I2C1_I2C1_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_I2C1_GRP_INT_ENABLE {0} \\\rCONFIG.PCW_I2C1_GRP_INT_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_I2C_RESET_ENABLE {1} \\\rCONFIG.PCW_I2C_RESET_SELECT {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_I2C1_RESET_ENABLE {0} \\\rCONFIG.PCW_I2C1_RESET_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_GPIO_PERIPHERAL_ENABLE {0} \\\rCONFIG.PCW_GPIO_MIO_GPIO_ENABLE {1} \\\rCONFIG.PCW_GPIO_MIO_GPIO_IO {MIO} \\\rCONFIG.PCW_GPIO_EMIO_GPIO_ENABLE {0} \\\rCONFIG.PCW_GPIO_EMIO_GPIO_IO {\u0026amp;lt;Select\u0026amp;gt;} \\\rCONFIG.PCW_APU_CLK_RATIO_ENABLE {6:2:1} \\\rCONFIG.PCW_ENET0_PERIPHERAL_FREQMHZ {1000 Mbps} \\\rCONFIG.PCW_ENET1_PERIPHERAL_FREQMHZ {1000 Mbps} \\\rCONFIG.PCW_CPU_PERIPHERAL_CLKSRC {ARM PLL} \\\rCONFIG.PCW_DDR_PERIPHERAL_CLKSRC {DDR PLL} \\\rCONFIG.PCW_SMC_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_QSPI_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_SDIO_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_UART_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_SPI_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_CAN_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_FCLK0_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_FCLK1_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_FCLK2_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_FCLK3_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_ENET0_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_ENET1_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_CAN0_PERIPHERAL_CLKSRC {External} \\\rCONFIG.PCW_CAN1_PERIPHERAL_CLKSRC {External} \\\rCONFIG.PCW_TPIU_PERIPHERAL_CLKSRC {External} \\\rCONFIG.PCW_TTC0_CLK0_PERIPHERAL_CLKSRC {CPU_1X} \\\rCONFIG.PCW_TTC0_CLK1_PERIPHERAL_CLKSRC {CPU_1X} \\\rCONFIG.PCW_TTC0_CLK2_PERIPHERAL_CLKSRC {CPU_1X} \\\rCONFIG.PCW_TTC1_CLK0_PERIPHERAL_CLKSRC {CPU_1X} \\\rCONFIG.PCW_TTC1_CLK1_PERIPHERAL_CLKSRC {CPU_1X} \\\rCONFIG.PCW_TTC1_CLK2_PERIPHERAL_CLKSRC {CPU_1X} \\\rCONFIG.PCW_WDT_PERIPHERAL_CLKSRC {CPU_1X} \\\rCONFIG.PCW_DCI_PERIPHERAL_CLKSRC {DDR PLL} \\\rCONFIG.PCW_PCAP_PERIPHERAL_CLKSRC {IO PLL} \\\rCONFIG.PCW_USB_RESET_POLARITY {Active Low} \\\rCONFIG.PCW_ENET_RESET_POLARITY {Active Low} \\\rCONFIG.PCW_I2C_RESET_POLARITY {Active Low} \\\rCONFIG.PCW_MIO_0_PULLUP {enabled} \\\rCONFIG.PCW_MIO_0_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_0_DIRECTION {inout} \\\rCONFIG.PCW_MIO_0_SLEW {slow} \\\rCONFIG.PCW_MIO_1_PULLUP {enabled} \\\rCONFIG.PCW_MIO_1_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_1_DIRECTION {out} \\\rCONFIG.PCW_MIO_1_SLEW {slow} \\\rCONFIG.PCW_MIO_2_PULLUP {disabled} \\\rCONFIG.PCW_MIO_2_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_2_DIRECTION {inout} \\\rCONFIG.PCW_MIO_2_SLEW {slow} \\\rCONFIG.PCW_MIO_3_PULLUP {disabled} \\\rCONFIG.PCW_MIO_3_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_3_DIRECTION {inout} \\\rCONFIG.PCW_MIO_3_SLEW {slow} \\\rCONFIG.PCW_MIO_4_PULLUP {disabled} \\\rCONFIG.PCW_MIO_4_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_4_DIRECTION {inout} \\\rCONFIG.PCW_MIO_4_SLEW {slow} \\\rCONFIG.PCW_MIO_5_PULLUP {disabled} \\\rCONFIG.PCW_MIO_5_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_5_DIRECTION {inout} \\\rCONFIG.PCW_MIO_5_SLEW {slow} \\\rCONFIG.PCW_MIO_6_PULLUP {disabled} \\\rCONFIG.PCW_MIO_6_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_6_DIRECTION {out} \\\rCONFIG.PCW_MIO_6_SLEW {slow} \\\rCONFIG.PCW_MIO_7_PULLUP {disabled} \\\rCONFIG.PCW_MIO_7_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_7_DIRECTION {out} \\\rCONFIG.PCW_MIO_7_SLEW {slow} \\\rCONFIG.PCW_MIO_8_PULLUP {disabled} \\\rCONFIG.PCW_MIO_8_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_8_DIRECTION {out} \\\rCONFIG.PCW_MIO_8_SLEW {slow} \\\rCONFIG.PCW_MIO_9_PULLUP {enabled} \\\rCONFIG.PCW_MIO_9_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_9_DIRECTION {out} \\\rCONFIG.PCW_MIO_9_SLEW {slow} \\\rCONFIG.PCW_MIO_10_PULLUP {enabled} \\\rCONFIG.PCW_MIO_10_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_10_DIRECTION {inout} \\\rCONFIG.PCW_MIO_10_SLEW {slow} \\\rCONFIG.PCW_MIO_11_PULLUP {enabled} \\\rCONFIG.PCW_MIO_11_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_11_DIRECTION {inout} \\\rCONFIG.PCW_MIO_11_SLEW {slow} \\\rCONFIG.PCW_MIO_12_PULLUP {enabled} \\\rCONFIG.PCW_MIO_12_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_12_DIRECTION {inout} \\\rCONFIG.PCW_MIO_12_SLEW {slow} \\\rCONFIG.PCW_MIO_13_PULLUP {enabled} \\\rCONFIG.PCW_MIO_13_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_13_DIRECTION {inout} \\\rCONFIG.PCW_MIO_13_SLEW {slow} \\\rCONFIG.PCW_MIO_14_PULLUP {enabled} \\\rCONFIG.PCW_MIO_14_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_14_DIRECTION {in} \\\rCONFIG.PCW_MIO_14_SLEW {slow} \\\rCONFIG.PCW_MIO_15_PULLUP {enabled} \\\rCONFIG.PCW_MIO_15_IOTYPE {LVCMOS 3.3V} \\\rCONFIG.PCW_MIO_15_DIRECTION {out} \\\rCONFIG.PCW_MIO_15_SLEW {slow} \\\rCONFIG.PCW_MIO_16_PULLUP {enabled} \\\rCONFIG.PCW_MIO_16_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_16_DIRECTION {out} \\\rCONFIG.PCW_MIO_16_SLEW {slow} \\\rCONFIG.PCW_MIO_17_PULLUP {enabled} \\\rCONFIG.PCW_MIO_17_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_17_DIRECTION {out} \\\rCONFIG.PCW_MIO_17_SLEW {slow} \\\rCONFIG.PCW_MIO_18_PULLUP {enabled} \\\rCONFIG.PCW_MIO_18_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_18_DIRECTION {out} \\\rCONFIG.PCW_MIO_18_SLEW {slow} \\\rCONFIG.PCW_MIO_19_PULLUP {enabled} \\\rCONFIG.PCW_MIO_19_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_19_DIRECTION {out} \\\rCONFIG.PCW_MIO_19_SLEW {slow} \\\rCONFIG.PCW_MIO_20_PULLUP {enabled} \\\rCONFIG.PCW_MIO_20_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_20_DIRECTION {out} \\\rCONFIG.PCW_MIO_20_SLEW {slow} \\\rCONFIG.PCW_MIO_21_PULLUP {enabled} \\\rCONFIG.PCW_MIO_21_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_21_DIRECTION {out} \\\rCONFIG.PCW_MIO_21_SLEW {slow} \\\rCONFIG.PCW_MIO_22_PULLUP {enabled} \\\rCONFIG.PCW_MIO_22_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_22_DIRECTION {in} \\\rCONFIG.PCW_MIO_22_SLEW {slow} \\\rCONFIG.PCW_MIO_23_PULLUP {enabled} \\\rCONFIG.PCW_MIO_23_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_23_DIRECTION {in} \\\rCONFIG.PCW_MIO_23_SLEW {slow} \\\rCONFIG.PCW_MIO_24_PULLUP {enabled} \\\rCONFIG.PCW_MIO_24_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_24_DIRECTION {in} \\\rCONFIG.PCW_MIO_24_SLEW {slow} \\\rCONFIG.PCW_MIO_25_PULLUP {enabled} \\\rCONFIG.PCW_MIO_25_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_25_DIRECTION {in} \\\rCONFIG.PCW_MIO_25_SLEW {slow} \\\rCONFIG.PCW_MIO_26_PULLUP {enabled} \\\rCONFIG.PCW_MIO_26_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_26_DIRECTION {in} \\\rCONFIG.PCW_MIO_26_SLEW {slow} \\\rCONFIG.PCW_MIO_27_PULLUP {enabled} \\\rCONFIG.PCW_MIO_27_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_27_DIRECTION {in} \\\rCONFIG.PCW_MIO_27_SLEW {slow} \\\rCONFIG.PCW_MIO_28_PULLUP {enabled} \\\rCONFIG.PCW_MIO_28_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_28_DIRECTION {inout} \\\rCONFIG.PCW_MIO_28_SLEW {slow} \\\rCONFIG.PCW_MIO_29_PULLUP {enabled} \\\rCONFIG.PCW_MIO_29_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_29_DIRECTION {in} \\\rCONFIG.PCW_MIO_29_SLEW {slow} \\\rCONFIG.PCW_MIO_30_PULLUP {enabled} \\\rCONFIG.PCW_MIO_30_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_30_DIRECTION {out} \\\rCONFIG.PCW_MIO_30_SLEW {slow} \\\rCONFIG.PCW_MIO_31_PULLUP {enabled} \\\rCONFIG.PCW_MIO_31_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_31_DIRECTION {in} \\\rCONFIG.PCW_MIO_31_SLEW {slow} \\\rCONFIG.PCW_MIO_32_PULLUP {enabled} \\\rCONFIG.PCW_MIO_32_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_32_DIRECTION {inout} \\\rCONFIG.PCW_MIO_32_SLEW {slow} \\\rCONFIG.PCW_MIO_33_PULLUP {enabled} \\\rCONFIG.PCW_MIO_33_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_33_DIRECTION {inout} \\\rCONFIG.PCW_MIO_33_SLEW {slow} \\\rCONFIG.PCW_MIO_34_PULLUP {enabled} \\\rCONFIG.PCW_MIO_34_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_34_DIRECTION {inout} \\\rCONFIG.PCW_MIO_34_SLEW {slow} \\\rCONFIG.PCW_MIO_35_PULLUP {enabled} \\\rCONFIG.PCW_MIO_35_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_35_DIRECTION {inout} \\\rCONFIG.PCW_MIO_35_SLEW {slow} \\\rCONFIG.PCW_MIO_36_PULLUP {enabled} \\\rCONFIG.PCW_MIO_36_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_36_DIRECTION {in} \\\rCONFIG.PCW_MIO_36_SLEW {slow} \\\rCONFIG.PCW_MIO_37_PULLUP {enabled} \\\rCONFIG.PCW_MIO_37_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_37_DIRECTION {inout} \\\rCONFIG.PCW_MIO_37_SLEW {slow} \\\rCONFIG.PCW_MIO_38_PULLUP {enabled} \\\rCONFIG.PCW_MIO_38_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_38_DIRECTION {inout} \\\rCONFIG.PCW_MIO_38_SLEW {slow} \\\rCONFIG.PCW_MIO_39_PULLUP {enabled} \\\rCONFIG.PCW_MIO_39_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_39_DIRECTION {inout} \\\rCONFIG.PCW_MIO_39_SLEW {slow} \\\rCONFIG.PCW_MIO_40_PULLUP {enabled} \\\rCONFIG.PCW_MIO_40_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_40_DIRECTION {inout} \\\rCONFIG.PCW_MIO_40_SLEW {slow} \\\rCONFIG.PCW_MIO_41_PULLUP {enabled} \\\rCONFIG.PCW_MIO_41_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_41_DIRECTION {inout} \\\rCONFIG.PCW_MIO_41_SLEW {slow} \\\rCONFIG.PCW_MIO_42_PULLUP {enabled} \\\rCONFIG.PCW_MIO_42_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_42_DIRECTION {inout} \\\rCONFIG.PCW_MIO_42_SLEW {slow} \\\rCONFIG.PCW_MIO_43_PULLUP {enabled} \\\rCONFIG.PCW_MIO_43_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_43_DIRECTION {inout} \\\rCONFIG.PCW_MIO_43_SLEW {slow} \\\rCONFIG.PCW_MIO_44_PULLUP {enabled} \\\rCONFIG.PCW_MIO_44_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_44_DIRECTION {inout} \\\rCONFIG.PCW_MIO_44_SLEW {slow} \\\rCONFIG.PCW_MIO_45_PULLUP {enabled} \\\rCONFIG.PCW_MIO_45_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_45_DIRECTION {inout} \\\rCONFIG.PCW_MIO_45_SLEW {slow} \\\rCONFIG.PCW_MIO_46_PULLUP {enabled} \\\rCONFIG.PCW_MIO_46_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_46_DIRECTION {out} \\\rCONFIG.PCW_MIO_46_SLEW {slow} \\\rCONFIG.PCW_MIO_47_PULLUP {enabled} \\\rCONFIG.PCW_MIO_47_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_47_DIRECTION {in} \\\rCONFIG.PCW_MIO_47_SLEW {slow} \\\rCONFIG.PCW_MIO_48_PULLUP {enabled} \\\rCONFIG.PCW_MIO_48_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_48_DIRECTION {inout} \\\rCONFIG.PCW_MIO_48_SLEW {slow} \\\rCONFIG.PCW_MIO_49_PULLUP {enabled} \\\rCONFIG.PCW_MIO_49_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_49_DIRECTION {inout} \\\rCONFIG.PCW_MIO_49_SLEW {slow} \\\rCONFIG.PCW_MIO_50_PULLUP {enabled} \\\rCONFIG.PCW_MIO_50_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_50_DIRECTION {inout} \\\rCONFIG.PCW_MIO_50_SLEW {slow} \\\rCONFIG.PCW_MIO_51_PULLUP {enabled} \\\rCONFIG.PCW_MIO_51_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_51_DIRECTION {inout} \\\rCONFIG.PCW_MIO_51_SLEW {slow} \\\rCONFIG.PCW_MIO_52_PULLUP {enabled} \\\rCONFIG.PCW_MIO_52_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_52_DIRECTION {out} \\\rCONFIG.PCW_MIO_52_SLEW {slow} \\\rCONFIG.PCW_MIO_53_PULLUP {enabled} \\\rCONFIG.PCW_MIO_53_IOTYPE {LVCMOS 1.8V} \\\rCONFIG.PCW_MIO_53_DIRECTION {inout} \\\rCONFIG.PCW_MIO_53_SLEW {slow} \\\rCONFIG.PCW_UIPARAM_GENERATE_SUMMARY {NA} \\\rCONFIG.PCW_MIO_TREE_PERIPHERALS {GPIO#Quad SPI Flash#Quad SPI Flash#Quad SPI Flash#Quad SPI Flash#Quad SPI Flash#Quad SPI Flash#GPIO#Quad SPI Flash#ENET Reset#GPIO#GPIO#GPIO#GPIO#UART 0#UART 0#Enet 0#Enet 0#Enet 0#Enet 0#Enet 0#Enet 0#Enet 0#Enet 0#Enet 0#Enet 0#Enet 0#Enet 0#USB 0#USB 0#USB 0#USB 0#USB 0#USB 0#USB 0#USB 0#USB 0#USB 0#USB 0#USB 0#SD 0#SD 0#SD 0#SD 0#SD 0#SD 0#USB Reset#SD 0#GPIO#GPIO#GPIO#GPIO#Enet 0#Enet 0} \\\rCONFIG.PCW_MIO_TREE_SIGNALS {gpio[0]#qspi0_ss_b#qspi0_io[0]#qspi0_io[1]#qspi0_io[2]#qspi0_io[3]#qspi0_sclk#gpio[7]#qspi_fbclk#reset#gpio[10]#gpio[11]#gpio[12]#gpio[13]#rx#tx#tx_clk#txd[0]#txd[1]#txd[2]#txd[3]#tx_ctl#rx_clk#rxd[0]#rxd[1]#rxd[2]#rxd[3]#rx_ctl#data[4]#dir#stp#nxt#data[0]#data[1]#data[2]#data[3]#clk#data[5]#data[6]#data[7]#clk#cmd#data[0]#data[1]#data[2]#data[3]#reset#cd#gpio[48]#gpio[49]#gpio[50]#gpio[51]#mdc#mdio} \\\rCONFIG.PCW_PS7_SI_REV {PRODUCTION} \\\rCONFIG.PCW_FPGA_FCLK0_ENABLE {1} \\\rCONFIG.PCW_FPGA_FCLK1_ENABLE {0} \\\rCONFIG.PCW_FPGA_FCLK2_ENABLE {0} \\\rCONFIG.PCW_FPGA_FCLK3_ENABLE {0} \\\rCONFIG.PCW_NOR_SRAM_CS0_T_TR {1} \\\rCONFIG.PCW_NOR_SRAM_CS0_T_PC {1} \\\rCONFIG.PCW_NOR_SRAM_CS0_T_WP {1} \\\rCONFIG.PCW_NOR_SRAM_CS0_T_CEOE {1} \\\rCONFIG.PCW_NOR_SRAM_CS0_T_WC {11} \\\rCONFIG.PCW_NOR_SRAM_CS0_T_RC {11} \\\rCONFIG.PCW_NOR_SRAM_CS0_WE_TIME {0} \\\rCONFIG.PCW_NOR_SRAM_CS1_T_TR {1} \\\rCONFIG.PCW_NOR_SRAM_CS1_T_PC {1} \\\rCONFIG.PCW_NOR_SRAM_CS1_T_WP {1} \\\rCONFIG.PCW_NOR_SRAM_CS1_T_CEOE {1} \\\rCONFIG.PCW_NOR_SRAM_CS1_T_WC {11} \\\rCONFIG.PCW_NOR_SRAM_CS1_T_RC {11} \\\rCONFIG.PCW_NOR_SRAM_CS1_WE_TIME {0} \\\rCONFIG.PCW_NOR_CS0_T_TR {1} \\\rCONFIG.PCW_NOR_CS0_T_PC {1} \\\rCONFIG.PCW_NOR_CS0_T_WP {1} \\\rCONFIG.PCW_NOR_CS0_T_CEOE {1} \\\rCONFIG.PCW_NOR_CS0_T_WC {11} \\\rCONFIG.PCW_NOR_CS0_T_RC {11} \\\rCONFIG.PCW_NOR_CS0_WE_TIME {0} \\\rCONFIG.PCW_NOR_CS1_T_TR {1} \\\rCONFIG.PCW_NOR_CS1_T_PC {1} \\\rCONFIG.PCW_NOR_CS1_T_WP {1} \\\rCONFIG.PCW_NOR_CS1_T_CEOE {1} \\\rCONFIG.PCW_NOR_CS1_T_WC {11} \\\rCONFIG.PCW_NOR_CS1_T_RC {11} \\\rCONFIG.PCW_NOR_CS1_WE_TIME {0} \\\rCONFIG.PCW_NAND_CYCLES_T_RR {1} \\\rCONFIG.PCW_NAND_CYCLES_T_AR {1} \\\rCONFIG.PCW_NAND_CYCLES_T_CLR {1} \\\rCONFIG.PCW_NAND_CYCLES_T_WP {1} \\\rCONFIG.PCW_NAND_CYCLES_T_REA {1} \\\rCONFIG.PCW_NAND_CYCLES_T_WC {11} \\\rCONFIG.PCW_NAND_CYCLES_T_RC {11} \\\rCONFIG.PCW_SMC_CYCLE_T0 {NA} \\\rCONFIG.PCW_SMC_CYCLE_T1 {NA} \\\rCONFIG.PCW_SMC_CYCLE_T2 {NA} \\\rCONFIG.PCW_SMC_CYCLE_T3 {NA} \\\rCONFIG.PCW_SMC_CYCLE_T4 {NA} \\\rCONFIG.PCW_SMC_CYCLE_T5 {NA} \\\rCONFIG.PCW_SMC_CYCLE_T6 {NA} \\\rCONFIG.PCW_PACKAGE_NAME {clg400} \\\rCONFIG.PCW_PLL_BYPASSMODE_ENABLE {0} \\\r] [get_bd_cells processing_system7_0] If you\u0026rsquo;ve changed the name of the PS block you might want to change the last line as,\n\u0026gt; get\\_bd\\_cells \u0026lt;ps\\_block\\_name\u0026gt; ","permalink":"http://localhost:1313/posts/configure-ps-pynq-work-sdk/","summary":"Hi,\nIf you\u0026rsquo;re an FPGA fan or someone who\u0026rsquo;s got PYNQ board for fun, you might be having a hard time making it run Vivado SDK projects. That\u0026rsquo;s because, the PYNQ-Z1, the cheap Zynq-7020 board doesn\u0026rsquo;t have any popular DDR ram on board. You need to configure it by hand, however, tcl is at your rescue.\nWhen you create a project and include Zynq-PS system to the block diagram, most of the time you don\u0026rsquo;t need to change the DDR timing properties because of most of the popular boards ie.","title":"Configure PS of PYNQ to work with SDK"},{"content":"So you are setting up HBase! Congratulations!\nWhen it comes to tuning HBase there are so many things you can do. And most of the things will be dependent upon type of data you will be storing and it\u0026rsquo;s access patterns. So I will be saying this a lot: \u0026lsquo;value of this parameter depends upon your workload\u0026rsquo;. Here I will try to enlist some of the variables that you can tweak while tuning hbase. This list is not at all exhaustive. 1. HBase RegionServer Maximum Memory (hbase_regionserver_heapsize) First thing\u0026rsquo;s first. If you have heavy load and/or available capacity in terms of RAM, you may want to increase this number. This means more space in memory to cache blocks for read and write. That simple.\n2. Percent of RegionServer Allocated to Read Buffers (hfile.block.cache.size) Now this one depends on your type of workload. If you have less writes compared to reads then you would want to set this higher (\u0026gt;40%) This will allocate more heap to cache blocks used for reading.\n3. Percent of RegionServer Allocated to Write Buffers (hbase.regionserver.global.memstore.size) Same as above, depends on your workload. If you have heavy writes you may want to set this higher. The total of read and write buffers should be around 80%. If you are not sure, you may want to leave these values 40-40.\n4. Memstore Flush Size (hbase.hregion.memstore.flush.size) This value defines, after what size, for a particular column family, memstore should be flushed to disk. If you have heavy writes with large row size you may want to increase this size from 128MB to 256MB. This will reduce the frequency of memstore flushes and hence increase the performance. Recommended value for this parameter is the value of block size of HDFS you have set. (64MB or 128MB usually)\n5. Number of Handlers per RegionServer (hbase.regionserver.handler.count) This is number of handlers available per regionserver to serve request. Default might be around 30. If you are seeing long size for calls in queue then it\u0026rsquo;s time to increase number of handlers. Increasing this very high will also affect performance. Try to find a sweet spot.\n6. Maximum Region File Size (hbase.hregion.max.filesize) This defines a size after which a region will be split into two regions. You want to tune this size because very small size will create more number of regions and it is recommended to maintain no more than 200 regions per regionserver. So you should take in account number of regionserver and size of your dataset while setting this parameter. You also want to look at split policies in hbase and want to choose one which suits your purpose.\n7. Timeouts (Zookeeper Session, RPC, Phoenix Query ) The values for these defaults are set very low and you want to set this values in magnitude of minutes (10 minutes around) to avoid facing kinds of timeouts.\n8. RegionServer Lease Period (hbase.regionserver.lease.period) For some kinds of requests (queries) which takes more time to get processed, you may face Scanner Timeouts or Scanner not Found Errors. And to avert this you should increase this value.\n9. Setting Off-Peak Hours (base.offpeak.[end|start].hour) HBase also allow to schedule major compaction during certain time period. HBase will try to run major compaction during this time. You should set these values when load on your cluster is relatively low (night time maybe).\n10. Compression and Datablock Encoding This is one more functionality HBase provides for which there is no reason not to use it. This article provides more insights into this. While this may impact the performance a little but it will save you lots of disk space.\nAs I said, this list is not exhaustive and there is a lot you can do while tuning hbase. Take a look at this chapter from HBase book. You also can perform benchmarking after tuning parameters so that you know how that particular change affected the performance.\nHappy Tuning 😀\n","permalink":"http://localhost:1313/posts/tuning-hbase/","summary":"So you are setting up HBase! Congratulations!\nWhen it comes to tuning HBase there are so many things you can do. And most of the things will be dependent upon type of data you will be storing and it\u0026rsquo;s access patterns. So I will be saying this a lot: \u0026lsquo;value of this parameter depends upon your workload\u0026rsquo;. Here I will try to enlist some of the variables that you can tweak while tuning hbase.","title":"Stuff You Can Do While Tuning HBase"},{"content":"Currently I am working with new setup of Apache HBase cluster to query data using Phoenix on top of HDP Distribution. After setting up cluster, the values for heap, cache and timeouts were all defaults. Now I needed to know how good is the cluster in current shape and how can it be improved. Now for the improvement part, understanding of HBase internals is needed. How does a write work in HBase. What is the read path. What is the data access and data writing patterns. By analyzing these aspects, you vary parameters. But after varying, one needs to see the effect of variance right? And thus you need something to measure performance and benchmark the cluster.\nI found two tools for this purpose:\nPerformanceEvaluation This comes built-in with the HBase. It has various parameters to run different kinds of workloads. So first we write data. I am using `–nomapred` option because I have not installed YARN. To get list of supported options and parameters just run the command without any options.\nSo here we first load the data. I am using randonWrite here. With 1 thread.\n$ time hbase org.apache.hadoop.hbase.PerformanceEvaluation -nomapred randomWrite 1 The result may show something like:\n2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest Min = 2.0 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest Avg = 28.936847686767578 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest StdDev = 3293.3700704030302 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 50th = 3.0 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 75th = 3.0 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 95th = 5.0 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 99th = 9.0 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 99.9th = 93.0 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 99.99th = 34495.983499997295 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 99.999th = 158854.06521204324 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest Max = 2486451.0 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest valueSize after 0 measures 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest Min = 0.0 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest Avg = 0.0 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest StdDev = 0.0 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 50th = 0.0 2017-02-24 08:22:41,491 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 75th = 0.0 2017-02-24 08:22:41,492 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 95th = 0.0 2017-02-24 08:22:41,492 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 99th = 0.0 2017-02-24 08:22:41,492 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 99.9th = 0.0 2017-02-24 08:22:41,492 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 99.99th = 0.0 2017-02-24 08:22:41,492 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest 99.999th = 0.0 2017-02-24 08:22:41,492 INFO [TestClient-0] hbase.PerformanceEvaluation: RandomWriteTest Max = 0.0 2017-02-24 08:22:41,572 INFO [TestClient-0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x25a46883b870366 2017-02-24 08:22:41,578 INFO [TestClient-0] zookeeper.ZooKeeper: Session: 0x25a46883b870366 closed 2017-02-24 08:22:41,578 INFO [TestClient-0-EventThread] zookeeper.ClientCnxn: EventThread shut down 2017-02-24 08:22:41,589 INFO [TestClient-0] hbase.PerformanceEvaluation: Finished class org.apache.hadoop.hbase.PerformanceEvaluation$RandomWriteTest in 32134ms at offset 0 for 1048576 rows (32.08 MB/s) 2017-02-24 08:22:41,589 INFO [TestClient-0] hbase.PerformanceEvaluation: Finished TestClient-0 in 32134ms over 1048576 rows 2017-02-24 08:22:41,590 INFO [main] hbase.PerformanceEvaluation: [RandomWriteTest] Summary of timings (ms): [32134] 2017-02-24 08:22:41,590 INFO [main] hbase.PerformanceEvaluation: [RandomWriteTest]\tMin: 32134ms\tMax: 32134ms\tAvg: 32134ms 2017-02-24 08:22:41,590 INFO [main] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x25a46883b870365 2017-02-24 08:22:41,647 INFO [main] zookeeper.ZooKeeper: Session: 0x25a46883b870365 closed 2017-02-24 08:22:41,647 INFO [main-EventThread] zookeeper.ClientCnxn: EventThread shut down real\t0m38.204s user\t0m44.662s sys\t0m2.264s To write in parallel:\n$ time hbase org.apache.hadoop.hbase.PerformanceEvaluation -nomapred randomWrite 3 And to read:\n$ time hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred randomRead 1 $ time hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows=100000 sequentialRead 1 Again, there are so many options to this utility, try them as needed.\nYCSB Yahoo! Cloud Serving Benchmark is tool for benchmarking various kind of databases like Cassandra, MongoGB, Voldemort, HBase etc. The steps for setup are explained here. These will the steps to follow while using this tool:\nCreate table named usertable Load the data Run workload tests Now after creating table, we first need to load the data into this table. There are various kinds of workloads for different purpose. We will use workload A to load the data in table we created.\nLoading the data: first parameter `load` which tells to write data to table. Second parameter is type of database. `-P` tells type of workload. Next three parameters are self explanatory. `-s` prints progress as loading happens.\n$ ./bin/ycsb load hbase10 -P workloads/workloada -p columnfamily=f1 -p recordcount=1000000 -threads 10 -s \u0026gt; new-A-load-1M.dat The output looks something like:\n[OVERALL], RunTime(ms), 396804.0 [OVERALL], Throughput(ops/sec), 2520.1358857269583 [TOTAL_GCS_PS_Scavenge], Count, 566.0 ... [TOTAL_GC_TIME_%], Time(%), 0.4821019949395672 [CLEANUP], Operations, 20.0 [CLEANUP], AverageLatency(us), 2676.6 [CLEANUP], MinLatency(us), 4.0 [CLEANUP], MaxLatency(us), 53055.0 [CLEANUP], 95thPercentileLatency(us), 252.0 [CLEANUP], 99thPercentileLatency(us), 53055.0 [INSERT], Operations, 1000000.0 [INSERT], AverageLatency(us), 3941.506955 [INSERT], MinLatency(us), 1125.0 [INSERT], MaxLatency(us), 664575.0 [INSERT], 95thPercentileLatency(us), 6155.0 [INSERT], 99thPercentileLatency(us), 8391.0 [INSERT], Return=OK, 1000000 So here I was getting around 2500 operations/sec\nReading the data I will use workload C which is read only. You can read more on types of workloads on link given above. Here first parameter is `run` as we are running the tests, as opposed to loading the data. All other parameters are familiar looking.\n$ ./bin/ycsb run hbase10 -P workloads/workloadc -p columnfamily=f1 -p recordcount=100000 -p operationcount=100000 -threads 10 -s \u0026gt; new-C-run-100k.dat The output for above test should look like\n[OVERALL], RunTime(ms), 12776.0 [OVERALL], Throughput(ops/sec), 7827.175954915467 [TOTAL_GCS_PS_Scavenge], Count, 17.0 ... [TOTAL_GC_TIME_%], Time(%), 0.9862241703193488 [READ], Operations, 100000.0 [READ], AverageLatency(us), 1073.35956 [READ], MinLatency(us), 302.0 [READ], MaxLatency(us), 471551.0 [READ], 95thPercentileLatency(us), 1972.0 [READ], 99thPercentileLatency(us), 5703.0 [READ], Return=OK, 100000 ... So for reads I was getting around 7800 operations/second.\nFind a workload suitable for your usecase. Or create one. Run tests on them. And let me know how is your cluster doing 🙂 I am also planning to write some of my thoughts and findings on tuning the cluster. Until then, happy hadooping\u0026hellip;\n","permalink":"http://localhost:1313/posts/hbase-benchmarking/","summary":"Currently I am working with new setup of Apache HBase cluster to query data using Phoenix on top of HDP Distribution. After setting up cluster, the values for heap, cache and timeouts were all defaults. Now I needed to know how good is the cluster in current shape and how can it be improved. Now for the improvement part, understanding of HBase internals is needed. How does a write work in HBase.","title":"HBase Benchmarking"},{"content":"So the other day I had to create a CentOS 6 AMI for HDP installation as it had Hue package available only for CentOS 6. I launched an instance with EBS attached of 10 GB with CentOS 6. Went on to create AMI out of it with EBS size of 100GB.\nThese all went good and I proceed with launching instances for HDP cluster (12 was the number of instances). Everything went good and installation was complete. Later only Ambari Server started throwing warnings about disk space. Despite attaching a 100 GB EBS.\nUpon checking it came to my notice that the EBS attached is indeed 100 GB but the root partition was only 8 GB. So I can make an fs out of remaining space and mount it somewhere but that\u0026rsquo;s not what I wanted.\n[sanketp@hdpm01 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 100G 0 disk └─xvda1 202:1 0 8G 0 part / So I started searching on how to extend root partition. Found may solutions. Including one on AWS Docs. Most solutions were requiring\nTo unmount and attach it somewhere else and then perform extend operation Take a snapshot and then extend and then launch new instance Taking 12 instances offline and mounting their EBS on some other instance and extending was not feasible for me. Neither was taking snapshot of every instance and launch new one with extended partition. I wanted to do it online. Tried various tools like parted, resize2fs etc. None of them worked, until I finally found one that did the job while partition being online and mounted.\ngrowpart: So this was the savior. And following were the steps I took to get it resized:\nGet the repo in place. Install the package Run the utility (last number is for partition, here it was first partition) Reboot! sudo yum install //download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm -y sudo yum install cloud-utils-growpart -y sudo growpart /dev/xvda 1 sudo reboot So this little tool came to rescue and saved many hours. This problem tends to happen with older linux versions like CentOS 6, Debian-jessie, SELS11.X. This guy has solution for Debian with same problem.\nCheers!\n","permalink":"http://localhost:1313/posts/resize-ebs-root-volume-centos-ami/","summary":"So the other day I had to create a CentOS 6 AMI for HDP installation as it had Hue package available only for CentOS 6. I launched an instance with EBS attached of 10 GB with CentOS 6. Went on to create AMI out of it with EBS size of 100GB.\nThese all went good and I proceed with launching instances for HDP cluster (12 was the number of instances). Everything went good and installation was complete.","title":"Resize EBS Root Volume of CentOS 6 AMI"},{"content":"The other day I faced a problem with monitoring setup and I found that the WebUI is not responding. I SSHed into server and checked if process is running. It was. Checked if port was open. It was. So as it happened, the process was running and listening on port but it was stuck somewhere and it was not accepting connection. So there it was, a running stuck process.\nNow I could simply have restarted the stuck process but that wouldn’t tell me what actually happened and where it was stuck.\nThis is not step by step guide but it provides an insight on how various tools commands can be used. So here’s what I did to investigate what was going on:\nFind The Stuck Process: You can use the following to get the process ID\n$ ps auxww | grep Okay so we have PID now. Let’s look into what the stuck process is doing right now. `strace` comes to rescue here and it showed something like\n$ strace -p \u0026lt;pid\u0026gt; recvfrom(11, ) If you google around for system call called `recvfrom` you will get something like:\nThe recvfrom() and recvmsg() calls are used to receive messages from a socket, and may be used to receive data on a socket whether or not it is connection-oriented.\nSo we now know process is trying to receive data and stuck there itself, reading further into that man entry it says the first argument is socketfd (which was 11 in my case) That can help us know more on that socket which is stuck.\nSo to dig more in that socketfd we use /proc filesystem.\nls -l /proc//fd lrwx-- 1 sanket sanket 64 Feb 5 23:00 0 -\u0026gt;; /dev/pts/19 lrwx-- 1 sanket sanket 64 Feb 5 23:00 1 -\u0026gt;; /dev/pts/19 lrwx-- 1 sanket sanket 64 Feb 5 22:59 2 -\u0026gt;; /dev/pts/19 ... lrwx-- 1 sanket sanket 64 Feb 5 23:00 11 -\u0026gt; socket:[102286] Note how FD 11 is a socket fd. Note the number (102286). Now let’s dig more into that socket. `lsof` can help us here.\n$ lsof -i -a -p \u0026lt;pid\u0026gt; | grep 102286 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME telnet 14480 sanket 3u IPv4 102286 0t0 TCP 192.168.1.2:59254 -\u0026gt;; maa03s21-in-f78.1e100.net:http (ESTABLISHED) This will finally tell us where the socket is connected to. It can be your database server. So there. You know you have to fix your database.\nDoing something with stuck process: I went a step ahead to unfreeze the process. Getting it back on without restarting it. So here comes a debugger in picture. Fire up gdb and force process to give up on that FD. ie call the close method on the stuck fd.\n$ gdb -p \u0026lt;pid\u0026gt; call close(11) This should close the FD and process should move on.\nHappy Debugging ?\n","permalink":"http://localhost:1313/posts/debugging-stuck-process-linux/","summary":"The other day I faced a problem with monitoring setup and I found that the WebUI is not responding. I SSHed into server and checked if process is running. It was. Checked if port was open. It was. So as it happened, the process was running and listening on port but it was stuck somewhere and it was not accepting connection. So there it was, a running stuck process.\nNow I could simply have restarted the stuck process but that wouldn’t tell me what actually happened and where it was stuck.","title":"Debugging Stuck Process in Linux"},{"content":"I was reading on HDFS (Hadoop\u0026rsquo;s distributed file system) and it\u0026rsquo;s internals. How does it store data. What is reading path. What is writing path. How does replication works. And to understand it better my mentor suggested me to implement the same. And so I made PyDFS. (Screenshots at bottom of the post)\nSo the choice of my language was python of course as it has vast number of modules available and you can code faster. I tried to implement very basic distributed file system and the code is ~200 lines. Before I started coding, some decisions needed to be taken.\nUpdate: I gave a talk at SRECon 2019 on the same subject, if you prefer detailed explanation in video format, checkout SRECon19 Asia: Let\u0026rsquo;s Build a Distributed File System or just scroll down to the bottom of the page.\nArchitecture: Because it is a HDFS clone. It also has similar architecture. The naming for components are inspired from SaltStack. It has a Master(NameNode) and a Minion(DataNode). And a client to communicate with file system.\nMaster will store file system namespace: Files,blocks,file to block mapping,block to minion mapping.\nMinion will just store the blocks. And upon request read or write operations on blocks.\nFor communication between components, I first thought of exposing HTTP API. So that every component will listen on a port and calls can be made on HTTP as implementing the same would have been fairly easy using Flask. But as mentioned in HDFS architecture, it uses custom RPC protocol for communication over TCP, I searched for something similar for Python and found RPyC. RPyC is very simple and easy to use.\nHow does this distributed file system work? Because I was trying to clone HDFS, I tried to follow similar read and write patterns.\nTo write a file, master will allocate blocks and a minion on which it will be stored. Client will write it to one minion and that minion will pass data to next one. For reading master will provide list of blocks and its location and client will read sequentially. If reading fails from one minion, it will try next minion. File system persistence is implemented via pickling the object. When you give Ctrl+C to master, it will dump the matadata to a file and it will load the same when you fire master up. Implementation: I have uploaded the code on GitHub under sanketplus/PyDFS. I made this during a weekend and I have not added comments but I think code is fairly understandable. Feel free to Star it or make a PR 🙂\nHere\u0026rsquo;s couple of screenshots of PyDFS in action:\nIn first image we are putting a file into DFS (my public key) and the lines you see are the blocks of the file (I have set smaller block size here). In second image we are trying to get the image from DFS and it will print it on stdout.\nRecording of the talk: ","permalink":"http://localhost:1313/posts/distributed-file-system-python/","summary":"I was reading on HDFS (Hadoop\u0026rsquo;s distributed file system) and it\u0026rsquo;s internals. How does it store data. What is reading path. What is writing path. How does replication works. And to understand it better my mentor suggested me to implement the same. And so I made PyDFS. (Screenshots at bottom of the post)\nSo the choice of my language was python of course as it has vast number of modules available and you can code faster.","title":"Simple Distributed File System in Python : PyDFS"}]