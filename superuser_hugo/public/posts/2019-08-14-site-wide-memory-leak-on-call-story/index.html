<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Site Wide Memory Leak: An On-Call Story | Superuser</title>
<meta name="keywords" content="">
<meta name="description" content="This happened a while back, sometime in the year of 2017. I was on-call for the week and it was the weekend. Usually, things are quiet over the weekends but not that weekend. Pages started coming frequently affecting different hosts. The alert was titled
WARNING: Memory usage is more than 80% And that was just not one or set of hosts. This started coming from random hosts from across the infrastructure.">
<meta name="author" content="sanket">
<link rel="canonical" href="https://exampleeeee.org/posts/2019-08-14-site-wide-memory-leak-on-call-story/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://exampleeeee.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://exampleeeee.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://exampleeeee.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://exampleeeee.org/apple-touch-icon.png">
<link rel="mask-icon" href="https://exampleeeee.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://exampleeeee.org/posts/2019-08-14-site-wide-memory-leak-on-call-story/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="Site Wide Memory Leak: An On-Call Story" />
<meta property="og:description" content="This happened a while back, sometime in the year of 2017. I was on-call for the week and it was the weekend. Usually, things are quiet over the weekends but not that weekend. Pages started coming frequently affecting different hosts. The alert was titled
WARNING: Memory usage is more than 80% And that was just not one or set of hosts. This started coming from random hosts from across the infrastructure." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://exampleeeee.org/posts/2019-08-14-site-wide-memory-leak-on-call-story/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-08-14T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-08-14T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Site Wide Memory Leak: An On-Call Story"/>
<meta name="twitter:description" content="This happened a while back, sometime in the year of 2017. I was on-call for the week and it was the weekend. Usually, things are quiet over the weekends but not that weekend. Pages started coming frequently affecting different hosts. The alert was titled
WARNING: Memory usage is more than 80% And that was just not one or set of hosts. This started coming from random hosts from across the infrastructure."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://exampleeeee.org/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Site Wide Memory Leak: An On-Call Story",
      "item": "https://exampleeeee.org/posts/2019-08-14-site-wide-memory-leak-on-call-story/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Site Wide Memory Leak: An On-Call Story",
  "name": "Site Wide Memory Leak: An On-Call Story",
  "description": "This happened a while back, sometime in the year of 2017. I was on-call for the week and it was the weekend. Usually, things are quiet over the weekends but not that weekend. Pages started coming frequently affecting different hosts. The alert was titled\nWARNING: Memory usage is more than 80% And that was just not one or set of hosts. This started coming from random hosts from across the infrastructure.",
  "keywords": [
    
  ],
  "articleBody": "This happened a while back, sometime in the year of 2017. I was on-call for the week and it was the weekend. Usually, things are quiet over the weekends but not that weekend. Pages started coming frequently affecting different hosts. The alert was titled\nWARNING: Memory usage is more than 80% And that was just not one or set of hosts. This started coming from random hosts from across the infrastructure. Be it a data host, an app host, some ops specific hosts. Pretty random. Something was obviously wrong\nInitial Symptoms and Observations: Alerts were triggered from random hosts Checking Grafana memory usage graphs clearly indicated an increase in memory usage. This also included hosts like mail servers which had consistent memory usage for months and suddenly they too started using more memory The time when memory usage started going up across hosts was close enough. That means some change triggered this. Next Steps: There were two obvious things from the initial look at the situation.\nSomething is causing the memory leak Some change triggered this This is how it went finding answers for the above two questions\nWhat is using the memory? The most interesting hosts were the mail servers I mentioned earlier. They had nothing significant running on them and yet memory usage touch a GB. Tried finding processes using the highest amount of memory but they all were using a nominal amount of memory.\nsanket@tfs:~$ ps auxw --sort rss | tail Even tried totaling RSS of all the processes running on that host but that amount was still very very less than total memory used by the server\nsanket@tfs:~$ ps auxw | awk 'BEGIN {sum=0} {sum +=$6} END {print sum/1024}' So this still wasn’t telling where exactly the memory is being used. Now, what tells you the comprehensive memory usage picture? /proc/meminfo\nsanket@tfs:~$ cat /proc/meminfo ... MemTotal: 3524440 kB MemFree: 477176 kB MemAvailable: 2270304 kB ... Slab: 538216 kB SReclaimable: 482496 kB SUnreclaim: 55720 kB ... So apparently, most of the leaking memory was listed under Slab section. Initially had no idea what does it mean but further searches on revealed that it’s a cache for kernel’s data structures. To see what specific data structures are taking that space\nsanket@tfs:~$ sudo slabtop --once ... OBJS ACTIVE USE OBJ SIZE SLABS OBJ/SLAB CACHE SIZE NAME 71883 49685 0% 0.19K 3423 21 313692K dentry ... So something called dentry was eating all the memory. What is it? It is a directory entry which is cached. When you look up for a file or a directory, the operation goes iteratively scanning each component of the path. For example, to get /etc/passwd, it has to first find dir / and list its contents, after finding etc in it, it has to list contents of etc and so on. When this lookup is finally done, the kernel will cache it because the above iterative resolution has to do expensive disk reads. (If you want to know more on this, checkout talk I gave on filesystem)\nNow, something was filling up the cache. Wasn’t sure what it was but having found what was using the memory, a fix was also found. Because it’s a cache, it should be (moderately?) safe to drop it.\nroot@tfs:~# echo 3 \u003e /proc/sys/vm/drop_caches So this fixes the symptom at least. We did not need to restart the servers anymore to reclaim the memory.\nWho triggered this? One of the symptoms of the problem was that it was spread sitewide. What else works site-wide? The configuration management system. Saltstack.\nGoing through git logs of salt repo, there was one commit that was correlating with increase in memory usage.\nWhat was the change? An API that each host contacts to (called from custom salt grain module if I remember correctly) was moved from HTTP to HTTPS. But why would switching to HTTPS cause a memory leak?!?!\nThe Root Cause: Running that API in a loop on a host with ample free memory reproduced the problem. Putting it under strace revealed that it’s trying to access lot of non-existing files. Which also justifies the increase in dentry cache usage.\nWas not exactly sure why would it try to access those files but quick search on the internet landed me on StackExchange post Unusually high dentry cache usage which revealed that it was due to a bug in NSS (Network Security Services, library developed by Mozilla, used when we do SSL stuff)\nThe fix was to upgrade the library or temporarily adjust cache pressure.\nFin Guess who learned something new that weekend! The feeling after solving the mystery is so satisfying :D\nNote: The command executions shown above and its results are not from actual affected servers. The numbers are made up.\n",
  "wordCount" : "790",
  "inLanguage": "en",
  "datePublished": "2019-08-14T00:00:00Z",
  "dateModified": "2019-08-14T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "sanket"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://exampleeeee.org/posts/2019-08-14-site-wide-memory-leak-on-call-story/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Superuser",
    "logo": {
      "@type": "ImageObject",
      "url": "https://exampleeeee.org/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://exampleeeee.org/" accesskey="h" title="Superuser (Alt + H)">Superuser</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://exampleeeee.org/">Home</a>&nbsp;»&nbsp;<a href="https://exampleeeee.org/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Site Wide Memory Leak: An On-Call Story
    </h1>
    <div class="post-meta"><span title='2019-08-14 00:00:00 +0000 UTC'>August 14, 2019</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;sanket

</div>
  </header> 
  <div class="post-content"><p>This happened a while back, sometime in the year of 2017. I was on-call for the week and it was the weekend. Usually, things are quiet over the weekends but not that weekend. Pages started coming frequently affecting different hosts. The alert was titled</p>
<h1 id="warning-memory-usage-is-more-than-80">WARNING: Memory usage is more than 80%<a hidden class="anchor" aria-hidden="true" href="#warning-memory-usage-is-more-than-80">#</a></h1>
<p>And that was just not one or set of hosts. This started coming from random hosts from across the infrastructure. Be it a data host, an app host, some ops specific hosts. Pretty random. <strong>Something was obviously wrong</strong></p>
<h2 id="initial-symptoms-and-observations">Initial Symptoms and Observations:<a hidden class="anchor" aria-hidden="true" href="#initial-symptoms-and-observations">#</a></h2>
<ol>
<li>Alerts were triggered from random hosts</li>
<li>Checking Grafana memory usage graphs clearly indicated an increase in memory usage.<!-- raw HTML omitted -->
This also included hosts like mail servers which had consistent memory usage for months and suddenly they too started using more memory</li>
<li>The time when memory usage started going up across hosts was close enough. That means some change triggered this.</li>
</ol>
<h2 id="next-steps">Next Steps:<a hidden class="anchor" aria-hidden="true" href="#next-steps">#</a></h2>
<p>There were two obvious things from the initial look at the situation.</p>
<ol>
<li><em>Something</em> is causing the memory leak</li>
<li><em>Some change</em> triggered this</li>
</ol>
<p>This is how it went finding answers for the above two questions</p>
<h3 id="what-is-using-the-memory">What is using the memory?<a hidden class="anchor" aria-hidden="true" href="#what-is-using-the-memory">#</a></h3>
<p>The most interesting hosts were the mail servers I mentioned earlier. They had nothing significant running on them and yet memory usage touch a GB. Tried finding processes using the highest amount of memory but they all were using a nominal amount of memory.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sanket@tfs:~$ ps auxw --sort rss | tail
</span></span></code></pre></div><p>Even tried totaling RSS of all the processes running on that host but that amount was still very very less than total memory used by the server</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sanket@tfs:~$ ps auxw | awk <span style="color:#e6db74">&#39;BEGIN {sum=0} {sum +=$6} END {print sum/1024}&#39;</span>
</span></span></code></pre></div><p>So this still wasn&rsquo;t telling where exactly the memory is being used. Now, what tells you the comprehensive memory usage picture? <code>/proc/meminfo</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sanket@tfs:~$ cat /proc/meminfo
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>MemTotal:        <span style="color:#ae81ff">3524440</span> kB
</span></span><span style="display:flex;"><span>MemFree:          <span style="color:#ae81ff">477176</span> kB
</span></span><span style="display:flex;"><span>MemAvailable:    <span style="color:#ae81ff">2270304</span> kB
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>Slab: <span style="color:#ae81ff">538216</span> kB
</span></span><span style="display:flex;"><span>SReclaimable: <span style="color:#ae81ff">482496</span> kB
</span></span><span style="display:flex;"><span>SUnreclaim: <span style="color:#ae81ff">55720</span> kB
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>So apparently, most of the <em>leaking</em> memory was listed under <code>Slab</code> section. Initially had no idea what does it mean but further searches on revealed that it&rsquo;s a cache for kernel&rsquo;s data structures. To see what specific data structures are taking that space</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sanket@tfs:~$ sudo slabtop --once
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">71883</span>  <span style="color:#ae81ff">49685</span>   0%    0.19K   <span style="color:#ae81ff">3423</span>       <span style="color:#ae81ff">21</span>     313692K dentry
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>So something called <code>dentry</code> was eating all the memory. What is it? It is a directory entry which is cached. When you look up for a file or a directory, the operation goes iteratively scanning each component of the path. For example, to get <code>/etc/passwd</code>, it has to first find dir <code>/</code> and list its contents, after finding <code>etc</code> in it, it has to list contents of <code>etc</code> and so on. When this lookup is finally done, the kernel will cache it because the above iterative resolution has to do expensive disk reads. (If you want to know more on this, checkout <a href="/lets-build-distributed-filesystem/">talk I gave on filesystem</a>)</p>
<p>Now, something was filling up the cache. Wasn&rsquo;t sure what it was but having found what was using the memory, a fix was also found. Because it&rsquo;s a cache, it should be (moderately?) safe to drop it.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@tfs:~# echo <span style="color:#ae81ff">3</span> &gt; /proc/sys/vm/drop_caches
</span></span></code></pre></div><p>So this fixes the symptom at least. We did not need to restart the servers anymore to reclaim the memory.</p>
<h3 id="who-triggered-this">Who triggered this?<a hidden class="anchor" aria-hidden="true" href="#who-triggered-this">#</a></h3>
<p>One of the symptoms of the problem was that it was spread sitewide. What else works site-wide? The configuration management system. <a href="https://www.saltstack.com/">Saltstack</a>.</p>
<p>Going through git logs of salt repo, there was one commit that was correlating with increase in memory usage.</p>
<p>What was the change? An API that each host contacts to (called from custom salt grain module if I remember correctly) was moved from HTTP to HTTPS. <strong>But why would switching to HTTPS cause a memory leak?!?!</strong></p>
<h2 id="the-root-cause">The Root Cause:<a hidden class="anchor" aria-hidden="true" href="#the-root-cause">#</a></h2>
<p>Running that API in a loop on a host with ample free memory reproduced the problem. Putting it under <code>strace</code> revealed that it&rsquo;s trying to access lot of non-existing files. Which also justifies the increase in dentry cache usage.</p>
<p>Was not exactly sure why would it try to access those files but quick search on the internet landed me on StackExchange post <a href="https://serverfault.com/questions/561350/unusually-high-dentry-cache-usage">Unusually high dentry cache usage</a> which revealed that it was due to <a href="https://bugzilla.redhat.com/show_bug.cgi?format=multiple&amp;id=1044666">a bug in NSS</a> (Network Security Services, library developed by Mozilla, used when we do SSL stuff)</p>
<p>The fix was to upgrade the library or temporarily <a href="https://www.kernel.org/doc/Documentation/sysctl/vm.txt">adjust cache pressure</a>.</p>
<h2 id="fin">Fin<a hidden class="anchor" aria-hidden="true" href="#fin">#</a></h2>
<p>Guess who learned something new that weekend! The feeling after solving the mystery is so satisfying :D</p>
<p><strong>Note:</strong> The command executions shown above and its results are not from actual affected servers. The numbers are made up.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://exampleeeee.org/">Superuser</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
